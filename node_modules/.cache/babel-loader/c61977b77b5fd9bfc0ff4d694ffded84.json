{"ast":null,"code":"/**\r\n * @license\r\n * Copyright 2018 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\nimport { dispose } from '../globals';\nimport { variableGrads } from '../gradients';\nimport { scalar } from '../ops/ops';\nimport { Serializable } from '../serialization';\n/** @doc {heading: 'Training', subheading: 'Classes', namespace: 'train'} */\n\nexport class Optimizer extends Serializable {\n  /**\r\n   * Executes `f()` and minimizes the scalar output of `f()` by computing\r\n   * gradients of y with respect to the list of trainable variables provided by\r\n   * `varList`. If no list is provided, it defaults to all trainable variables.\r\n   *\r\n   * @param f The function to execute and whose output to minimize.\r\n   * @param returnCost Whether to return the scalar cost value produced by\r\n   * executing `f()`.\r\n   * @param varList An optional list of variables to update. If specified, only\r\n   * the trainable variables in varList will be updated by minimize. Defaults to\r\n   * all trainable variables.\r\n   *\r\n   * @doc {heading: 'Training', subheading: 'Optimizers'}\r\n   */\n  minimize(f, returnCost = false, varList) {\n    const {\n      value,\n      grads\n    } = this.computeGradients(f, varList);\n\n    if (varList != null) {\n      const gradArray = varList.map(v => ({\n        name: v.name,\n        tensor: grads[v.name]\n      }));\n      this.applyGradients(gradArray);\n    } else {\n      this.applyGradients(grads);\n    } // Dispose gradients.\n\n\n    dispose(grads);\n\n    if (returnCost) {\n      return value;\n    } else {\n      value.dispose();\n      return null;\n    }\n  }\n  /**\r\n   * The number of iterations that this optimizer instance has been invoked for.\r\n   */\n\n\n  get iterations() {\n    if (this.iterations_ == null) {\n      this.iterations_ = 0;\n    }\n\n    return this.iterations_;\n  }\n\n  incrementIterations() {\n    this.iterations_ = this.iterations + 1;\n  }\n  /**\r\n   * Executes f() and computes the gradient of the scalar output of f() with\r\n   * respect to the list of trainable variables provided by `varList`. If no\r\n   * list is provided, it defaults to all trainable variables.\r\n   *\r\n   * @param f The function to execute and whose output to use for computing\r\n   * gradients with respect to variables.\r\n   * @param varList An optional list of variables to compute gradients with\r\n   * respect to. If specified, only the trainable variables in varList will have\r\n   * gradients computed with respect to. Defaults to all trainable variables.\r\n   *\r\n   * @doc {heading: 'Training', subheading: 'Optimizers'}\r\n   */\n\n\n  computeGradients(f, varList) {\n    return variableGrads(f, varList);\n  }\n  /**\r\n   * Dispose the variables (if any) owned by this optimizer instance.\r\n   */\n\n\n  dispose() {\n    if (this.iterations_ != null) {\n      dispose(this.iterations_);\n    }\n  }\n\n  async saveIterations() {\n    if (this.iterations_ == null) {\n      this.iterations_ = 0;\n    }\n\n    return {\n      name: 'iter',\n      // TODO(cais): Use 'int64' type when available.\n      tensor: scalar(this.iterations_, 'int32')\n    };\n  }\n\n  async getWeights() {\n    throw new Error('getWeights() is not implemented for this optimizer yet.');\n  }\n\n  async setWeights(weightValues) {\n    throw new Error(`setWeights() is not implemented for this optimizer class ` + `${this.getClassName()}`);\n  }\n  /**\r\n   * Extract the first element of the weight values and set it\r\n   * as the iterations counter variable of this instance of optimizer.\r\n   *\r\n   * @param weightValues\r\n   * @returns Weight values with the first element consumed and excluded.\r\n   */\n\n\n  async extractIterations(weightValues) {\n    this.iterations_ = (await weightValues[0].tensor.data())[0];\n    return weightValues.slice(1);\n  }\n\n}\nObject.defineProperty(Optimizer, Symbol.hasInstance, {\n  value: instance => {\n    return instance.minimize != null && instance.computeGradients != null && instance.applyGradients != null;\n  }\n});","map":{"version":3,"sources":["../../src/optimizers/optimizer.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,OAAR,QAAsB,YAAtB;AACA,SAAQ,aAAR,QAA4B,cAA5B;AACA,SAAQ,MAAR,QAAqB,YAArB;AACA,SAAQ,YAAR,QAA2B,kBAA3B;AAoBA;;AACA,OAAM,MAAgB,SAAhB,SAAkC,YAAlC,CAA8C;AAGlD;;;;;;;;;;;;;AAaG;AACH,EAAA,QAAQ,CAAC,CAAD,EAAkB,UAAU,GAAG,KAA/B,EAAsC,OAAtC,EAA0D;AAEhE,UAAM;AAAC,MAAA,KAAD;AAAQ,MAAA;AAAR,QAAiB,KAAK,gBAAL,CAAsB,CAAtB,EAAyB,OAAzB,CAAvB;;AAEA,QAAI,OAAO,IAAI,IAAf,EAAqB;AACnB,YAAM,SAAS,GACX,OAAO,CAAC,GAAR,CAAY,CAAC,KAAK;AAAC,QAAA,IAAI,EAAE,CAAC,CAAC,IAAT;AAAe,QAAA,MAAM,EAAE,KAAK,CAAC,CAAC,CAAC,IAAH;AAA5B,OAAL,CAAb,CADJ;AAEA,WAAK,cAAL,CAAoB,SAApB;AACD,KAJD,MAIO;AACL,WAAK,cAAL,CAAoB,KAApB;AACD,KAV+D,CAYhE;;;AACA,IAAA,OAAO,CAAC,KAAD,CAAP;;AAEA,QAAI,UAAJ,EAAgB;AACd,aAAO,KAAP;AACD,KAFD,MAEO;AACL,MAAA,KAAK,CAAC,OAAN;AACA,aAAO,IAAP;AACD;AACF;AAED;;AAEG;;;AACH,MAAI,UAAJ,GAAc;AACZ,QAAI,KAAK,WAAL,IAAoB,IAAxB,EAA8B;AAC5B,WAAK,WAAL,GAAmB,CAAnB;AACD;;AACD,WAAO,KAAK,WAAZ;AACD;;AAES,EAAA,mBAAmB,GAAA;AAC3B,SAAK,WAAL,GAAmB,KAAK,UAAL,GAAkB,CAArC;AACD;AAED;;;;;;;;;;;;AAYG;;;AACH,EAAA,gBAAgB,CAAC,CAAD,EAAkB,OAAlB,EAAsC;AAEpD,WAAO,aAAa,CAAC,CAAD,EAAI,OAAJ,CAApB;AACD;AAYD;;AAEG;;;AACH,EAAA,OAAO,GAAA;AACL,QAAI,KAAK,WAAL,IAAoB,IAAxB,EAA8B;AAC5B,MAAA,OAAO,CAAC,KAAK,WAAN,CAAP;AACD;AACF;;AAED,QAAM,cAAN,GAAoB;AAClB,QAAI,KAAK,WAAL,IAAoB,IAAxB,EAA8B;AAC5B,WAAK,WAAL,GAAmB,CAAnB;AACD;;AACD,WAAO;AACL,MAAA,IAAI,EAAE,MADD;AAEL;AACA,MAAA,MAAM,EAAE,MAAM,CAAC,KAAK,WAAN,EAAmB,OAAnB;AAHT,KAAP;AAKD;;AAED,QAAM,UAAN,GAAgB;AACd,UAAM,IAAI,KAAJ,CAAU,yDAAV,CAAN;AACD;;AAED,QAAM,UAAN,CAAiB,YAAjB,EAA4C;AAC1C,UAAM,IAAI,KAAJ,CACF,2DAAA,GACA,GAAG,KAAK,YAAL,EAAmB,EAFpB,CAAN;AAGD;AAED;;;;;;AAMG;;;AACO,QAAM,iBAAN,CAAwB,YAAxB,EAAmD;AAE3D,SAAK,WAAL,GAAmB,CAAC,MAAM,YAAY,CAAC,CAAD,CAAZ,CAAgB,MAAhB,CAAuB,IAAvB,EAAP,EAAsC,CAAtC,CAAnB;AACA,WAAO,YAAY,CAAC,KAAb,CAAmB,CAAnB,CAAP;AACD;;AA3HiD;AA8HpD,MAAM,CAAC,cAAP,CAAsB,SAAtB,EAAiC,MAAM,CAAC,WAAxC,EAAqD;AACnD,EAAA,KAAK,EAAG,QAAD,IAAwB;AAC7B,WAAO,QAAQ,CAAC,QAAT,IAAqB,IAArB,IAA6B,QAAQ,CAAC,gBAAT,IAA6B,IAA1D,IACH,QAAQ,CAAC,cAAT,IAA2B,IAD/B;AAED;AAJkD,CAArD","sourceRoot":"","sourcesContent":["/**\r\n * @license\r\n * Copyright 2018 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\r\nimport { dispose } from '../globals';\r\nimport { variableGrads } from '../gradients';\r\nimport { scalar } from '../ops/ops';\r\nimport { Serializable } from '../serialization';\r\n/** @doc {heading: 'Training', subheading: 'Classes', namespace: 'train'} */\r\nexport class Optimizer extends Serializable {\r\n    /**\r\n     * Executes `f()` and minimizes the scalar output of `f()` by computing\r\n     * gradients of y with respect to the list of trainable variables provided by\r\n     * `varList`. If no list is provided, it defaults to all trainable variables.\r\n     *\r\n     * @param f The function to execute and whose output to minimize.\r\n     * @param returnCost Whether to return the scalar cost value produced by\r\n     * executing `f()`.\r\n     * @param varList An optional list of variables to update. If specified, only\r\n     * the trainable variables in varList will be updated by minimize. Defaults to\r\n     * all trainable variables.\r\n     *\r\n     * @doc {heading: 'Training', subheading: 'Optimizers'}\r\n     */\r\n    minimize(f, returnCost = false, varList) {\r\n        const { value, grads } = this.computeGradients(f, varList);\r\n        if (varList != null) {\r\n            const gradArray = varList.map(v => ({ name: v.name, tensor: grads[v.name] }));\r\n            this.applyGradients(gradArray);\r\n        }\r\n        else {\r\n            this.applyGradients(grads);\r\n        }\r\n        // Dispose gradients.\r\n        dispose(grads);\r\n        if (returnCost) {\r\n            return value;\r\n        }\r\n        else {\r\n            value.dispose();\r\n            return null;\r\n        }\r\n    }\r\n    /**\r\n     * The number of iterations that this optimizer instance has been invoked for.\r\n     */\r\n    get iterations() {\r\n        if (this.iterations_ == null) {\r\n            this.iterations_ = 0;\r\n        }\r\n        return this.iterations_;\r\n    }\r\n    incrementIterations() {\r\n        this.iterations_ = this.iterations + 1;\r\n    }\r\n    /**\r\n     * Executes f() and computes the gradient of the scalar output of f() with\r\n     * respect to the list of trainable variables provided by `varList`. If no\r\n     * list is provided, it defaults to all trainable variables.\r\n     *\r\n     * @param f The function to execute and whose output to use for computing\r\n     * gradients with respect to variables.\r\n     * @param varList An optional list of variables to compute gradients with\r\n     * respect to. If specified, only the trainable variables in varList will have\r\n     * gradients computed with respect to. Defaults to all trainable variables.\r\n     *\r\n     * @doc {heading: 'Training', subheading: 'Optimizers'}\r\n     */\r\n    computeGradients(f, varList) {\r\n        return variableGrads(f, varList);\r\n    }\r\n    /**\r\n     * Dispose the variables (if any) owned by this optimizer instance.\r\n     */\r\n    dispose() {\r\n        if (this.iterations_ != null) {\r\n            dispose(this.iterations_);\r\n        }\r\n    }\r\n    async saveIterations() {\r\n        if (this.iterations_ == null) {\r\n            this.iterations_ = 0;\r\n        }\r\n        return {\r\n            name: 'iter',\r\n            // TODO(cais): Use 'int64' type when available.\r\n            tensor: scalar(this.iterations_, 'int32')\r\n        };\r\n    }\r\n    async getWeights() {\r\n        throw new Error('getWeights() is not implemented for this optimizer yet.');\r\n    }\r\n    async setWeights(weightValues) {\r\n        throw new Error(`setWeights() is not implemented for this optimizer class ` +\r\n            `${this.getClassName()}`);\r\n    }\r\n    /**\r\n     * Extract the first element of the weight values and set it\r\n     * as the iterations counter variable of this instance of optimizer.\r\n     *\r\n     * @param weightValues\r\n     * @returns Weight values with the first element consumed and excluded.\r\n     */\r\n    async extractIterations(weightValues) {\r\n        this.iterations_ = (await weightValues[0].tensor.data())[0];\r\n        return weightValues.slice(1);\r\n    }\r\n}\r\nObject.defineProperty(Optimizer, Symbol.hasInstance, {\r\n    value: (instance) => {\r\n        return instance.minimize != null && instance.computeGradients != null &&\r\n            instance.applyGradients != null;\r\n    }\r\n});\r\n//# sourceMappingURL=optimizer.js.map"]},"metadata":{},"sourceType":"module"}