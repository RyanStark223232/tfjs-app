{"ast":null,"code":"/**\r\n * @license\r\n * Copyright 2018 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\nimport { customGrad } from '../gradients';\nimport { convertToTensor } from '../tensor_util_env';\nimport { mul } from './mul';\nimport { neg } from './neg';\nimport { op } from './operation';\nimport { sigmoid } from './sigmoid';\nimport { softplus } from './softplus';\n/**\r\n * Computes log sigmoid of the input `tf.Tensor` element-wise:\r\n * `logSigmoid(x)`. For numerical stability, we use `-tf.softplus(-x)`.\r\n *\r\n * ```js\r\n * const x = tf.tensor1d([0, 1, -1, .7]);\r\n *\r\n * x.logSigmoid().print();  // or tf.logSigmoid(x)\r\n * ```\r\n * @param x The input tensor.\r\n *\r\n * @doc {heading: 'Operations', subheading: 'Basic math'}\r\n */\n\nfunction logSigmoid_(x) {\n  var $x = convertToTensor(x, 'x', 'logSigmoid'); // Use a custom gradient to maintain previous implementation.\n  // There is no LogSigmoid kernel in TF so we can't use engine.runKernel\n  // directly\n\n  var customOp = customGrad(function (x) {\n    // TODO(yassogba) we can remove the chained softplus call here only\n    // after backends have modualrized softplus at which point we can call\n    // engine runKernel(..., Sotfplus, ...) directly.\n    var value = neg(softplus(neg(x)));\n\n    var gradFunc = function gradFunc(dy) {\n      var derX = mul(dy, sigmoid(neg(x)));\n      return derX;\n    };\n\n    return {\n      value: value,\n      gradFunc: gradFunc\n    };\n  });\n  return customOp($x);\n}\n\nexport var logSigmoid = op({\n  logSigmoid_: logSigmoid_\n});","map":{"version":3,"sources":["../../src/ops/log_sigmoid.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,UAAR,QAAyB,cAAzB;AAEA,SAAQ,eAAR,QAA8B,oBAA9B;AAGA,SAAQ,GAAR,QAAkB,OAAlB;AACA,SAAQ,GAAR,QAAkB,OAAlB;AACA,SAAQ,EAAR,QAAiB,aAAjB;AACA,SAAQ,OAAR,QAAsB,WAAtB;AACA,SAAQ,QAAR,QAAuB,YAAvB;AAEA;;;;;;;;;;;;AAYG;;AACH,SAAS,WAAT,CAAuC,CAAvC,EAAsD;AACpD,MAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,YAAT,CAA1B,CADoD,CAGpD;AACA;AACA;;AACA,MAAM,QAAQ,GAAG,UAAU,CAAC,UAAC,CAAD,EAAc;AACxC;AACA;AACA;AACA,QAAM,KAAK,GAAG,GAAG,CAAC,QAAQ,CAAC,GAAG,CAAC,CAAD,CAAJ,CAAT,CAAjB;;AAEA,QAAM,QAAQ,GAAG,SAAX,QAAW,CAAC,EAAD,EAAU;AACzB,UAAM,IAAI,GAAG,GAAG,CAAC,EAAD,EAAK,OAAO,CAAC,GAAG,CAAC,CAAD,CAAJ,CAAZ,CAAhB;AACA,aAAO,IAAP;AACD,KAHD;;AAIA,WAAO;AAAC,MAAA,KAAK,EAAL,KAAD;AAAQ,MAAA,QAAQ,EAAR;AAAR,KAAP;AACD,GAX0B,CAA3B;AAaA,SAAO,QAAQ,CAAC,EAAD,CAAf;AACD;;AACD,OAAO,IAAM,UAAU,GAAG,EAAE,CAAC;AAAC,EAAA,WAAW,EAAX;AAAD,CAAD,CAArB","sourceRoot":"","sourcesContent":["/**\r\n * @license\r\n * Copyright 2018 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\r\nimport { customGrad } from '../gradients';\r\nimport { convertToTensor } from '../tensor_util_env';\r\nimport { mul } from './mul';\r\nimport { neg } from './neg';\r\nimport { op } from './operation';\r\nimport { sigmoid } from './sigmoid';\r\nimport { softplus } from './softplus';\r\n/**\r\n * Computes log sigmoid of the input `tf.Tensor` element-wise:\r\n * `logSigmoid(x)`. For numerical stability, we use `-tf.softplus(-x)`.\r\n *\r\n * ```js\r\n * const x = tf.tensor1d([0, 1, -1, .7]);\r\n *\r\n * x.logSigmoid().print();  // or tf.logSigmoid(x)\r\n * ```\r\n * @param x The input tensor.\r\n *\r\n * @doc {heading: 'Operations', subheading: 'Basic math'}\r\n */\r\nfunction logSigmoid_(x) {\r\n    const $x = convertToTensor(x, 'x', 'logSigmoid');\r\n    // Use a custom gradient to maintain previous implementation.\r\n    // There is no LogSigmoid kernel in TF so we can't use engine.runKernel\r\n    // directly\r\n    const customOp = customGrad((x) => {\r\n        // TODO(yassogba) we can remove the chained softplus call here only\r\n        // after backends have modualrized softplus at which point we can call\r\n        // engine runKernel(..., Sotfplus, ...) directly.\r\n        const value = neg(softplus(neg(x)));\r\n        const gradFunc = (dy) => {\r\n            const derX = mul(dy, sigmoid(neg(x)));\r\n            return derX;\r\n        };\r\n        return { value, gradFunc };\r\n    });\r\n    return customOp($x);\r\n}\r\nexport const logSigmoid = op({ logSigmoid_ });\r\n//# sourceMappingURL=log_sigmoid.js.map"]},"metadata":{},"sourceType":"module"}