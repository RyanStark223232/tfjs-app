{"ast":null,"code":"import _slicedToArray from \"C:/Users/wongh/Documents/GitHub/tfjs-app/node_modules/@babel/runtime/helpers/esm/slicedToArray\";\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\nimport { customGrad } from '../../gradients';\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { add } from '../add';\nimport { expandShapeToKeepDim } from '../axis_util';\nimport { cast } from '../cast';\nimport { div } from '../div';\nimport { exp } from '../exp';\nimport { logSumExp } from '../log_sum_exp';\nimport { Reduction } from '../loss_ops_utils';\nimport { mul } from '../mul';\nimport { neg } from '../neg';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\nimport { scalar } from '../scalar';\nimport { sub } from '../sub';\nimport { sum } from '../sum';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n/**\r\n * Computes softmax cross entropy between logits and labels.\r\n *\r\n * Measures the probability error in discrete classification tasks in which\r\n * the classes are mutually exclusive (each entry is in exactly one class).\r\n * For example, each CIFAR-10 image is labeled with one and only one label: an\r\n * image can be a dog or a truck, but not both.\r\n *\r\n * `NOTE`: While the classes are mutually exclusive, their probabilities need\r\n * not be. All that is required is that each row of labels is a valid\r\n * probability distribution. If they are not, the computation of the gradient\r\n * will be incorrect.\r\n *\r\n * `WARNING`: This op expects unscaled logits, since it performs a softmax on\r\n * logits internally for efficiency. Do not call this op with the output of\r\n * softmax, as it will produce incorrect results.\r\n *\r\n * logits and labels must have the same shape, e.g. [batch_size, num_classes]\r\n * and the same dtype.\r\n * @param labels The labels array.\r\n * @param logits The logits array.\r\n * @param dim The dimension softmax would be performed on. Defaults to `-1`\r\n *     which indicates the last dimension.\r\n */\n\nfunction softmaxCrossEntropyWithLogits_(labels, logits) {\n  var dim = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : -1;\n\n  if (dim === -1) {\n    dim = logits.rank - 1;\n  }\n\n  if (dim !== logits.rank - 1) {\n    throw Error(\"Softmax cross entropy along a non-last dimension is not yet \" + \"supported. Labels / logits was rank \".concat(logits.rank, \" \") + \"and dim was \".concat(dim));\n  } // Use a custom gradient for numerical stability.\n\n\n  var customOp = customGrad(function (labels, logits, save) {\n    // Reference:\n    //   1. http://cs231n.github.io/linear-classify/#softmax\n    //   2. https://blog.feedly.com/tricks-of-the-trade-logsumexp/\n    var keepDims = true;\n    var lse = logSumExp(logits, [dim], keepDims);\n    var logResult = sub(cast(logits, 'float32'), lse);\n    save([labels, logResult]);\n    var costVector = neg(mul(logResult, labels));\n    var value = sum(costVector, [dim]);\n\n    var gradFunc = function gradFunc(dy, saved) {\n      var _saved = _slicedToArray(saved, 2),\n          labels = _saved[0],\n          logResult = _saved[1];\n\n      var dyShape = expandShapeToKeepDim(dy.shape, [dim]);\n      return [mul(reshape(dy, dyShape), sub(cast(labels, 'float32'), exp(logResult))), mul(reshape(dy, dyShape), sub(exp(logResult), cast(labels, 'float32')))];\n    };\n\n    return {\n      value: value,\n      gradFunc: gradFunc\n    };\n  });\n  return customOp(labels, logits);\n}\n/**\r\n * Computes the softmax cross entropy loss between two tensors.\r\n *\r\n * If labelSmoothing is nonzero, smooth the labels towards 1/2:\r\n *\r\n *   newOnehotLabels = onehotLabels * (1 - labelSmoothing)\r\n *                         + labelSmoothing / numClasses\r\n *\r\n * @param onehotLabels One hot encoded labels\r\n *    [batch_size, num_classes], same dimensions as 'predictions'.\r\n * @param logits The predicted outputs.\r\n * @param weights Tensor whose rank is either 0, or 1, and must be\r\n *    broadcastable to `loss`  of shape [batch_size]\r\n * @param labelSmoothing If greater than 0, then smooth the labels.\r\n * @param reduction Type of reduction to apply to loss. Should be of type\r\n *    `Reduction`\r\n *\r\n * @doc { heading: 'Training', subheading: 'Losses', namespace: 'losses' }\r\n */\n\n\nfunction softmaxCrossEntropy_(onehotLabels, logits, weights) {\n  var labelSmoothing = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : 0;\n  var reduction = arguments.length > 4 && arguments[4] !== undefined ? arguments[4] : Reduction.SUM_BY_NONZERO_WEIGHTS;\n  var $onehotLabels = convertToTensor(onehotLabels, 'onehotLabels', 'softmaxCrossEntropy');\n  var $logits = convertToTensor(logits, 'logits', 'softmaxCrossEntropy');\n  var $weights = null;\n\n  if (weights != null) {\n    $weights = convertToTensor(weights, 'weights', 'softmaxCrossEntropy');\n  }\n\n  assertShapesMatch($onehotLabels.shape, $logits.shape, 'Error in softmaxCrossEntropy: ');\n\n  if (labelSmoothing > 0) {\n    var labelSmoothingScalar = scalar(labelSmoothing);\n    var one = scalar(1);\n    var numClasses = scalar($onehotLabels.shape[1]);\n    $onehotLabels = add(mul($onehotLabels, sub(one, labelSmoothingScalar)), div(labelSmoothingScalar, numClasses));\n  }\n\n  var losses = softmaxCrossEntropyWithLogits_($onehotLabels, $logits);\n  return computeWeightedLoss(losses, $weights, reduction);\n}\n\nexport var softmaxCrossEntropy = op({\n  softmaxCrossEntropy_: softmaxCrossEntropy_\n});","map":{"version":3,"sources":["../../../src/ops/losses/softmax_cross_entropy.ts"],"names":[],"mappings":";;AAAA;;;;;;;;;;;;;;;AAeG;AACH,SAAQ,UAAR,QAAyB,iBAAzB;AAGA,SAAQ,eAAR,QAA8B,uBAA9B;AAEA,SAAQ,iBAAR,QAAgC,YAAhC;AACA,SAAQ,GAAR,QAAkB,QAAlB;AACA,SAAQ,oBAAR,QAAmC,cAAnC;AACA,SAAQ,IAAR,QAAmB,SAAnB;AACA,SAAQ,GAAR,QAAkB,QAAlB;AACA,SAAQ,GAAR,QAAkB,QAAlB;AACA,SAAQ,SAAR,QAAwB,gBAAxB;AACA,SAAQ,SAAR,QAAwB,mBAAxB;AACA,SAAQ,GAAR,QAAkB,QAAlB;AACA,SAAQ,GAAR,QAAkB,QAAlB;AACA,SAAQ,EAAR,QAAiB,cAAjB;AACA,SAAQ,OAAR,QAAsB,YAAtB;AACA,SAAQ,MAAR,QAAqB,WAArB;AACA,SAAQ,GAAR,QAAkB,QAAlB;AACA,SAAQ,GAAR,QAAkB,QAAlB;AAEA,SAAQ,mBAAR,QAAkC,yBAAlC;AAEA;;;;;;;;;;;;;;;;;;;;;;;AAuBG;;AACH,SAAS,8BAAT,CACI,MADJ,EACe,MADf,EACkC;AAAA,MAAR,GAAQ,uEAAF,CAAC,CAAC;;AAChC,MAAI,GAAG,KAAK,CAAC,CAAb,EAAgB;AACd,IAAA,GAAG,GAAG,MAAM,CAAC,IAAP,GAAc,CAApB;AACD;;AAED,MAAI,GAAG,KAAK,MAAM,CAAC,IAAP,GAAc,CAA1B,EAA6B;AAC3B,UAAM,KAAK,CACP,+GACuC,MAAM,CAAC,IAD9C,+BAEe,GAFf,CADO,CAAX;AAID,GAV+B,CAWhC;;;AACA,MAAM,QAAQ,GACV,UAAU,CAAC,UAAC,MAAD,EAAiB,MAAjB,EAAiC,IAAjC,EAAuD;AAChE;AACA;AACA;AACA,QAAM,QAAQ,GAAG,IAAjB;AACA,QAAM,GAAG,GAAG,SAAS,CAAC,MAAD,EAAS,CAAC,GAAD,CAAT,EAAgB,QAAhB,CAArB;AACA,QAAM,SAAS,GAAG,GAAG,CAAC,IAAI,CAAC,MAAD,EAAS,SAAT,CAAL,EAA0B,GAA1B,CAArB;AACA,IAAA,IAAI,CAAC,CAAC,MAAD,EAAS,SAAT,CAAD,CAAJ;AAEA,QAAM,UAAU,GAAG,GAAG,CAAC,GAAG,CAAC,SAAD,EAAY,MAAZ,CAAJ,CAAtB;AACA,QAAM,KAAK,GAAM,GAAG,CAAC,UAAD,EAAa,CAAC,GAAD,CAAb,CAApB;;AAEA,QAAM,QAAQ,GAAG,SAAX,QAAW,CAAC,EAAD,EAAQ,KAAR,EAA2B;AAAA,kCACd,KADc;AAAA,UACnC,MADmC;AAAA,UAC3B,SAD2B;;AAE1C,UAAM,OAAO,GAAG,oBAAoB,CAAC,EAAE,CAAC,KAAJ,EAAW,CAAC,GAAD,CAAX,CAApC;AACA,aAAO,CACL,GAAG,CAAC,OAAO,CAAC,EAAD,EAAK,OAAL,CAAR,EACC,GAAG,CAAC,IAAI,CAAC,MAAD,EAAS,SAAT,CAAL,EAA0B,GAAG,CAAC,SAAD,CAA7B,CADJ,CADE,EAGL,GAAG,CAAC,OAAO,CAAC,EAAD,EAAK,OAAL,CAAR,EACC,GAAG,CAAC,GAAG,CAAC,SAAD,CAAJ,EAAiB,IAAI,CAAC,MAAD,EAAS,SAAT,CAArB,CADJ,CAHE,CAAP;AAMD,KATD;;AAUA,WAAO;AAAC,MAAA,KAAK,EAAL,KAAD;AAAQ,MAAA,QAAQ,EAAR;AAAR,KAAP;AACD,GAvBS,CADd;AA0BA,SAAO,QAAQ,CAAC,MAAD,EAAS,MAAT,CAAf;AACD;AAED;;;;;;;;;;;;;;;;;;AAkBG;;;AACH,SAAS,oBAAT,CACI,YADJ,EACgC,MADhC,EAEI,OAFJ,EAGgD;AAAA,MADf,cACe,uEADE,CACF;AAAA,MAA5C,SAA4C,uEAAhC,SAAS,CAAC,sBAAsB;AAC9C,MAAI,aAAa,GACb,eAAe,CAAC,YAAD,EAAe,cAAf,EAA+B,qBAA/B,CADnB;AAEA,MAAM,OAAO,GAAG,eAAe,CAAC,MAAD,EAAS,QAAT,EAAmB,qBAAnB,CAA/B;AACA,MAAI,QAAQ,GAAW,IAAvB;;AAEA,MAAI,OAAO,IAAI,IAAf,EAAqB;AACnB,IAAA,QAAQ,GAAG,eAAe,CAAC,OAAD,EAAU,SAAV,EAAqB,qBAArB,CAA1B;AACD;;AAED,EAAA,iBAAiB,CACb,aAAa,CAAC,KADD,EACQ,OAAO,CAAC,KADhB,EACuB,gCADvB,CAAjB;;AAGA,MAAI,cAAc,GAAG,CAArB,EAAwB;AACtB,QAAM,oBAAoB,GAAG,MAAM,CAAC,cAAD,CAAnC;AACA,QAAM,GAAG,GAAG,MAAM,CAAC,CAAD,CAAlB;AACA,QAAM,UAAU,GAAG,MAAM,CAAC,aAAa,CAAC,KAAd,CAAoB,CAApB,CAAD,CAAzB;AAEA,IAAA,aAAa,GACT,GAAG,CAAC,GAAG,CAAC,aAAD,EAAgB,GAAG,CAAC,GAAD,EAAM,oBAAN,CAAnB,CAAJ,EACC,GAAG,CAAC,oBAAD,EAAuB,UAAvB,CADJ,CADP;AAGD;;AAED,MAAM,MAAM,GAAG,8BAA8B,CAAC,aAAD,EAAgB,OAAhB,CAA7C;AAEA,SAAO,mBAAmB,CAAC,MAAD,EAAS,QAAT,EAAmB,SAAnB,CAA1B;AACD;;AAED,OAAO,IAAM,mBAAmB,GAAG,EAAE,CAAC;AAAC,EAAA,oBAAoB,EAApB;AAAD,CAAD,CAA9B","sourceRoot":"","sourcesContent":["/**\r\n * @license\r\n * Copyright 2020 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\r\nimport { customGrad } from '../../gradients';\r\nimport { convertToTensor } from '../../tensor_util_env';\r\nimport { assertShapesMatch } from '../../util';\r\nimport { add } from '../add';\r\nimport { expandShapeToKeepDim } from '../axis_util';\r\nimport { cast } from '../cast';\r\nimport { div } from '../div';\r\nimport { exp } from '../exp';\r\nimport { logSumExp } from '../log_sum_exp';\r\nimport { Reduction } from '../loss_ops_utils';\r\nimport { mul } from '../mul';\r\nimport { neg } from '../neg';\r\nimport { op } from '../operation';\r\nimport { reshape } from '../reshape';\r\nimport { scalar } from '../scalar';\r\nimport { sub } from '../sub';\r\nimport { sum } from '../sum';\r\nimport { computeWeightedLoss } from './compute_weighted_loss';\r\n/**\r\n * Computes softmax cross entropy between logits and labels.\r\n *\r\n * Measures the probability error in discrete classification tasks in which\r\n * the classes are mutually exclusive (each entry is in exactly one class).\r\n * For example, each CIFAR-10 image is labeled with one and only one label: an\r\n * image can be a dog or a truck, but not both.\r\n *\r\n * `NOTE`: While the classes are mutually exclusive, their probabilities need\r\n * not be. All that is required is that each row of labels is a valid\r\n * probability distribution. If they are not, the computation of the gradient\r\n * will be incorrect.\r\n *\r\n * `WARNING`: This op expects unscaled logits, since it performs a softmax on\r\n * logits internally for efficiency. Do not call this op with the output of\r\n * softmax, as it will produce incorrect results.\r\n *\r\n * logits and labels must have the same shape, e.g. [batch_size, num_classes]\r\n * and the same dtype.\r\n * @param labels The labels array.\r\n * @param logits The logits array.\r\n * @param dim The dimension softmax would be performed on. Defaults to `-1`\r\n *     which indicates the last dimension.\r\n */\r\nfunction softmaxCrossEntropyWithLogits_(labels, logits, dim = -1) {\r\n    if (dim === -1) {\r\n        dim = logits.rank - 1;\r\n    }\r\n    if (dim !== logits.rank - 1) {\r\n        throw Error(`Softmax cross entropy along a non-last dimension is not yet ` +\r\n            `supported. Labels / logits was rank ${logits.rank} ` +\r\n            `and dim was ${dim}`);\r\n    }\r\n    // Use a custom gradient for numerical stability.\r\n    const customOp = customGrad((labels, logits, save) => {\r\n        // Reference:\r\n        //   1. http://cs231n.github.io/linear-classify/#softmax\r\n        //   2. https://blog.feedly.com/tricks-of-the-trade-logsumexp/\r\n        const keepDims = true;\r\n        const lse = logSumExp(logits, [dim], keepDims);\r\n        const logResult = sub(cast(logits, 'float32'), lse);\r\n        save([labels, logResult]);\r\n        const costVector = neg(mul(logResult, labels));\r\n        const value = sum(costVector, [dim]);\r\n        const gradFunc = (dy, saved) => {\r\n            const [labels, logResult] = saved;\r\n            const dyShape = expandShapeToKeepDim(dy.shape, [dim]);\r\n            return [\r\n                mul(reshape(dy, dyShape), sub(cast(labels, 'float32'), exp(logResult))),\r\n                mul(reshape(dy, dyShape), sub(exp(logResult), cast(labels, 'float32'))),\r\n            ];\r\n        };\r\n        return { value, gradFunc };\r\n    });\r\n    return customOp(labels, logits);\r\n}\r\n/**\r\n * Computes the softmax cross entropy loss between two tensors.\r\n *\r\n * If labelSmoothing is nonzero, smooth the labels towards 1/2:\r\n *\r\n *   newOnehotLabels = onehotLabels * (1 - labelSmoothing)\r\n *                         + labelSmoothing / numClasses\r\n *\r\n * @param onehotLabels One hot encoded labels\r\n *    [batch_size, num_classes], same dimensions as 'predictions'.\r\n * @param logits The predicted outputs.\r\n * @param weights Tensor whose rank is either 0, or 1, and must be\r\n *    broadcastable to `loss`  of shape [batch_size]\r\n * @param labelSmoothing If greater than 0, then smooth the labels.\r\n * @param reduction Type of reduction to apply to loss. Should be of type\r\n *    `Reduction`\r\n *\r\n * @doc { heading: 'Training', subheading: 'Losses', namespace: 'losses' }\r\n */\r\nfunction softmaxCrossEntropy_(onehotLabels, logits, weights, labelSmoothing = 0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\r\n    let $onehotLabels = convertToTensor(onehotLabels, 'onehotLabels', 'softmaxCrossEntropy');\r\n    const $logits = convertToTensor(logits, 'logits', 'softmaxCrossEntropy');\r\n    let $weights = null;\r\n    if (weights != null) {\r\n        $weights = convertToTensor(weights, 'weights', 'softmaxCrossEntropy');\r\n    }\r\n    assertShapesMatch($onehotLabels.shape, $logits.shape, 'Error in softmaxCrossEntropy: ');\r\n    if (labelSmoothing > 0) {\r\n        const labelSmoothingScalar = scalar(labelSmoothing);\r\n        const one = scalar(1);\r\n        const numClasses = scalar($onehotLabels.shape[1]);\r\n        $onehotLabels =\r\n            add(mul($onehotLabels, sub(one, labelSmoothingScalar)), div(labelSmoothingScalar, numClasses));\r\n    }\r\n    const losses = softmaxCrossEntropyWithLogits_($onehotLabels, $logits);\r\n    return computeWeightedLoss(losses, $weights, reduction);\r\n}\r\nexport const softmaxCrossEntropy = op({ softmaxCrossEntropy_ });\r\n//# sourceMappingURL=softmax_cross_entropy.js.map"]},"metadata":{},"sourceType":"module"}