{"ast":null,"code":"import _regeneratorRuntime from \"C:/Users/wongh/Documents/GitHub/tfjs-app/node_modules/@babel/runtime/regenerator\";\nimport _asyncToGenerator from \"C:/Users/wongh/Documents/GitHub/tfjs-app/node_modules/@babel/runtime/helpers/esm/asyncToGenerator\";\nimport _classCallCheck from \"C:/Users/wongh/Documents/GitHub/tfjs-app/node_modules/@babel/runtime/helpers/esm/classCallCheck\";\nimport _createClass from \"C:/Users/wongh/Documents/GitHub/tfjs-app/node_modules/@babel/runtime/helpers/esm/createClass\";\n\n/**\r\n * @license\r\n * Copyright 2018 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\nimport { io, Tensor } from '@tensorflow/tfjs-core';\nimport { OperationMapper } from '../operations/operation_mapper';\nimport { GraphExecutor } from './graph_executor';\nexport var TFHUB_SEARCH_PARAM = '?tfjs-format=file';\nexport var DEFAULT_MODEL_NAME = 'model.json';\n/**\r\n * A `tf.GraphModel` is a directed, acyclic graph built from a\r\n * SavedModel GraphDef and allows inference execution.\r\n *\r\n * A `tf.GraphModel` can only be created by loading from a model converted from\r\n * a [TensorFlow SavedModel](https://www.tensorflow.org/guide/saved_model) using\r\n * the command line converter tool and loaded via `tf.loadGraphModel`.\r\n *\r\n * @doc {heading: 'Models', subheading: 'Classes'}\r\n */\n\nexport var GraphModel = /*#__PURE__*/function () {\n  /**\r\n   * @param modelUrl url for the model, or an `io.IOHandler`.\r\n   * @param weightManifestUrl url for the weight file generated by\r\n   * scripts/convert.py script.\r\n   * @param requestOption options for Request, which allows to send credentials\r\n   * and custom headers.\r\n   * @param onProgress Optional, progress callback function, fired periodically\r\n   * before the load is completed.\r\n   */\n  function GraphModel(modelUrl) {\n    var loadOptions = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : {};\n\n    _classCallCheck(this, GraphModel);\n\n    this.modelUrl = modelUrl;\n    this.loadOptions = loadOptions;\n    this.version = 'n/a';\n\n    if (loadOptions == null) {\n      this.loadOptions = {};\n    }\n  } // Returns the version information for the tensorflow model GraphDef.\n\n\n  _createClass(GraphModel, [{\n    key: \"findIOHandler\",\n    value: function findIOHandler() {\n      var path = this.modelUrl;\n\n      if (path.load != null) {\n        // Path is an IO Handler.\n        this.handler = path;\n      } else if (this.loadOptions.requestInit != null) {\n        this.handler = io.browserHTTPRequest(path, this.loadOptions);\n      } else {\n        var handlers = io.getLoadHandlers(path, this.loadOptions);\n\n        if (handlers.length === 0) {\n          // For backward compatibility: if no load handler can be found,\n          // assume it is a relative http path.\n          handlers.push(io.browserHTTPRequest(path, this.loadOptions));\n        } else if (handlers.length > 1) {\n          throw new Error(\"Found more than one (\".concat(handlers.length, \") load handlers for \") + \"URL '\".concat([path], \"'\"));\n        }\n\n        this.handler = handlers[0];\n      }\n    }\n    /**\r\n     * Loads the model and weight files, construct the in memory weight map and\r\n     * compile the inference graph.\r\n     */\n\n  }, {\n    key: \"load\",\n    value: function () {\n      var _load = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee() {\n        var artifacts;\n        return _regeneratorRuntime.wrap(function _callee$(_context) {\n          while (1) {\n            switch (_context.prev = _context.next) {\n              case 0:\n                this.findIOHandler();\n\n                if (!(this.handler.load == null)) {\n                  _context.next = 3;\n                  break;\n                }\n\n                throw new Error('Cannot proceed with model loading because the IOHandler provided ' + 'does not have the `load` method implemented.');\n\n              case 3:\n                _context.next = 5;\n                return this.handler.load();\n\n              case 5:\n                artifacts = _context.sent;\n                return _context.abrupt(\"return\", this.loadSync(artifacts));\n\n              case 7:\n              case \"end\":\n                return _context.stop();\n            }\n          }\n        }, _callee, this);\n      }));\n\n      function load() {\n        return _load.apply(this, arguments);\n      }\n\n      return load;\n    }()\n    /**\r\n     * Synchronously construct the in memory weight map and\r\n     * compile the inference graph. Also initialize hashtable if any.\r\n     *\r\n     * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\r\n     */\n\n  }, {\n    key: \"loadSync\",\n    value: function loadSync(artifacts) {\n      this.artifacts = artifacts;\n      var graph = this.artifacts.modelTopology;\n      var signature = {};\n\n      if (this.artifacts.userDefinedMetadata != null) {\n        signature = // tslint:disable-next-line:no-any\n        this.artifacts.userDefinedMetadata.signature;\n      }\n\n      this.version = \"\".concat(graph.versions.producer, \".\").concat(graph.versions.minConsumer);\n      var weightMap = io.decodeWeights(this.artifacts.weightData, this.artifacts.weightSpecs);\n      this.executor = new GraphExecutor(OperationMapper.Instance.transformGraph(graph, signature));\n      this.executor.weightMap = this.convertTensorMapToTensorsMap(weightMap);\n\n      if (artifacts.modelInitializer != null) {\n        var initializer = OperationMapper.Instance.transformGraph(artifacts.modelInitializer);\n        this.initializer = new GraphExecutor(initializer);\n        this.initializer.weightMap = this.executor.weightMap;\n        this.initializer.execute({}, []);\n      }\n\n      return true;\n    }\n    /**\r\n     * Save the configuration and/or weights of the GraphModel.\r\n     *\r\n     * An `IOHandler` is an object that has a `save` method of the proper\r\n     * signature defined. The `save` method manages the storing or\r\n     * transmission of serialized data (\"artifacts\") that represent the\r\n     * model's topology and weights onto or via a specific medium, such as\r\n     * file downloads, local storage, IndexedDB in the web browser and HTTP\r\n     * requests to a server. TensorFlow.js provides `IOHandler`\r\n     * implementations for a number of frequently used saving mediums, such as\r\n     * `tf.io.browserDownloads` and `tf.io.browserLocalStorage`. See `tf.io`\r\n     * for more details.\r\n     *\r\n     * This method also allows you to refer to certain types of `IOHandler`s\r\n     * as URL-like string shortcuts, such as 'localstorage://' and\r\n     * 'indexeddb://'.\r\n     *\r\n     * Example 1: Save `model`'s topology and weights to browser [local\r\n     * storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage);\r\n     * then load it back.\r\n     *\r\n     * ```js\r\n     * const modelUrl =\r\n     *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';\r\n     * const model = await tf.loadGraphModel(modelUrl);\r\n     * const zeros = tf.zeros([1, 224, 224, 3]);\r\n     * model.predict(zeros).print();\r\n     *\r\n     * const saveResults = await model.save('localstorage://my-model-1');\r\n     *\r\n     * const loadedModel = await tf.loadGraphModel('localstorage://my-model-1');\r\n     * console.log('Prediction from loaded model:');\r\n     * model.predict(zeros).print();\r\n     * ```\r\n     *\r\n     * @param handlerOrURL An instance of `IOHandler` or a URL-like,\r\n     * scheme-based string shortcut for `IOHandler`.\r\n     * @param config Options for saving the model.\r\n     * @returns A `Promise` of `SaveResult`, which summarizes the result of\r\n     * the saving, such as byte sizes of the saved artifacts for the model's\r\n     *   topology and weight values.\r\n     *\r\n     * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\r\n     */\n\n  }, {\n    key: \"save\",\n    value: function () {\n      var _save = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee2(handlerOrURL, config) {\n        var handlers;\n        return _regeneratorRuntime.wrap(function _callee2$(_context2) {\n          while (1) {\n            switch (_context2.prev = _context2.next) {\n              case 0:\n                if (!(typeof handlerOrURL === 'string')) {\n                  _context2.next = 9;\n                  break;\n                }\n\n                handlers = io.getSaveHandlers(handlerOrURL);\n\n                if (!(handlers.length === 0)) {\n                  _context2.next = 6;\n                  break;\n                }\n\n                throw new Error(\"Cannot find any save handlers for URL '\".concat(handlerOrURL, \"'\"));\n\n              case 6:\n                if (!(handlers.length > 1)) {\n                  _context2.next = 8;\n                  break;\n                }\n\n                throw new Error(\"Found more than one (\".concat(handlers.length, \") save handlers for \") + \"URL '\".concat(handlerOrURL, \"'\"));\n\n              case 8:\n                handlerOrURL = handlers[0];\n\n              case 9:\n                if (!(handlerOrURL.save == null)) {\n                  _context2.next = 11;\n                  break;\n                }\n\n                throw new Error('GraphModel.save() cannot proceed because the IOHandler ' + 'provided does not have the `save` attribute defined.');\n\n              case 11:\n                return _context2.abrupt(\"return\", handlerOrURL.save(this.artifacts));\n\n              case 12:\n              case \"end\":\n                return _context2.stop();\n            }\n          }\n        }, _callee2, this);\n      }));\n\n      function save(_x, _x2) {\n        return _save.apply(this, arguments);\n      }\n\n      return save;\n    }()\n    /**\r\n     * Execute the inference for the input tensors.\r\n     *\r\n     * @param input The input tensors, when there is single input for the model,\r\n     * inputs param should be a `tf.Tensor`. For models with mutliple inputs,\r\n     * inputs params should be in either `tf.Tensor`[] if the input order is\r\n     * fixed, or otherwise NamedTensorMap format.\r\n     *\r\n     * For model with multiple inputs, we recommend you use NamedTensorMap as the\r\n     * input type, if you use `tf.Tensor`[], the order of the array needs to\r\n     * follow the\r\n     * order of inputNodes array. @see {@link GraphModel.inputNodes}\r\n     *\r\n     * You can also feed any intermediate nodes using the NamedTensorMap as the\r\n     * input type. For example, given the graph\r\n     *    InputNode => Intermediate => OutputNode,\r\n     * you can execute the subgraph Intermediate => OutputNode by calling\r\n     *    model.execute('IntermediateNode' : tf.tensor(...));\r\n     *\r\n     * This is useful for models that uses tf.dynamic_rnn, where the intermediate\r\n     * state needs to be fed manually.\r\n     *\r\n     * For batch inference execution, the tensors for each input need to be\r\n     * concatenated together. For example with mobilenet, the required input shape\r\n     * is [1, 244, 244, 3], which represents the [batch, height, width, channel].\r\n     * If we are provide a batched data of 100 images, the input tensor should be\r\n     * in the shape of [100, 244, 244, 3].\r\n     *\r\n     * @param config Prediction configuration for specifying the batch size and\r\n     * output node names. Currently the batch size option is ignored for graph\r\n     * model.\r\n     *\r\n     * @returns Inference result tensors. The output would be single `tf.Tensor`\r\n     * if model has single output node, otherwise Tensor[] or NamedTensorMap[]\r\n     * will be returned for model with multiple outputs.\r\n     *\r\n     * @doc {heading: 'Models', subheading: 'Classes'}\r\n     */\n\n  }, {\n    key: \"predict\",\n    value: function predict(inputs, config) {\n      return this.execute(inputs, this.outputNodes);\n    }\n  }, {\n    key: \"normalizeInputs\",\n    value: function normalizeInputs(inputs) {\n      if (!(inputs instanceof Tensor) && !Array.isArray(inputs)) {\n        // The input is already a NamedTensorMap.\n        return inputs;\n      }\n\n      inputs = Array.isArray(inputs) ? inputs : [inputs];\n\n      if (inputs.length !== this.inputNodes.length) {\n        throw new Error('Input tensor count mismatch,' + \"the graph model has \".concat(this.inputNodes.length, \" placeholders, \") + \"while there are \".concat(inputs.length, \" input tensors.\"));\n      }\n\n      return this.inputNodes.reduce(function (map, inputName, i) {\n        map[inputName] = inputs[i];\n        return map;\n      }, {});\n    }\n  }, {\n    key: \"normalizeOutputs\",\n    value: function normalizeOutputs(outputs) {\n      outputs = outputs || this.outputNodes;\n      return !Array.isArray(outputs) ? [outputs] : outputs;\n    }\n    /**\r\n     * Executes inference for the model for given input tensors.\r\n     * @param inputs tensor, tensor array or tensor map of the inputs for the\r\n     * model, keyed by the input node names.\r\n     * @param outputs output node name from the Tensorflow model, if no\r\n     * outputs are specified, the default outputs of the model would be used.\r\n     * You can inspect intermediate nodes of the model by adding them to the\r\n     * outputs array.\r\n     *\r\n     * @returns A single tensor if provided with a single output or no outputs\r\n     * are provided and there is only one default output, otherwise return a\r\n     * tensor array. The order of the tensor array is the same as the outputs\r\n     * if provided, otherwise the order of outputNodes attribute of the model.\r\n     *\r\n     * @doc {heading: 'Models', subheading: 'Classes'}\r\n     */\n\n  }, {\n    key: \"execute\",\n    value: function execute(inputs, outputs) {\n      inputs = this.normalizeInputs(inputs);\n      outputs = this.normalizeOutputs(outputs);\n      var result = this.executor.execute(inputs, outputs);\n      return result.length > 1 ? result : result[0];\n    }\n    /**\r\n     * Executes inference for the model for given input tensors in async\r\n     * fashion, use this method when your model contains control flow ops.\r\n     * @param inputs tensor, tensor array or tensor map of the inputs for the\r\n     * model, keyed by the input node names.\r\n     * @param outputs output node name from the Tensorflow model, if no outputs\r\n     * are specified, the default outputs of the model would be used. You can\r\n     * inspect intermediate nodes of the model by adding them to the outputs\r\n     * array.\r\n     *\r\n     * @returns A Promise of single tensor if provided with a single output or\r\n     * no outputs are provided and there is only one default output, otherwise\r\n     * return a tensor map.\r\n     *\r\n     * @doc {heading: 'Models', subheading: 'Classes'}\r\n     */\n\n  }, {\n    key: \"executeAsync\",\n    value: function () {\n      var _executeAsync = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee3(inputs, outputs) {\n        var result;\n        return _regeneratorRuntime.wrap(function _callee3$(_context3) {\n          while (1) {\n            switch (_context3.prev = _context3.next) {\n              case 0:\n                inputs = this.normalizeInputs(inputs);\n                outputs = this.normalizeOutputs(outputs);\n                _context3.next = 4;\n                return this.executor.executeAsync(inputs, outputs);\n\n              case 4:\n                result = _context3.sent;\n                return _context3.abrupt(\"return\", result.length > 1 ? result : result[0]);\n\n              case 6:\n              case \"end\":\n                return _context3.stop();\n            }\n          }\n        }, _callee3, this);\n      }));\n\n      function executeAsync(_x3, _x4) {\n        return _executeAsync.apply(this, arguments);\n      }\n\n      return executeAsync;\n    }()\n  }, {\n    key: \"convertTensorMapToTensorsMap\",\n    value: function convertTensorMapToTensorsMap(map) {\n      return Object.keys(map).reduce(function (newMap, key) {\n        newMap[key] = [map[key]];\n        return newMap;\n      }, {});\n    }\n    /**\r\n     * Releases the memory used by the weight tensors.\r\n     *\r\n     * @doc {heading: 'Models', subheading: 'Classes'}\r\n     */\n\n  }, {\n    key: \"dispose\",\n    value: function dispose() {\n      this.executor.dispose();\n\n      if (this.initializer) {\n        this.initializer.dispose();\n      }\n    }\n  }, {\n    key: \"modelVersion\",\n    get: function get() {\n      return this.version;\n    }\n  }, {\n    key: \"inputNodes\",\n    get: function get() {\n      return this.executor.inputNodes;\n    }\n  }, {\n    key: \"outputNodes\",\n    get: function get() {\n      return this.executor.outputNodes;\n    }\n  }, {\n    key: \"inputs\",\n    get: function get() {\n      return this.executor.inputs;\n    }\n  }, {\n    key: \"outputs\",\n    get: function get() {\n      return this.executor.outputs;\n    }\n  }, {\n    key: \"weights\",\n    get: function get() {\n      return this.executor.weightMap;\n    }\n  }]);\n\n  return GraphModel;\n}();\n/**\r\n * Load a graph model given a URL to the model definition.\r\n *\r\n * Example of loading MobileNetV2 from a URL and making a prediction with a\r\n * zeros input:\r\n *\r\n * ```js\r\n * const modelUrl =\r\n *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';\r\n * const model = await tf.loadGraphModel(modelUrl);\r\n * const zeros = tf.zeros([1, 224, 224, 3]);\r\n * model.predict(zeros).print();\r\n * ```\r\n *\r\n * Example of loading MobileNetV2 from a TF Hub URL and making a prediction with\r\n * a zeros input:\r\n *\r\n * ```js\r\n * const modelUrl =\r\n *    'https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/2';\r\n * const model = await tf.loadGraphModel(modelUrl, {fromTFHub: true});\r\n * const zeros = tf.zeros([1, 224, 224, 3]);\r\n * model.predict(zeros).print();\r\n * ```\r\n * @param modelUrl The url or an `io.IOHandler` that loads the model.\r\n * @param options Options for the HTTP request, which allows to send credentials\r\n *    and custom headers.\r\n *\r\n * @doc {heading: 'Models', subheading: 'Loading'}\r\n */\n\nexport function loadGraphModel(_x5) {\n  return _loadGraphModel.apply(this, arguments);\n}\n\nfunction _loadGraphModel() {\n  _loadGraphModel = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee4(modelUrl) {\n    var options,\n        model,\n        _args4 = arguments;\n    return _regeneratorRuntime.wrap(function _callee4$(_context4) {\n      while (1) {\n        switch (_context4.prev = _context4.next) {\n          case 0:\n            options = _args4.length > 1 && _args4[1] !== undefined ? _args4[1] : {};\n\n            if (!(modelUrl == null)) {\n              _context4.next = 3;\n              break;\n            }\n\n            throw new Error('modelUrl in loadGraphModel() cannot be null. Please provide a url ' + 'or an IOHandler that loads the model');\n\n          case 3:\n            if (options == null) {\n              options = {};\n            }\n\n            if (options.fromTFHub) {\n              if (modelUrl.load == null) {\n                if (!modelUrl.endsWith('/')) {\n                  modelUrl = modelUrl + '/';\n                }\n\n                modelUrl = \"\".concat(modelUrl).concat(DEFAULT_MODEL_NAME).concat(TFHUB_SEARCH_PARAM);\n              }\n            }\n\n            model = new GraphModel(modelUrl, options);\n            _context4.next = 8;\n            return model.load();\n\n          case 8:\n            return _context4.abrupt(\"return\", model);\n\n          case 9:\n          case \"end\":\n            return _context4.stop();\n        }\n      }\n    }, _callee4);\n  }));\n  return _loadGraphModel.apply(this, arguments);\n}","map":{"version":3,"sources":["../../src/executor/graph_model.ts"],"names":[],"mappings":";;;;;AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAwB,EAAxB,EAAgE,MAAhE,QAA6E,uBAA7E;AAIA,SAAQ,eAAR,QAA8B,gCAA9B;AAEA,SAAQ,aAAR,QAA4B,kBAA5B;AAEA,OAAO,IAAM,kBAAkB,GAAG,mBAA3B;AACP,OAAO,IAAM,kBAAkB,GAAG,YAA3B;AACP;;;;;;;;;AASG;;AACH,WAAa,UAAb;AAgCE;;;;;;;;AAQG;AACH,sBACY,QADZ,EAE4C;AAAA,QAAhC,WAAgC,uEAAF,EAAE;;AAAA;;AADhC,SAAA,QAAA,GAAA,QAAA;AACA,SAAA,WAAA,GAAA,WAAA;AAzCJ,SAAA,OAAA,GAAU,KAAV;;AA0CN,QAAI,WAAW,IAAI,IAAnB,EAAyB;AACvB,WAAK,WAAL,GAAmB,EAAnB;AACD;AACF,GA/CH,CAOE;;;AAPF;AAAA;AAAA,oCAiDuB;AACnB,UAAM,IAAI,GAAG,KAAK,QAAlB;;AACA,UAAK,IAAqB,CAAC,IAAtB,IAA8B,IAAnC,EAAyC;AACvC;AACA,aAAK,OAAL,GAAe,IAAf;AACD,OAHD,MAGO,IAAI,KAAK,WAAL,CAAiB,WAAjB,IAAgC,IAApC,EAA0C;AAC/C,aAAK,OAAL,GAAe,EAAE,CAAC,kBAAH,CAAsB,IAAtB,EAAsC,KAAK,WAA3C,CAAf;AACD,OAFM,MAEA;AACL,YAAM,QAAQ,GAAG,EAAE,CAAC,eAAH,CAAmB,IAAnB,EAAmC,KAAK,WAAxC,CAAjB;;AACA,YAAI,QAAQ,CAAC,MAAT,KAAoB,CAAxB,EAA2B;AACzB;AACA;AACA,UAAA,QAAQ,CAAC,IAAT,CAAc,EAAE,CAAC,kBAAH,CAAsB,IAAtB,EAAsC,KAAK,WAA3C,CAAd;AACD,SAJD,MAIO,IAAI,QAAQ,CAAC,MAAT,GAAkB,CAAtB,EAAyB;AAC9B,gBAAM,IAAI,KAAJ,CACF,+BAAwB,QAAQ,CAAC,MAAjC,2CACQ,CAAC,IAAD,CADR,MADE,CAAN;AAGD;;AACD,aAAK,OAAL,GAAe,QAAQ,CAAC,CAAD,CAAvB;AACD;AACF;AAED;;;AAGG;;AA1EL;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AA4EI,qBAAK,aAAL;;AA5EJ,sBA6EQ,KAAK,OAAL,CAAa,IAAb,IAAqB,IA7E7B;AAAA;AAAA;AAAA;;AAAA,sBA8EY,IAAI,KAAJ,CACF,sEACA,8CAFE,CA9EZ;;AAAA;AAAA;AAAA,uBAkF4B,KAAK,OAAL,CAAa,IAAb,EAlF5B;;AAAA;AAkFU,gBAAA,SAlFV;AAAA,iDAoFW,KAAK,QAAL,CAAc,SAAd,CApFX;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;AAAA;AAAA;AAAA;;AAAA;AAAA;AAuFE;;;;;AAKG;;AA5FL;AAAA;AAAA,6BA6FW,SA7FX,EA6FuC;AACnC,WAAK,SAAL,GAAiB,SAAjB;AACA,UAAM,KAAK,GAAG,KAAK,SAAL,CAAe,aAA7B;AACA,UAAI,SAAS,GAAG,EAAhB;;AACA,UAAI,KAAK,SAAL,CAAe,mBAAf,IAAsC,IAA1C,EAAgD;AAC9C,QAAA,SAAS,GAAI;AACR,aAAK,SAAL,CAAe,mBAAf,CAA2C,SADhD;AAGD;;AAED,WAAK,OAAL,aAAkB,KAAK,CAAC,QAAN,CAAe,QAAjC,cAA6C,KAAK,CAAC,QAAN,CAAe,WAA5D;AACA,UAAM,SAAS,GACX,EAAE,CAAC,aAAH,CAAiB,KAAK,SAAL,CAAe,UAAhC,EAA4C,KAAK,SAAL,CAAe,WAA3D,CADJ;AAEA,WAAK,QAAL,GAAgB,IAAI,aAAJ,CACZ,eAAe,CAAC,QAAhB,CAAyB,cAAzB,CAAwC,KAAxC,EAA+C,SAA/C,CADY,CAAhB;AAEA,WAAK,QAAL,CAAc,SAAd,GAA0B,KAAK,4BAAL,CAAkC,SAAlC,CAA1B;;AAEA,UAAI,SAAS,CAAC,gBAAV,IAA8B,IAAlC,EAAwC;AACtC,YAAM,WAAW,GACb,eAAe,CAAC,QAAhB,CAAyB,cAAzB,CAAwC,SAAS,CAAC,gBAAlD,CADJ;AAEA,aAAK,WAAL,GAAmB,IAAI,aAAJ,CAAkB,WAAlB,CAAnB;AACA,aAAK,WAAL,CAAiB,SAAjB,GAA6B,KAAK,QAAL,CAAc,SAA3C;AACA,aAAK,WAAL,CAAiB,OAAjB,CAAyB,EAAzB,EAA6B,EAA7B;AACD;;AAED,aAAO,IAAP;AACD;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA2CG;;AApKL;AAAA;AAAA;AAAA,6FAqKa,YArKb,EAqKgD,MArKhD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBAuKQ,OAAO,YAAP,KAAwB,QAvKhC;AAAA;AAAA;AAAA;;AAwKY,gBAAA,QAxKZ,GAwKuB,EAAE,CAAC,eAAH,CAAmB,YAAnB,CAxKvB;;AAAA,sBAyKU,QAAQ,CAAC,MAAT,KAAoB,CAzK9B;AAAA;AAAA;AAAA;;AAAA,sBA0Kc,IAAI,KAAJ,kDACwC,YADxC,OA1Kd;;AAAA;AAAA,sBA4KiB,QAAQ,CAAC,MAAT,GAAkB,CA5KnC;AAAA;AAAA;AAAA;;AAAA,sBA6Kc,IAAI,KAAJ,CACF,+BAAwB,QAAQ,CAAC,MAAjC,2CACQ,YADR,MADE,CA7Kd;;AAAA;AAiLM,gBAAA,YAAY,GAAG,QAAQ,CAAC,CAAD,CAAvB;;AAjLN;AAAA,sBAmLQ,YAAY,CAAC,IAAb,IAAqB,IAnL7B;AAAA;AAAA;AAAA;;AAAA,sBAoLY,IAAI,KAAJ,CACF,4DACA,sDAFE,CApLZ;;AAAA;AAAA,kDAyLW,YAAY,CAAC,IAAb,CAAkB,KAAK,SAAvB,CAzLX;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;AAAA;AAAA;AAAA;;AAAA;AAAA;AA4LE;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAqCG;;AAjOL;AAAA;AAAA,4BAkOU,MAlOV,EAkOkD,MAlOlD,EAkO6E;AAEzE,aAAO,KAAK,OAAL,CAAa,MAAb,EAAqB,KAAK,WAA1B,CAAP;AACD;AArOH;AAAA;AAAA,oCAuO0B,MAvO1B,EAwOwC;AACpC,UAAI,EAAE,MAAM,YAAY,MAApB,KAA+B,CAAC,KAAK,CAAC,OAAN,CAAc,MAAd,CAApC,EAA2D;AACzD;AACA,eAAO,MAAP;AACD;;AACD,MAAA,MAAM,GAAG,KAAK,CAAC,OAAN,CAAc,MAAd,IAAwB,MAAxB,GAAiC,CAAC,MAAD,CAA1C;;AACA,UAAI,MAAM,CAAC,MAAP,KAAkB,KAAK,UAAL,CAAgB,MAAtC,EAA8C;AAC5C,cAAM,IAAI,KAAJ,CACF,+DACuB,KAAK,UAAL,CAAgB,MADvC,iDAEmB,MAAM,CAAC,MAF1B,oBADE,CAAN;AAID;;AACD,aAAO,KAAK,UAAL,CAAgB,MAAhB,CAAuB,UAAC,GAAD,EAAM,SAAN,EAAiB,CAAjB,EAAsB;AAClD,QAAA,GAAG,CAAC,SAAD,CAAH,GAAkB,MAAmB,CAAC,CAAD,CAArC;AACA,eAAO,GAAP;AACD,OAHM,EAGJ,EAHI,CAAP;AAID;AAxPH;AAAA;AAAA,qCA0P2B,OA1P3B,EA0PmD;AAC/C,MAAA,OAAO,GAAG,OAAO,IAAI,KAAK,WAA1B;AACA,aAAO,CAAC,KAAK,CAAC,OAAN,CAAc,OAAd,CAAD,GAA0B,CAAC,OAAD,CAA1B,GAAsC,OAA7C;AACD;AAED;;;;;;;;;;;;;;;AAeG;;AA9QL;AAAA;AAAA,4BA+QU,MA/QV,EA+QkD,OA/QlD,EA+Q2E;AAEvE,MAAA,MAAM,GAAG,KAAK,eAAL,CAAqB,MAArB,CAAT;AACA,MAAA,OAAO,GAAG,KAAK,gBAAL,CAAsB,OAAtB,CAAV;AACA,UAAM,MAAM,GAAG,KAAK,QAAL,CAAc,OAAd,CAAsB,MAAtB,EAA8B,OAA9B,CAAf;AACA,aAAO,MAAM,CAAC,MAAP,GAAgB,CAAhB,GAAoB,MAApB,GAA6B,MAAM,CAAC,CAAD,CAA1C;AACD;AACD;;;;;;;;;;;;;;;AAeG;;AArSL;AAAA;AAAA;AAAA,qGAuSM,MAvSN,EAwSM,OAxSN;AAAA;AAAA;AAAA;AAAA;AAAA;AAySI,gBAAA,MAAM,GAAG,KAAK,eAAL,CAAqB,MAArB,CAAT;AACA,gBAAA,OAAO,GAAG,KAAK,gBAAL,CAAsB,OAAtB,CAAV;AA1SJ;AAAA,uBA2SyB,KAAK,QAAL,CAAc,YAAd,CAA2B,MAA3B,EAAmC,OAAnC,CA3SzB;;AAAA;AA2SU,gBAAA,MA3SV;AAAA,kDA4SW,MAAM,CAAC,MAAP,GAAgB,CAAhB,GAAoB,MAApB,GAA6B,MAAM,CAAC,CAAD,CA5S9C;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;AAAA;AAAA;AAAA;;AAAA;AAAA;AAAA;AAAA;AAAA,iDA+SuC,GA/SvC,EA+S0D;AACtD,aAAO,MAAM,CAAC,IAAP,CAAY,GAAZ,EAAiB,MAAjB,CAAwB,UAAC,MAAD,EAA0B,GAA1B,EAAiC;AAC9D,QAAA,MAAM,CAAC,GAAD,CAAN,GAAc,CAAC,GAAG,CAAC,GAAD,CAAJ,CAAd;AACA,eAAO,MAAP;AACD,OAHM,EAGJ,EAHI,CAAP;AAID;AAED;;;;AAIG;;AA1TL;AAAA;AAAA,8BA2TS;AACL,WAAK,QAAL,CAAc,OAAd;;AAEA,UAAI,KAAK,WAAT,EAAsB;AACpB,aAAK,WAAL,CAAiB,OAAjB;AACD;AACF;AAjUH;AAAA;AAAA,wBAQkB;AACd,aAAO,KAAK,OAAZ;AACD;AAVH;AAAA;AAAA,wBAYgB;AACZ,aAAO,KAAK,QAAL,CAAc,UAArB;AACD;AAdH;AAAA;AAAA,wBAgBiB;AACb,aAAO,KAAK,QAAL,CAAc,WAArB;AACD;AAlBH;AAAA;AAAA,wBAoBY;AACR,aAAO,KAAK,QAAL,CAAc,MAArB;AACD;AAtBH;AAAA;AAAA,wBAwBa;AACT,aAAO,KAAK,QAAL,CAAc,OAArB;AACD;AA1BH;AAAA;AAAA,wBA4Ba;AACT,aAAO,KAAK,QAAL,CAAc,SAArB;AACD;AA9BH;;AAAA;AAAA;AAoUA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AA6BG;;AACH,gBAAsB,cAAtB;AAAA;AAAA;;;6EAAO,kBACH,QADG;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAEH,YAAA,OAFG,8DAEuB,EAFvB;;AAAA,kBAGD,QAAQ,IAAI,IAHX;AAAA;AAAA;AAAA;;AAAA,kBAIG,IAAI,KAAJ,CACF,uEACA,sCAFE,CAJH;;AAAA;AAQL,gBAAI,OAAO,IAAI,IAAf,EAAqB;AACnB,cAAA,OAAO,GAAG,EAAV;AACD;;AAED,gBAAI,OAAO,CAAC,SAAZ,EAAuB;AACrB,kBAAK,QAAyB,CAAC,IAA1B,IAAkC,IAAvC,EAA6C;AAC3C,oBAAI,CAAE,QAAmB,CAAC,QAApB,CAA6B,GAA7B,CAAN,EAAyC;AACvC,kBAAA,QAAQ,GAAI,QAAmB,GAAG,GAAlC;AACD;;AACD,gBAAA,QAAQ,aAAM,QAAN,SAAiB,kBAAjB,SAAsC,kBAAtC,CAAR;AACD;AACF;;AACK,YAAA,KApBD,GAoBS,IAAI,UAAJ,CAAe,QAAf,EAAyB,OAAzB,CApBT;AAAA;AAAA,mBAqBC,KAAK,CAAC,IAAN,EArBD;;AAAA;AAAA,8CAsBE,KAtBF;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,G","sourceRoot":"","sourcesContent":["/**\r\n * @license\r\n * Copyright 2018 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\r\nimport { io, Tensor } from '@tensorflow/tfjs-core';\r\nimport { OperationMapper } from '../operations/operation_mapper';\r\nimport { GraphExecutor } from './graph_executor';\r\nexport const TFHUB_SEARCH_PARAM = '?tfjs-format=file';\r\nexport const DEFAULT_MODEL_NAME = 'model.json';\r\n/**\r\n * A `tf.GraphModel` is a directed, acyclic graph built from a\r\n * SavedModel GraphDef and allows inference execution.\r\n *\r\n * A `tf.GraphModel` can only be created by loading from a model converted from\r\n * a [TensorFlow SavedModel](https://www.tensorflow.org/guide/saved_model) using\r\n * the command line converter tool and loaded via `tf.loadGraphModel`.\r\n *\r\n * @doc {heading: 'Models', subheading: 'Classes'}\r\n */\r\nexport class GraphModel {\r\n    /**\r\n     * @param modelUrl url for the model, or an `io.IOHandler`.\r\n     * @param weightManifestUrl url for the weight file generated by\r\n     * scripts/convert.py script.\r\n     * @param requestOption options for Request, which allows to send credentials\r\n     * and custom headers.\r\n     * @param onProgress Optional, progress callback function, fired periodically\r\n     * before the load is completed.\r\n     */\r\n    constructor(modelUrl, loadOptions = {}) {\r\n        this.modelUrl = modelUrl;\r\n        this.loadOptions = loadOptions;\r\n        this.version = 'n/a';\r\n        if (loadOptions == null) {\r\n            this.loadOptions = {};\r\n        }\r\n    }\r\n    // Returns the version information for the tensorflow model GraphDef.\r\n    get modelVersion() {\r\n        return this.version;\r\n    }\r\n    get inputNodes() {\r\n        return this.executor.inputNodes;\r\n    }\r\n    get outputNodes() {\r\n        return this.executor.outputNodes;\r\n    }\r\n    get inputs() {\r\n        return this.executor.inputs;\r\n    }\r\n    get outputs() {\r\n        return this.executor.outputs;\r\n    }\r\n    get weights() {\r\n        return this.executor.weightMap;\r\n    }\r\n    findIOHandler() {\r\n        const path = this.modelUrl;\r\n        if (path.load != null) {\r\n            // Path is an IO Handler.\r\n            this.handler = path;\r\n        }\r\n        else if (this.loadOptions.requestInit != null) {\r\n            this.handler = io.browserHTTPRequest(path, this.loadOptions);\r\n        }\r\n        else {\r\n            const handlers = io.getLoadHandlers(path, this.loadOptions);\r\n            if (handlers.length === 0) {\r\n                // For backward compatibility: if no load handler can be found,\r\n                // assume it is a relative http path.\r\n                handlers.push(io.browserHTTPRequest(path, this.loadOptions));\r\n            }\r\n            else if (handlers.length > 1) {\r\n                throw new Error(`Found more than one (${handlers.length}) load handlers for ` +\r\n                    `URL '${[path]}'`);\r\n            }\r\n            this.handler = handlers[0];\r\n        }\r\n    }\r\n    /**\r\n     * Loads the model and weight files, construct the in memory weight map and\r\n     * compile the inference graph.\r\n     */\r\n    async load() {\r\n        this.findIOHandler();\r\n        if (this.handler.load == null) {\r\n            throw new Error('Cannot proceed with model loading because the IOHandler provided ' +\r\n                'does not have the `load` method implemented.');\r\n        }\r\n        const artifacts = await this.handler.load();\r\n        return this.loadSync(artifacts);\r\n    }\r\n    /**\r\n     * Synchronously construct the in memory weight map and\r\n     * compile the inference graph. Also initialize hashtable if any.\r\n     *\r\n     * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\r\n     */\r\n    loadSync(artifacts) {\r\n        this.artifacts = artifacts;\r\n        const graph = this.artifacts.modelTopology;\r\n        let signature = {};\r\n        if (this.artifacts.userDefinedMetadata != null) {\r\n            signature = // tslint:disable-next-line:no-any\r\n                this.artifacts.userDefinedMetadata.signature;\r\n        }\r\n        this.version = `${graph.versions.producer}.${graph.versions.minConsumer}`;\r\n        const weightMap = io.decodeWeights(this.artifacts.weightData, this.artifacts.weightSpecs);\r\n        this.executor = new GraphExecutor(OperationMapper.Instance.transformGraph(graph, signature));\r\n        this.executor.weightMap = this.convertTensorMapToTensorsMap(weightMap);\r\n        if (artifacts.modelInitializer != null) {\r\n            const initializer = OperationMapper.Instance.transformGraph(artifacts.modelInitializer);\r\n            this.initializer = new GraphExecutor(initializer);\r\n            this.initializer.weightMap = this.executor.weightMap;\r\n            this.initializer.execute({}, []);\r\n        }\r\n        return true;\r\n    }\r\n    /**\r\n     * Save the configuration and/or weights of the GraphModel.\r\n     *\r\n     * An `IOHandler` is an object that has a `save` method of the proper\r\n     * signature defined. The `save` method manages the storing or\r\n     * transmission of serialized data (\"artifacts\") that represent the\r\n     * model's topology and weights onto or via a specific medium, such as\r\n     * file downloads, local storage, IndexedDB in the web browser and HTTP\r\n     * requests to a server. TensorFlow.js provides `IOHandler`\r\n     * implementations for a number of frequently used saving mediums, such as\r\n     * `tf.io.browserDownloads` and `tf.io.browserLocalStorage`. See `tf.io`\r\n     * for more details.\r\n     *\r\n     * This method also allows you to refer to certain types of `IOHandler`s\r\n     * as URL-like string shortcuts, such as 'localstorage://' and\r\n     * 'indexeddb://'.\r\n     *\r\n     * Example 1: Save `model`'s topology and weights to browser [local\r\n     * storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage);\r\n     * then load it back.\r\n     *\r\n     * ```js\r\n     * const modelUrl =\r\n     *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';\r\n     * const model = await tf.loadGraphModel(modelUrl);\r\n     * const zeros = tf.zeros([1, 224, 224, 3]);\r\n     * model.predict(zeros).print();\r\n     *\r\n     * const saveResults = await model.save('localstorage://my-model-1');\r\n     *\r\n     * const loadedModel = await tf.loadGraphModel('localstorage://my-model-1');\r\n     * console.log('Prediction from loaded model:');\r\n     * model.predict(zeros).print();\r\n     * ```\r\n     *\r\n     * @param handlerOrURL An instance of `IOHandler` or a URL-like,\r\n     * scheme-based string shortcut for `IOHandler`.\r\n     * @param config Options for saving the model.\r\n     * @returns A `Promise` of `SaveResult`, which summarizes the result of\r\n     * the saving, such as byte sizes of the saved artifacts for the model's\r\n     *   topology and weight values.\r\n     *\r\n     * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}\r\n     */\r\n    async save(handlerOrURL, config) {\r\n        if (typeof handlerOrURL === 'string') {\r\n            const handlers = io.getSaveHandlers(handlerOrURL);\r\n            if (handlers.length === 0) {\r\n                throw new Error(`Cannot find any save handlers for URL '${handlerOrURL}'`);\r\n            }\r\n            else if (handlers.length > 1) {\r\n                throw new Error(`Found more than one (${handlers.length}) save handlers for ` +\r\n                    `URL '${handlerOrURL}'`);\r\n            }\r\n            handlerOrURL = handlers[0];\r\n        }\r\n        if (handlerOrURL.save == null) {\r\n            throw new Error('GraphModel.save() cannot proceed because the IOHandler ' +\r\n                'provided does not have the `save` attribute defined.');\r\n        }\r\n        return handlerOrURL.save(this.artifacts);\r\n    }\r\n    /**\r\n     * Execute the inference for the input tensors.\r\n     *\r\n     * @param input The input tensors, when there is single input for the model,\r\n     * inputs param should be a `tf.Tensor`. For models with mutliple inputs,\r\n     * inputs params should be in either `tf.Tensor`[] if the input order is\r\n     * fixed, or otherwise NamedTensorMap format.\r\n     *\r\n     * For model with multiple inputs, we recommend you use NamedTensorMap as the\r\n     * input type, if you use `tf.Tensor`[], the order of the array needs to\r\n     * follow the\r\n     * order of inputNodes array. @see {@link GraphModel.inputNodes}\r\n     *\r\n     * You can also feed any intermediate nodes using the NamedTensorMap as the\r\n     * input type. For example, given the graph\r\n     *    InputNode => Intermediate => OutputNode,\r\n     * you can execute the subgraph Intermediate => OutputNode by calling\r\n     *    model.execute('IntermediateNode' : tf.tensor(...));\r\n     *\r\n     * This is useful for models that uses tf.dynamic_rnn, where the intermediate\r\n     * state needs to be fed manually.\r\n     *\r\n     * For batch inference execution, the tensors for each input need to be\r\n     * concatenated together. For example with mobilenet, the required input shape\r\n     * is [1, 244, 244, 3], which represents the [batch, height, width, channel].\r\n     * If we are provide a batched data of 100 images, the input tensor should be\r\n     * in the shape of [100, 244, 244, 3].\r\n     *\r\n     * @param config Prediction configuration for specifying the batch size and\r\n     * output node names. Currently the batch size option is ignored for graph\r\n     * model.\r\n     *\r\n     * @returns Inference result tensors. The output would be single `tf.Tensor`\r\n     * if model has single output node, otherwise Tensor[] or NamedTensorMap[]\r\n     * will be returned for model with multiple outputs.\r\n     *\r\n     * @doc {heading: 'Models', subheading: 'Classes'}\r\n     */\r\n    predict(inputs, config) {\r\n        return this.execute(inputs, this.outputNodes);\r\n    }\r\n    normalizeInputs(inputs) {\r\n        if (!(inputs instanceof Tensor) && !Array.isArray(inputs)) {\r\n            // The input is already a NamedTensorMap.\r\n            return inputs;\r\n        }\r\n        inputs = Array.isArray(inputs) ? inputs : [inputs];\r\n        if (inputs.length !== this.inputNodes.length) {\r\n            throw new Error('Input tensor count mismatch,' +\r\n                `the graph model has ${this.inputNodes.length} placeholders, ` +\r\n                `while there are ${inputs.length} input tensors.`);\r\n        }\r\n        return this.inputNodes.reduce((map, inputName, i) => {\r\n            map[inputName] = inputs[i];\r\n            return map;\r\n        }, {});\r\n    }\r\n    normalizeOutputs(outputs) {\r\n        outputs = outputs || this.outputNodes;\r\n        return !Array.isArray(outputs) ? [outputs] : outputs;\r\n    }\r\n    /**\r\n     * Executes inference for the model for given input tensors.\r\n     * @param inputs tensor, tensor array or tensor map of the inputs for the\r\n     * model, keyed by the input node names.\r\n     * @param outputs output node name from the Tensorflow model, if no\r\n     * outputs are specified, the default outputs of the model would be used.\r\n     * You can inspect intermediate nodes of the model by adding them to the\r\n     * outputs array.\r\n     *\r\n     * @returns A single tensor if provided with a single output or no outputs\r\n     * are provided and there is only one default output, otherwise return a\r\n     * tensor array. The order of the tensor array is the same as the outputs\r\n     * if provided, otherwise the order of outputNodes attribute of the model.\r\n     *\r\n     * @doc {heading: 'Models', subheading: 'Classes'}\r\n     */\r\n    execute(inputs, outputs) {\r\n        inputs = this.normalizeInputs(inputs);\r\n        outputs = this.normalizeOutputs(outputs);\r\n        const result = this.executor.execute(inputs, outputs);\r\n        return result.length > 1 ? result : result[0];\r\n    }\r\n    /**\r\n     * Executes inference for the model for given input tensors in async\r\n     * fashion, use this method when your model contains control flow ops.\r\n     * @param inputs tensor, tensor array or tensor map of the inputs for the\r\n     * model, keyed by the input node names.\r\n     * @param outputs output node name from the Tensorflow model, if no outputs\r\n     * are specified, the default outputs of the model would be used. You can\r\n     * inspect intermediate nodes of the model by adding them to the outputs\r\n     * array.\r\n     *\r\n     * @returns A Promise of single tensor if provided with a single output or\r\n     * no outputs are provided and there is only one default output, otherwise\r\n     * return a tensor map.\r\n     *\r\n     * @doc {heading: 'Models', subheading: 'Classes'}\r\n     */\r\n    async executeAsync(inputs, outputs) {\r\n        inputs = this.normalizeInputs(inputs);\r\n        outputs = this.normalizeOutputs(outputs);\r\n        const result = await this.executor.executeAsync(inputs, outputs);\r\n        return result.length > 1 ? result : result[0];\r\n    }\r\n    convertTensorMapToTensorsMap(map) {\r\n        return Object.keys(map).reduce((newMap, key) => {\r\n            newMap[key] = [map[key]];\r\n            return newMap;\r\n        }, {});\r\n    }\r\n    /**\r\n     * Releases the memory used by the weight tensors.\r\n     *\r\n     * @doc {heading: 'Models', subheading: 'Classes'}\r\n     */\r\n    dispose() {\r\n        this.executor.dispose();\r\n        if (this.initializer) {\r\n            this.initializer.dispose();\r\n        }\r\n    }\r\n}\r\n/**\r\n * Load a graph model given a URL to the model definition.\r\n *\r\n * Example of loading MobileNetV2 from a URL and making a prediction with a\r\n * zeros input:\r\n *\r\n * ```js\r\n * const modelUrl =\r\n *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';\r\n * const model = await tf.loadGraphModel(modelUrl);\r\n * const zeros = tf.zeros([1, 224, 224, 3]);\r\n * model.predict(zeros).print();\r\n * ```\r\n *\r\n * Example of loading MobileNetV2 from a TF Hub URL and making a prediction with\r\n * a zeros input:\r\n *\r\n * ```js\r\n * const modelUrl =\r\n *    'https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/2';\r\n * const model = await tf.loadGraphModel(modelUrl, {fromTFHub: true});\r\n * const zeros = tf.zeros([1, 224, 224, 3]);\r\n * model.predict(zeros).print();\r\n * ```\r\n * @param modelUrl The url or an `io.IOHandler` that loads the model.\r\n * @param options Options for the HTTP request, which allows to send credentials\r\n *    and custom headers.\r\n *\r\n * @doc {heading: 'Models', subheading: 'Loading'}\r\n */\r\nexport async function loadGraphModel(modelUrl, options = {}) {\r\n    if (modelUrl == null) {\r\n        throw new Error('modelUrl in loadGraphModel() cannot be null. Please provide a url ' +\r\n            'or an IOHandler that loads the model');\r\n    }\r\n    if (options == null) {\r\n        options = {};\r\n    }\r\n    if (options.fromTFHub) {\r\n        if (modelUrl.load == null) {\r\n            if (!modelUrl.endsWith('/')) {\r\n                modelUrl = modelUrl + '/';\r\n            }\r\n            modelUrl = `${modelUrl}${DEFAULT_MODEL_NAME}${TFHUB_SEARCH_PARAM}`;\r\n        }\r\n    }\r\n    const model = new GraphModel(modelUrl, options);\r\n    await model.load();\r\n    return model;\r\n}\r\n//# sourceMappingURL=graph_model.js.map"]},"metadata":{},"sourceType":"module"}