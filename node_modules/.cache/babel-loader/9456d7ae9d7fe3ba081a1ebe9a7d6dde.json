{"ast":null,"code":"import _createForOfIteratorHelper from \"C:/Users/wongh/Documents/GitHub/tfjs-app/node_modules/@babel/runtime/helpers/esm/createForOfIteratorHelper\";\nimport _toConsumableArray from \"C:/Users/wongh/Documents/GitHub/tfjs-app/node_modules/@babel/runtime/helpers/esm/toConsumableArray\";\nimport _regeneratorRuntime from \"C:/Users/wongh/Documents/GitHub/tfjs-app/node_modules/@babel/runtime/regenerator\";\nimport _asyncToGenerator from \"C:/Users/wongh/Documents/GitHub/tfjs-app/node_modules/@babel/runtime/helpers/esm/asyncToGenerator\";\nimport _classCallCheck from \"C:/Users/wongh/Documents/GitHub/tfjs-app/node_modules/@babel/runtime/helpers/esm/classCallCheck\";\nimport _createClass from \"C:/Users/wongh/Documents/GitHub/tfjs-app/node_modules/@babel/runtime/helpers/esm/createClass\";\n\n/**\r\n * @license\r\n * Copyright 2018 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\nimport { KernelBackend } from './backends/backend';\nimport { Environment, setEnvironmentGlobal } from './environment';\nimport { getGlobalNamespace } from './global_util';\nimport { Add, Cast } from './kernel_names';\nimport { getGradient, getKernel, getKernelsForBackend } from './kernel_registry';\nimport { Profiler } from './profiler';\nimport { backpropagateGradients, getFilteredNodesXToY } from './tape';\nimport { setTensorTracker, Tensor, Variable } from './tensor';\nimport { getTensorsInContainer } from './tensor_util';\nimport * as util from './util';\nimport { bytesFromStringArray, makeOnesTypedArray, now, sizeFromShape } from './util';\n\nvar EngineState = /*#__PURE__*/function () {\n  function EngineState() {\n    _classCallCheck(this, EngineState);\n\n    // Public since optimizers will use it.\n    this.registeredVariables = {};\n    this.nextTapeNodeId = 0;\n    this.numBytes = 0;\n    this.numTensors = 0;\n    this.numStringTensors = 0;\n    this.numDataBuffers = 0; // Number of nested tf.grad() statements when computing higher-order\n    // gradients. E.g. `1` for first-order gradients and `2` for second-order\n    // gradients. Used to track if the tape should be removed after a backprop.\n\n    this.gradientDepth = 0; // Number of nested kernel calls. When kernel depth is greater than 1, we turn\n    // off the tape.\n\n    this.kernelDepth = 0;\n    this.scopeStack = [];\n    /**\r\n     * Keeps track of the number of data moves during a kernel execution. We\r\n     * maintain a stack since kernels can call other kernels, recursively.\r\n     */\n\n    this.numDataMovesStack = [];\n    this.nextScopeId = 0;\n    this.tensorInfo = new WeakMap();\n    this.profiling = false;\n    this.activeProfile = {\n      newBytes: 0,\n      newTensors: 0,\n      peakBytes: 0,\n      kernels: [],\n      result: null\n    };\n  }\n\n  _createClass(EngineState, [{\n    key: \"dispose\",\n    value: function dispose() {\n      for (var variableName in this.registeredVariables) {\n        this.registeredVariables[variableName].dispose();\n      }\n    }\n  }]);\n\n  return EngineState;\n}();\n\nexport var Engine = /*#__PURE__*/function () {\n  function Engine(ENV) {\n    _classCallCheck(this, Engine);\n\n    this.ENV = ENV;\n    this.registry = {};\n    this.registryFactory = {};\n    this.pendingBackendInitId = 0;\n    this.state = new EngineState();\n  }\n\n  _createClass(Engine, [{\n    key: \"ready\",\n    value: function () {\n      var _ready = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee() {\n        var sortedBackends, i, backendName, success;\n        return _regeneratorRuntime.wrap(function _callee$(_context) {\n          while (1) {\n            switch (_context.prev = _context.next) {\n              case 0:\n                if (!(this.pendingBackendInit != null)) {\n                  _context.next = 2;\n                  break;\n                }\n\n                return _context.abrupt(\"return\", this.pendingBackendInit.then(function () {}));\n\n              case 2:\n                if (!(this.backendInstance != null)) {\n                  _context.next = 4;\n                  break;\n                }\n\n                return _context.abrupt(\"return\");\n\n              case 4:\n                sortedBackends = this.getSortedBackends();\n                i = 0;\n\n              case 6:\n                if (!(i < sortedBackends.length)) {\n                  _context.next = 18;\n                  break;\n                }\n\n                backendName = sortedBackends[i];\n                _context.next = 10;\n                return this.initializeBackend(backendName).success;\n\n              case 10:\n                success = _context.sent;\n\n                if (!success) {\n                  _context.next = 15;\n                  break;\n                }\n\n                _context.next = 14;\n                return this.setBackend(backendName);\n\n              case 14:\n                return _context.abrupt(\"return\");\n\n              case 15:\n                i++;\n                _context.next = 6;\n                break;\n\n              case 18:\n                throw new Error(\"Could not initialize any backends, all backend initializations \" + \"failed.\");\n\n              case 19:\n              case \"end\":\n                return _context.stop();\n            }\n          }\n        }, _callee, this);\n      }));\n\n      function ready() {\n        return _ready.apply(this, arguments);\n      }\n\n      return ready;\n    }()\n  }, {\n    key: \"backendNames\",\n    value: function backendNames() {\n      return Object.keys(this.registryFactory);\n    }\n  }, {\n    key: \"findBackend\",\n    value: function findBackend(backendName) {\n      if (!(backendName in this.registry)) {\n        // If the backend hasn't been initialized but we have a registry entry for\n        // it, initialize it and return it.\n        if (backendName in this.registryFactory) {\n          var _this$initializeBacke = this.initializeBackend(backendName),\n              asyncInit = _this$initializeBacke.asyncInit;\n\n          if (asyncInit) {\n            // Backend is not ready yet.\n            return null;\n          }\n        } else {\n          return null;\n        }\n      }\n\n      return this.registry[backendName];\n    }\n  }, {\n    key: \"findBackendFactory\",\n    value: function findBackendFactory(backendName) {\n      if (!(backendName in this.registryFactory)) {\n        return null;\n      }\n\n      return this.registryFactory[backendName].factory;\n    }\n  }, {\n    key: \"registerBackend\",\n    value: function registerBackend(backendName, factory) {\n      var priority = arguments.length > 2 && arguments[2] !== undefined ? arguments[2] : 1;\n\n      if (backendName in this.registryFactory) {\n        console.warn(\"\".concat(backendName, \" backend was already registered. \") + \"Reusing existing backend factory.\");\n        return false;\n      }\n\n      this.registryFactory[backendName] = {\n        factory: factory,\n        priority: priority\n      };\n      return true;\n    }\n  }, {\n    key: \"setBackend\",\n    value: function () {\n      var _setBackend = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee2(backendName) {\n        var _this$initializeBacke2, success, asyncInit, result;\n\n        return _regeneratorRuntime.wrap(function _callee2$(_context2) {\n          while (1) {\n            switch (_context2.prev = _context2.next) {\n              case 0:\n                if (!(this.registryFactory[backendName] == null)) {\n                  _context2.next = 2;\n                  break;\n                }\n\n                throw new Error(\"Backend name '\".concat(backendName, \"' not found in registry\"));\n\n              case 2:\n                this.backendName = backendName;\n\n                if (!(this.registry[backendName] == null)) {\n                  _context2.next = 16;\n                  break;\n                }\n\n                this.backendInstance = null;\n                _this$initializeBacke2 = this.initializeBackend(backendName), success = _this$initializeBacke2.success, asyncInit = _this$initializeBacke2.asyncInit;\n\n                if (!asyncInit) {\n                  _context2.next = 12;\n                  break;\n                }\n\n                _context2.next = 9;\n                return success;\n\n              case 9:\n                _context2.t0 = _context2.sent;\n                _context2.next = 13;\n                break;\n\n              case 12:\n                _context2.t0 = success;\n\n              case 13:\n                result = _context2.t0;\n\n                if (result) {\n                  _context2.next = 16;\n                  break;\n                }\n\n                return _context2.abrupt(\"return\", false);\n\n              case 16:\n                this.backendInstance = this.registry[backendName];\n                this.setupRegisteredKernels(); // Reset the profiler.\n\n                this.profiler = new Profiler(this.backendInstance);\n                return _context2.abrupt(\"return\", true);\n\n              case 20:\n              case \"end\":\n                return _context2.stop();\n            }\n          }\n        }, _callee2, this);\n      }));\n\n      function setBackend(_x) {\n        return _setBackend.apply(this, arguments);\n      }\n\n      return setBackend;\n    }()\n  }, {\n    key: \"setupRegisteredKernels\",\n    value: function setupRegisteredKernels() {\n      var _this = this;\n\n      var kernels = getKernelsForBackend(this.backendName);\n      kernels.forEach(function (kernel) {\n        if (kernel.setupFunc != null) {\n          kernel.setupFunc(_this.backendInstance);\n        }\n      });\n    }\n  }, {\n    key: \"disposeRegisteredKernels\",\n    value: function disposeRegisteredKernels(backendName) {\n      var _this2 = this;\n\n      var kernels = getKernelsForBackend(backendName);\n      kernels.forEach(function (kernel) {\n        if (kernel.disposeFunc != null) {\n          kernel.disposeFunc(_this2.registry[backendName]);\n        }\n      });\n    }\n    /**\r\n     * Initializes a backend by looking up the backend name in the factory\r\n     * registry and calling the factory method. Returns a boolean representing\r\n     * whether the initialization of the backend suceeded. Throws an error if\r\n     * there is no backend in the factory registry.\r\n     */\n\n  }, {\n    key: \"initializeBackend\",\n    value: function initializeBackend(backendName) {\n      var _this3 = this;\n\n      var registryFactoryEntry = this.registryFactory[backendName];\n\n      if (registryFactoryEntry == null) {\n        throw new Error(\"Cannot initialize backend \".concat(backendName, \", no registration found.\"));\n      }\n\n      try {\n        var backend = registryFactoryEntry.factory();\n        /* Test if the factory returns a promise.\r\n        Done in a more liberal way than\r\n        previous 'Promise.resolve(backend)===backend'\r\n        as we needed to account for custom Promise\r\n        implementations (e.g. Angular) */\n\n        if (backend && !(backend instanceof KernelBackend) && typeof backend.then === 'function') {\n          var promiseId = ++this.pendingBackendInitId;\n          var success = backend.then(function (backendInstance) {\n            // Outdated promise. Another backend was set in the meantime.\n            if (promiseId < _this3.pendingBackendInitId) {\n              return false;\n            }\n\n            _this3.registry[backendName] = backendInstance;\n            _this3.pendingBackendInit = null;\n            return true;\n          }).catch(function (err) {\n            // Outdated promise. Another backend was set in the meantime.\n            if (promiseId < _this3.pendingBackendInitId) {\n              return false;\n            }\n\n            _this3.pendingBackendInit = null;\n            console.warn(\"Initialization of backend \".concat(backendName, \" failed\"));\n            console.warn(err.stack || err.message);\n            return false;\n          });\n          this.pendingBackendInit = success;\n          return {\n            success: success,\n            asyncInit: true\n          };\n        } else {\n          this.registry[backendName] = backend;\n          return {\n            success: true,\n            asyncInit: false\n          };\n        }\n      } catch (err) {\n        console.warn(\"Initialization of backend \".concat(backendName, \" failed\"));\n        console.warn(err.stack || err.message);\n        return {\n          success: false,\n          asyncInit: false\n        };\n      }\n    }\n  }, {\n    key: \"removeBackend\",\n    value: function removeBackend(backendName) {\n      if (!(backendName in this.registryFactory)) {\n        throw new Error(\"\".concat(backendName, \" backend not found in registry\"));\n      }\n\n      if (this.backendName === backendName && this.pendingBackendInit != null) {\n        // There is a pending promise of the backend we want to remove. Make it\n        // obsolete.\n        this.pendingBackendInitId++;\n      }\n\n      if (backendName in this.registry) {\n        this.disposeRegisteredKernels(backendName);\n        this.registry[backendName].dispose();\n        delete this.registry[backendName];\n      }\n\n      delete this.registryFactory[backendName]; // Unset the backend if it is active.\n\n      if (this.backendName === backendName) {\n        this.pendingBackendInit = null;\n        this.backendName = null;\n        this.backendInstance = null;\n      }\n    }\n  }, {\n    key: \"getSortedBackends\",\n    value: function getSortedBackends() {\n      var _this4 = this;\n\n      if (Object.keys(this.registryFactory).length === 0) {\n        throw new Error('No backend found in registry.');\n      }\n\n      return Object.keys(this.registryFactory).sort(function (a, b) {\n        // Highest priority comes first.\n        return _this4.registryFactory[b].priority - _this4.registryFactory[a].priority;\n      });\n    }\n  }, {\n    key: \"initializeBackendsAndReturnBest\",\n    value: function initializeBackendsAndReturnBest() {\n      var sortedBackends = this.getSortedBackends();\n\n      for (var i = 0; i < sortedBackends.length; i++) {\n        var backendName = sortedBackends[i];\n\n        var _this$initializeBacke3 = this.initializeBackend(backendName),\n            success = _this$initializeBacke3.success,\n            asyncInit = _this$initializeBacke3.asyncInit;\n\n        if (asyncInit || success) {\n          return {\n            name: backendName,\n            asyncInit: asyncInit\n          };\n        }\n      }\n\n      throw new Error(\"Could not initialize any backends, all backend initializations \" + \"failed.\");\n    }\n  }, {\n    key: \"moveData\",\n    value: function moveData(backend, dataId) {\n      var info = this.state.tensorInfo.get(dataId);\n      var srcBackend = info.backend;\n      var values = this.readSync(dataId); // Delete the tensor from the old backend and move it to the new\n      // backend.\n\n      srcBackend.disposeData(dataId);\n      info.backend = backend;\n      backend.move(dataId, values, info.shape, info.dtype);\n\n      if (this.shouldCheckForMemLeaks()) {\n        // Track the number of moves during a kernel execution to correctly\n        // detect memory leaks.\n        this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1]++;\n      }\n    }\n  }, {\n    key: \"tidy\",\n    value: function tidy(nameOrFn, fn) {\n      var _this5 = this;\n\n      var name = null;\n\n      if (fn == null) {\n        // Called with only 1 argument.\n        if (typeof nameOrFn !== 'function') {\n          throw new Error('Please provide a function to tidy()');\n        }\n\n        fn = nameOrFn;\n      } else {\n        // Called with 2 arguments.\n        if (typeof nameOrFn !== 'string' && !(nameOrFn instanceof String)) {\n          throw new Error('When calling with two arguments, the first argument ' + 'to tidy() must be a string');\n        }\n\n        if (typeof fn !== 'function') {\n          throw new Error('When calling with two arguments, the 2nd argument ' + 'to tidy() must be a function');\n        }\n\n        name = nameOrFn; // TODO(nsthorat,smilkov): Do operation logging and performance\n        // profiling.\n      }\n\n      var result;\n      return this.scopedRun(function () {\n        return _this5.startScope(name);\n      }, function () {\n        return _this5.endScope(result);\n      }, function () {\n        result = fn();\n\n        if (result instanceof Promise) {\n          console.error('Cannot return a Promise inside of tidy.');\n        }\n\n        return result;\n      });\n    }\n  }, {\n    key: \"scopedRun\",\n    value: function scopedRun(start, end, f) {\n      start();\n\n      try {\n        var res = f();\n        end();\n        return res;\n      } catch (ex) {\n        end();\n        throw ex;\n      }\n    }\n  }, {\n    key: \"nextTensorId\",\n    value: function nextTensorId() {\n      return Engine.nextTensorId++;\n    }\n  }, {\n    key: \"nextVariableId\",\n    value: function nextVariableId() {\n      return Engine.nextVariableId++;\n    }\n    /**\r\n     * This method is called instead of the public-facing tensor.clone() when\r\n     * saving a tensor for backwards pass. It makes sure to add the clone\r\n     * operation to the tape regardless of being called inside a kernel\r\n     * execution.\r\n     *\r\n     * This method will go away once all kernels are modularized since we won't\r\n     * need to turn off the tape inside runKernel().\r\n     */\n\n  }, {\n    key: \"clone\",\n    value: function clone(x) {\n      var y = this.makeTensorFromDataId(x.dataId, x.shape, x.dtype);\n      var inputs = {\n        x: x\n      };\n\n      var grad = function grad(dy) {\n        return {\n          x: function x() {\n            var dtype = 'float32';\n            var gradInputs = {\n              x: dy\n            };\n            var attrs = {\n              dtype: dtype\n            };\n            return ENGINE.runKernelFunc(function (backend) {\n              return backend.cast(dy, dtype);\n            }, gradInputs, null\n            /* grad */\n            , Cast, attrs);\n          }\n        };\n      };\n\n      var saved = [];\n      this.addTapeNode(this.state.activeScope.name, inputs, [y], grad, saved, {});\n      return y;\n    }\n    /**\r\n     * Execute a kernel with the given name and return the output tensor.\r\n     *\r\n     * @param kernelName The name of the kernel to execute.\r\n     * @param inputs A map of input names to tensors.\r\n     * @param attrs A map of attribute names to their values. An attribute is a\r\n     *     primitive (non-tensor) input to the kernel.\r\n     * @param inputsToSave A list of tensors, inputs to save for the backprop\r\n     *     computation.\r\n     * @param outputsToSave A list of booleans, specifying which output to save\r\n     *     for the backprop computation. These are booleans since the output\r\n     * tensors are not visible to the user.\r\n     */\n\n  }, {\n    key: \"runKernel\",\n    value: function runKernel(kernelName, inputs, attrs, inputsToSave, outputsToSave) {\n      var forwardFunc = null;\n      var backwardsFunc = null; // Call runKernel as a stop-gap until we modularize all kernels.\n      // Once we modularize all kernels, we will remove the existing\n      // `runKernelFunc`.\n\n      return this.runKernelFunc(forwardFunc, inputs, backwardsFunc, kernelName, attrs, inputsToSave, outputsToSave);\n    }\n  }, {\n    key: \"shouldCheckForMemLeaks\",\n    value: function shouldCheckForMemLeaks() {\n      return this.ENV.getBool('IS_TEST');\n    }\n  }, {\n    key: \"checkKernelForMemLeak\",\n    value: function checkKernelForMemLeak(kernelName, numDataIdsBefore, outInfos) {\n      var numDataIdsAfter = this.backend.numDataIds(); // Count the number of data ids associated with the result of the kernel.\n\n      var numOutputDataIds = 0;\n      outInfos.forEach(function (info) {\n        // Complex numbers allocate 3 data ids, one for 'real', one for\n        // 'imaginary', and one for the container that holds the former two.\n        numOutputDataIds += info.dtype === 'complex64' ? 3 : 1;\n      }); // Account for the number of moves during kernel execution. A \"data move\"\n      // can happen in the middle of a kernel execution, placing a new (key,value)\n      // pair in the data storage. Since data moves have net zero effect (we\n      // always remove the data from the old backend), we have to cancel them out\n      // when detecting memory leaks.\n\n      var numMoves = this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1];\n      var dataIdsLeaked = numDataIdsAfter - numDataIdsBefore - numOutputDataIds - numMoves;\n\n      if (dataIdsLeaked > 0) {\n        throw new Error(\"Backend '\".concat(this.backendName, \"' has an internal memory leak \") + \"(\".concat(dataIdsLeaked, \" data ids) after running '\").concat(kernelName, \"'\"));\n      }\n    }\n    /**\r\n     * @deprecated Use `runKernel` for newly added kernels. Keep using this method\r\n     *     only for kernels that are not yet fully modularized.\r\n     */\n\n  }, {\n    key: \"runKernelFunc\",\n    value: function runKernelFunc(forwardFunc, inputs, backwardsFunc, kernelName, attrs, inputsToSave, outputsToSave) {\n      var _this6 = this;\n\n      var outputs;\n      var saved = [];\n      var isTapeOn = this.isTapeOn();\n\n      if (kernelName == null) {\n        kernelName = this.state.activeScope != null ? this.state.activeScope.name : '';\n      }\n\n      var startingBytecount = this.state.numBytes;\n      var startingNumTensors = this.state.numTensors;\n\n      if (this.shouldCheckForMemLeaks()) {\n        this.state.numDataMovesStack.push(0);\n      }\n\n      var kernelFunc;\n      var kernel = getKernel(kernelName, this.backendName);\n      var out;\n\n      if (kernel != null) {\n        kernelFunc = function kernelFunc() {\n          var numDataIdsBefore = _this6.backend.numDataIds();\n\n          out = kernel.kernelFunc({\n            inputs: inputs,\n            attrs: attrs,\n            backend: _this6.backend\n          });\n          var outInfos = Array.isArray(out) ? out : [out];\n\n          if (_this6.shouldCheckForMemLeaks()) {\n            _this6.checkKernelForMemLeak(kernelName, numDataIdsBefore, outInfos);\n          }\n\n          var outTensors = outInfos.map(function (_ref) {\n            var dataId = _ref.dataId,\n                shape = _ref.shape,\n                dtype = _ref.dtype;\n            return _this6.makeTensorFromDataId(dataId, shape, dtype);\n          }); // Save the inputs and outputs.\n          // Do not save unless we are recording to the tape. Otherwise it would\n          // cause a mem leak since we would never run backprop, which disposes\n          // the kept tensors.\n\n          if (isTapeOn) {\n            var tensorsToSave = _this6.getTensorsForGradient(kernelName, inputs, outTensors);\n\n            if (tensorsToSave == null) {\n              // Fallback for ops that call runKernelFunc and pass in\n              // inputsToSave and outputsToSave. Currently this is the set of ops\n              // with kernel support in the WASM backend. Once those ops and\n              // respective gradients are modularised we can remove this path.\n              if (outputsToSave == null) {\n                outputsToSave = [];\n              }\n\n              var outsToSave = outTensors.filter(function (_, i) {\n                return outputsToSave[i];\n              });\n              tensorsToSave = (inputsToSave || []).slice().concat(outsToSave);\n            }\n\n            saved = _this6.saveTensorsForBackwardMode(tensorsToSave);\n          }\n\n          return outTensors;\n        };\n      } else {\n        var saveFunc = function saveFunc(tensors) {\n          // Do not save unless we are recording to the tape. Otherwise it would\n          // cause a mem leak since we would never run backprop, which disposes\n          // the kept tensors.\n          if (!isTapeOn) {\n            return;\n          }\n\n          saved = tensors.map(function (tensor) {\n            return _this6.keep(_this6.clone(tensor));\n          });\n        };\n\n        kernelFunc = function kernelFunc() {\n          var numDataIdsBefore = _this6.backend.numDataIds();\n\n          out = _this6.tidy(function () {\n            return forwardFunc(_this6.backend, saveFunc);\n          });\n          var outs = Array.isArray(out) ? out : [out];\n\n          if (_this6.shouldCheckForMemLeaks()) {\n            _this6.checkKernelForMemLeak(kernelName, numDataIdsBefore, outs);\n          }\n\n          return outs;\n        };\n      } // Stop recording to a tape when running a kernel.\n\n\n      var kernelProfile;\n      this.scopedRun(function () {\n        return _this6.state.kernelDepth++;\n      }, function () {\n        return _this6.state.kernelDepth--;\n      }, function () {\n        if (!_this6.ENV.getBool('DEBUG') && !_this6.state.profiling) {\n          outputs = kernelFunc();\n        } else {\n          kernelProfile = _this6.profiler.profileKernel(kernelName, inputs, function () {\n            return kernelFunc();\n          });\n\n          if (_this6.ENV.getBool('DEBUG')) {\n            _this6.profiler.logKernelProfile(kernelProfile);\n          }\n\n          outputs = kernelProfile.outputs;\n        }\n      });\n\n      if (isTapeOn) {\n        this.addTapeNode(kernelName, inputs, outputs, backwardsFunc, saved, attrs);\n      }\n\n      if (this.state.profiling) {\n        this.state.activeProfile.kernels.push({\n          name: kernelName,\n          bytesAdded: this.state.numBytes - startingBytecount,\n          totalBytesSnapshot: this.state.numBytes,\n          tensorsAdded: this.state.numTensors - startingNumTensors,\n          totalTensorsSnapshot: this.state.numTensors,\n          inputShapes: Object.keys(inputs).map(function (key) {\n            return inputs[key] != null ? inputs[key].shape : null;\n          }),\n          outputShapes: outputs.map(function (item) {\n            return item.shape;\n          }),\n          kernelTimeMs: kernelProfile.timeMs,\n          extraInfo: kernelProfile.extraInfo\n        });\n      }\n\n      return Array.isArray(out) ? outputs : outputs[0];\n    }\n    /**\r\n     * Saves tensors used in forward mode for use in backward mode.\r\n     *\r\n     * @param tensors the list of tensors to save.\r\n     */\n\n  }, {\n    key: \"saveTensorsForBackwardMode\",\n    value: function saveTensorsForBackwardMode(tensors) {\n      var _this7 = this;\n\n      var saved = tensors.map(function (tensor) {\n        return _this7.keep(_this7.clone(tensor));\n      });\n      return saved;\n    }\n    /**\r\n     * Returns a list of tensors to save for a given gradient calculation.\r\n     *\r\n     * Returns undefined if their is no registered gradient for this kernel in the\r\n     * gradient registry.\r\n     *\r\n     * @param kernelName name of kernel to look up gradient for.\r\n     * @param inputs a map of input tensors.\r\n     * @param outputs an array of output tensors from forward mode of kernel.\r\n     */\n\n  }, {\n    key: \"getTensorsForGradient\",\n    value: function getTensorsForGradient(kernelName, inputs, outputs) {\n      var gradConfig = getGradient(kernelName);\n\n      if (gradConfig != null) {\n        var inputsToSave = gradConfig.inputsToSave || [];\n        var outputsToSave = gradConfig.outputsToSave || []; // If saveAllInputs is true, all inputs will be saved. Otherwise, inputs\n        // specified in inputsToSave will be saved.\n\n        var inputTensorsToSave;\n\n        if (gradConfig.saveAllInputs) {\n          util.assert(Array.isArray(inputs), function () {\n            return 'saveAllInputs is true, expected inputs to be an array.';\n          });\n          inputTensorsToSave = Object.keys(inputs).map(function (key) {\n            return inputs[key];\n          });\n        } else {\n          inputTensorsToSave = inputsToSave.map(function (inputName) {\n            return inputs[inputName];\n          });\n        }\n\n        var outputTensorsToSave = outputs.filter(function (_, i) {\n          return outputsToSave[i];\n        });\n        return inputTensorsToSave.concat(outputTensorsToSave);\n      } // TODO(yassogba) throw exception here once all runkernelFunc calls with\n      // inputsToSave/outputsToSave are removed\n\n\n      return null;\n    }\n    /**\r\n     * Internal method used by public APIs for tensor creation. Makes a new\r\n     * tensor with the provided shape, dtype and values. It always\r\n     * creates a new data id and writes the values to the underlying backend.\r\n     */\n\n  }, {\n    key: \"makeTensor\",\n    value: function makeTensor(values, shape, dtype, backend) {\n      if (values == null) {\n        throw new Error('Values passed to engine.makeTensor() are null');\n      }\n\n      dtype = dtype || 'float32';\n      backend = backend || this.backend;\n      var backendVals = values;\n\n      if (dtype === 'string' && util.isString(values[0])) {\n        backendVals = values.map(function (d) {\n          return util.encodeString(d);\n        });\n      }\n\n      var dataId = backend.write(backendVals, shape, dtype);\n      var t = new Tensor(shape, dtype, dataId, this.nextTensorId());\n      this.incRef(t, backend); // Count bytes for string tensors.\n\n      if (dtype === 'string') {\n        var info = this.state.tensorInfo.get(dataId);\n        var newBytes = bytesFromStringArray(backendVals);\n        this.state.numBytes += newBytes - info.bytes;\n        info.bytes = newBytes;\n      }\n\n      return t;\n    }\n    /**\r\n     * Internal method used by backends. Makes a new tensor\r\n     * that is a wrapper around an existing data id. It doesn't create\r\n     * a new data id, only increments the ref count used in memory tracking.\r\n     */\n\n  }, {\n    key: \"makeTensorFromDataId\",\n    value: function makeTensorFromDataId(dataId, shape, dtype, backend) {\n      dtype = dtype || 'float32';\n      var t = new Tensor(shape, dtype, dataId, this.nextTensorId());\n      this.incRef(t, backend);\n      return t;\n    }\n  }, {\n    key: \"makeVariable\",\n    value: function makeVariable(initialValue) {\n      var trainable = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : true;\n      var name = arguments.length > 2 ? arguments[2] : undefined;\n      var dtype = arguments.length > 3 ? arguments[3] : undefined;\n      name = name || this.nextVariableId().toString();\n\n      if (dtype != null && dtype !== initialValue.dtype) {\n        initialValue = initialValue.cast(dtype);\n      }\n\n      var v = new Variable(initialValue, trainable, name, this.nextTensorId());\n\n      if (this.state.registeredVariables[v.name] != null) {\n        throw new Error(\"Variable with name \".concat(v.name, \" was already registered\"));\n      }\n\n      this.state.registeredVariables[v.name] = v;\n      this.incRef(v, this.backend);\n      return v;\n    }\n  }, {\n    key: \"incRef\",\n    value: function incRef(a, backend) {\n      var refCount = this.state.tensorInfo.has(a.dataId) ? this.state.tensorInfo.get(a.dataId).refCount : 0;\n      this.state.numTensors++;\n\n      if (a.dtype === 'string') {\n        this.state.numStringTensors++;\n      }\n\n      if (refCount === 0) {\n        this.state.numDataBuffers++; // Bytes for complex numbers are counted by their components. Bytes for\n        // string tensors are counted when writing values.\n\n        var bytes = 0;\n\n        if (a.dtype !== 'complex64' && a.dtype !== 'string') {\n          bytes = a.size * util.bytesPerElement(a.dtype);\n        }\n\n        this.state.tensorInfo.set(a.dataId, {\n          backend: backend || this.backend,\n          dtype: a.dtype,\n          shape: a.shape,\n          bytes: bytes,\n          refCount: 0\n        });\n        this.state.numBytes += bytes;\n      }\n\n      this.state.tensorInfo.get(a.dataId).refCount++;\n\n      if (!(a instanceof Variable)) {\n        this.track(a);\n      }\n    }\n  }, {\n    key: \"disposeTensor\",\n    value: function disposeTensor(a) {\n      if (!this.state.tensorInfo.has(a.dataId)) {\n        return;\n      }\n\n      this.state.numTensors--;\n\n      if (a.dtype === 'string') {\n        this.state.numStringTensors--;\n      }\n\n      var info = this.state.tensorInfo.get(a.dataId);\n      var refCount = info.refCount;\n\n      if (refCount <= 1) {\n        // Don't count bytes for complex numbers as they are counted by their\n        // components.\n        if (a.dtype !== 'complex64') {\n          this.state.numBytes -= info.bytes;\n        }\n\n        this.state.numDataBuffers--;\n        info.backend.disposeData(a.dataId);\n        this.state.tensorInfo.delete(a.dataId);\n      } else {\n        this.state.tensorInfo.get(a.dataId).refCount--;\n      } // TODO(nsthorat): Construct an error and save the stack trace for\n      // debugging when in debug mode. Creating a stack trace is too expensive\n      // to do unconditionally.\n\n    }\n  }, {\n    key: \"disposeVariables\",\n    value: function disposeVariables() {\n      for (var varName in this.state.registeredVariables) {\n        var v = this.state.registeredVariables[varName];\n        this.disposeVariable(v);\n      }\n    }\n  }, {\n    key: \"disposeVariable\",\n    value: function disposeVariable(v) {\n      this.disposeTensor(v);\n\n      if (this.state.registeredVariables[v.name] != null) {\n        delete this.state.registeredVariables[v.name];\n      }\n    }\n  }, {\n    key: \"memory\",\n    value: function memory() {\n      var info = this.backend.memory();\n      info.numTensors = this.state.numTensors;\n      info.numDataBuffers = this.state.numDataBuffers;\n      info.numBytes = this.state.numBytes;\n\n      if (this.state.numStringTensors > 0) {\n        info.unreliable = true;\n\n        if (info.reasons == null) {\n          info.reasons = [];\n        }\n\n        info.reasons.push('Memory usage by string tensors is approximate ' + '(2 bytes per character)');\n      }\n\n      return info;\n    }\n  }, {\n    key: \"profile\",\n    value: function () {\n      var _profile = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee3(query) {\n        var startBytes, startNumTensors, _iterator, _step, kernel;\n\n        return _regeneratorRuntime.wrap(function _callee3$(_context3) {\n          while (1) {\n            switch (_context3.prev = _context3.next) {\n              case 0:\n                this.state.profiling = true;\n                startBytes = this.state.numBytes;\n                startNumTensors = this.state.numTensors;\n                this.state.activeProfile.kernels = [];\n                _context3.next = 6;\n                return query();\n\n              case 6:\n                this.state.activeProfile.result = _context3.sent;\n                this.state.profiling = false;\n                this.state.activeProfile.peakBytes = Math.max.apply(Math, _toConsumableArray(this.state.activeProfile.kernels.map(function (d) {\n                  return d.totalBytesSnapshot;\n                })));\n                this.state.activeProfile.newBytes = this.state.numBytes - startBytes;\n                this.state.activeProfile.newTensors = this.state.numTensors - startNumTensors;\n                _iterator = _createForOfIteratorHelper(this.state.activeProfile.kernels);\n                _context3.prev = 12;\n\n                _iterator.s();\n\n              case 14:\n                if ((_step = _iterator.n()).done) {\n                  _context3.next = 24;\n                  break;\n                }\n\n                kernel = _step.value;\n                _context3.next = 18;\n                return kernel.kernelTimeMs;\n\n              case 18:\n                kernel.kernelTimeMs = _context3.sent;\n                _context3.next = 21;\n                return kernel.extraInfo;\n\n              case 21:\n                kernel.extraInfo = _context3.sent;\n\n              case 22:\n                _context3.next = 14;\n                break;\n\n              case 24:\n                _context3.next = 29;\n                break;\n\n              case 26:\n                _context3.prev = 26;\n                _context3.t0 = _context3[\"catch\"](12);\n\n                _iterator.e(_context3.t0);\n\n              case 29:\n                _context3.prev = 29;\n\n                _iterator.f();\n\n                return _context3.finish(29);\n\n              case 32:\n                return _context3.abrupt(\"return\", this.state.activeProfile);\n\n              case 33:\n              case \"end\":\n                return _context3.stop();\n            }\n          }\n        }, _callee3, this, [[12, 26, 29, 32]]);\n      }));\n\n      function profile(_x2) {\n        return _profile.apply(this, arguments);\n      }\n\n      return profile;\n    }()\n  }, {\n    key: \"isTapeOn\",\n    value: function isTapeOn() {\n      return this.state.gradientDepth > 0 && this.state.kernelDepth === 0;\n    }\n  }, {\n    key: \"addTapeNode\",\n    value: function addTapeNode(kernelName, inputs, outputs, gradientsFunc, saved, attrs) {\n      var _this8 = this;\n\n      var tapeNode = {\n        id: this.state.nextTapeNodeId++,\n        kernelName: kernelName,\n        inputs: inputs,\n        outputs: outputs,\n        saved: saved\n      };\n      var gradConfig = getGradient(kernelName);\n\n      if (gradConfig != null) {\n        gradientsFunc = gradConfig.gradFunc;\n      }\n\n      if (gradientsFunc != null) {\n        tapeNode.gradient = function (dys) {\n          // TODO(smilkov): To optimize back-prop, pass dys that are not used in\n          // the backprop graph to the user as null instead of zeros\n          dys = dys.map(function (dy, i) {\n            if (dy == null) {\n              var output = outputs[i];\n              var vals = util.makeZerosTypedArray(output.size, output.dtype);\n              return _this8.makeTensor(vals, output.shape, output.dtype);\n            }\n\n            return dy;\n          }); // Grad functions of ops with single outputs expect a dy, while ops\n          // with multiple outputs expect dys (array of dy).\n\n          return gradientsFunc(dys.length > 1 ? dys : dys[0], saved, attrs);\n        };\n      }\n\n      this.state.activeTape.push(tapeNode);\n    }\n  }, {\n    key: \"keep\",\n    value: function keep(result) {\n      result.kept = true;\n      return result;\n    }\n  }, {\n    key: \"startTape\",\n    value: function startTape() {\n      if (this.state.gradientDepth === 0) {\n        this.state.activeTape = [];\n      }\n\n      this.state.gradientDepth++;\n    }\n  }, {\n    key: \"endTape\",\n    value: function endTape() {\n      this.state.gradientDepth--;\n    }\n    /**\r\n     * Start a scope. Use this with endScope() to achieve the same functionality\r\n     * as scope() without the need for a function closure.\r\n     */\n\n  }, {\n    key: \"startScope\",\n    value: function startScope(name) {\n      var scopeInfo = {\n        track: [],\n        name: 'unnamed scope',\n        id: this.state.nextScopeId++\n      };\n\n      if (name) {\n        scopeInfo.name = name;\n      }\n\n      this.state.scopeStack.push(scopeInfo);\n      this.state.activeScope = scopeInfo;\n    }\n    /**\r\n     * End a scope. Use this with startScope() to achieve the same functionality\r\n     * as scope() without the need for a function closure.\r\n     */\n\n  }, {\n    key: \"endScope\",\n    value: function endScope(result) {\n      var _this9 = this;\n\n      var tensorsToTrackInParent = getTensorsInContainer(result);\n      var tensorsToTrackInParentSet = new Set(tensorsToTrackInParent.map(function (t) {\n        return t.id;\n      })); // Dispose the arrays tracked in this scope.\n\n      for (var i = 0; i < this.state.activeScope.track.length; i++) {\n        var tensor = this.state.activeScope.track[i];\n\n        if (!tensor.kept && !tensorsToTrackInParentSet.has(tensor.id)) {\n          tensor.dispose();\n        }\n      }\n\n      var oldScope = this.state.scopeStack.pop();\n      this.state.activeScope = this.state.scopeStack.length === 0 ? null : this.state.scopeStack[this.state.scopeStack.length - 1]; // Track the current result in the parent scope.\n\n      tensorsToTrackInParent.forEach(function (tensor) {\n        // Only track the tensor if was allocated in the inner scope and is not\n        // globally kept.\n        if (!tensor.kept && tensor.scopeId === oldScope.id) {\n          _this9.track(tensor);\n        }\n      });\n    }\n    /**\r\n     * Returns gradients of `f` with respect to each of the `xs`. The gradients\r\n     * returned are of the same length as `xs`, but some might be null if `f`\r\n     * was not a function of that `x`. It also takes optional dy to multiply the\r\n     * gradient, which defaults to `1`.\r\n     */\n\n  }, {\n    key: \"gradients\",\n    value: function gradients(f, xs, dy) {\n      var _this10 = this;\n\n      var allowNoGradients = arguments.length > 3 && arguments[3] !== undefined ? arguments[3] : false;\n      util.assert(xs.length > 0, function () {\n        return 'gradients() received an empty list of xs.';\n      });\n\n      if (dy != null && dy.dtype !== 'float32') {\n        throw new Error(\"dy must have 'float32' dtype, but has '\".concat(dy.dtype, \"'\"));\n      }\n\n      var y = this.scopedRun(function () {\n        return _this10.startTape();\n      }, function () {\n        return _this10.endTape();\n      }, function () {\n        return _this10.tidy('forward', f);\n      });\n      util.assert(y instanceof Tensor, function () {\n        return 'The result y returned by f() must be a tensor.';\n      }); // Filter out the nodes that don't connect x => y.\n\n      var filteredTape = getFilteredNodesXToY(this.state.activeTape, xs, y);\n\n      if (!allowNoGradients && filteredTape.length === 0 && xs.length > 0) {\n        throw new Error('Cannot compute gradient of y=f(x) with respect to x. Make sure ' + 'that the f you passed encloses all operations that lead from x ' + 'to y.');\n      }\n\n      return this.tidy('backward', function () {\n        var accumulatedGradientMap = {};\n        accumulatedGradientMap[y.id] = dy == null ? ones(y.shape) : dy; // Backprop gradients through the filtered nodes.\n\n        backpropagateGradients(accumulatedGradientMap, filteredTape, // Pass the tidy function to avoid circular dep with `tape.ts`.\n        function (f) {\n          return _this10.tidy(f);\n        }, // Pass an add function to avoide a circular dep with `tape.ts`.\n        add);\n        var grads = xs.map(function (x) {\n          return accumulatedGradientMap[x.id];\n        });\n\n        if (_this10.state.gradientDepth === 0) {\n          // This means that we are not computing higher-order gradients\n          // and can clean up the tape.\n          _this10.state.activeTape.forEach(function (node) {\n            var _iterator2 = _createForOfIteratorHelper(node.saved),\n                _step2;\n\n            try {\n              for (_iterator2.s(); !(_step2 = _iterator2.n()).done;) {\n                var tensor = _step2.value;\n                tensor.dispose();\n              }\n            } catch (err) {\n              _iterator2.e(err);\n            } finally {\n              _iterator2.f();\n            }\n          });\n\n          _this10.state.activeTape = null;\n        }\n\n        return {\n          value: y,\n          grads: grads\n        };\n      });\n    }\n  }, {\n    key: \"customGrad\",\n    value: function customGrad(f) {\n      var _this11 = this;\n\n      util.assert(util.isFunction(f), function () {\n        return 'The f passed in customGrad(f) must be a function.';\n      });\n      return function () {\n        for (var _len = arguments.length, inputs = new Array(_len), _key = 0; _key < _len; _key++) {\n          inputs[_key] = arguments[_key];\n        }\n\n        util.assert(inputs.every(function (t) {\n          return t instanceof Tensor;\n        }), function () {\n          return 'The args passed in customGrad(f)(x1, x2,...) must all be ' + 'tensors';\n        });\n        var res;\n        var inputMap = {};\n        inputs.forEach(function (input, i) {\n          inputMap[i] = input;\n        });\n        return _this11.runKernelFunc(function (_, save) {\n          res = f.apply(void 0, [].concat(inputs, [save]));\n          util.assert(res.value instanceof Tensor, function () {\n            return 'The function f passed in customGrad(f) must return an ' + 'object where `obj.value` is a tensor';\n          });\n          util.assert(util.isFunction(res.gradFunc), function () {\n            return 'The function f passed in customGrad(f) must return an ' + 'object where `obj.gradFunc` is a function.';\n          });\n          return res.value;\n        }, inputMap, function (dy, saved) {\n          var gradRes = res.gradFunc(dy, saved);\n          var grads = Array.isArray(gradRes) ? gradRes : [gradRes];\n          util.assert(grads.length === inputs.length, function () {\n            return 'The function f passed in customGrad(f) must return an ' + 'object where `obj.gradFunc` is a function that returns ' + 'the same number of tensors as inputs passed to f(...).';\n          });\n          util.assert(grads.every(function (t) {\n            return t instanceof Tensor;\n          }), function () {\n            return 'The function f passed in customGrad(f) must return an ' + 'object where `obj.gradFunc` is a function that returns ' + 'a list of only tensors.';\n          });\n          var gradMap = {};\n          grads.forEach(function (grad, i) {\n            gradMap[i] = function () {\n              return grad;\n            };\n          });\n          return gradMap;\n        });\n      };\n    }\n  }, {\n    key: \"readSync\",\n    value: function readSync(dataId) {\n      // Route the read to the correct backend.\n      var info = this.state.tensorInfo.get(dataId);\n      return info.backend.readSync(dataId);\n    }\n  }, {\n    key: \"read\",\n    value: function read(dataId) {\n      // Route the read to the correct backend.\n      var info = this.state.tensorInfo.get(dataId);\n      return info.backend.read(dataId);\n    }\n  }, {\n    key: \"time\",\n    value: function () {\n      var _time = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee4(query) {\n        var start, timingInfo;\n        return _regeneratorRuntime.wrap(function _callee4$(_context4) {\n          while (1) {\n            switch (_context4.prev = _context4.next) {\n              case 0:\n                start = now();\n                _context4.next = 3;\n                return this.backend.time(query);\n\n              case 3:\n                timingInfo = _context4.sent;\n                timingInfo.wallMs = now() - start;\n                return _context4.abrupt(\"return\", timingInfo);\n\n              case 6:\n              case \"end\":\n                return _context4.stop();\n            }\n          }\n        }, _callee4, this);\n      }));\n\n      function time(_x3) {\n        return _time.apply(this, arguments);\n      }\n\n      return time;\n    }()\n    /**\r\n     * Tracks a Tensor in the current scope to be automatically cleaned up\r\n     * when the current scope ends, and returns the value.\r\n     *\r\n     * @param result The Tensor to track in the current scope.\r\n     */\n\n  }, {\n    key: \"track\",\n    value: function track(result) {\n      if (this.state.activeScope != null) {\n        result.scopeId = this.state.activeScope.id;\n        this.state.activeScope.track.push(result);\n      }\n\n      return result;\n    }\n  }, {\n    key: \"reset\",\n\n    /**\r\n     * Resets the engine state. Removes all backends but does not remove\r\n     * registered backend factories.\r\n     */\n    value: function reset() {\n      // Make any pending promise obsolete.\n      this.pendingBackendInitId++;\n      this.state.dispose();\n      this.ENV.reset();\n      this.state = new EngineState();\n\n      for (var backendName in this.registry) {\n        this.disposeRegisteredKernels(backendName);\n        this.registry[backendName].dispose();\n        delete this.registry[backendName];\n      }\n\n      this.backendName = null;\n      this.backendInstance = null;\n      this.pendingBackendInit = null;\n    }\n  }, {\n    key: \"backend\",\n    get: function get() {\n      if (this.pendingBackendInit != null) {\n        throw new Error(\"Backend '\".concat(this.backendName, \"' has not yet been initialized. Make \") + \"sure to await tf.ready() or await tf.setBackend() before calling \" + \"other methods\");\n      }\n\n      if (this.backendInstance == null) {\n        var _this$initializeBacke4 = this.initializeBackendsAndReturnBest(),\n            name = _this$initializeBacke4.name,\n            asyncInit = _this$initializeBacke4.asyncInit;\n\n        if (asyncInit) {\n          throw new Error(\"The highest priority backend '\".concat(name, \"' has not yet been \") + \"initialized. Make sure to await tf.ready() or \" + \"await tf.setBackend() before calling other methods\");\n        }\n\n        this.setBackend(name);\n      }\n\n      return this.backendInstance;\n    }\n  }, {\n    key: \"registeredVariables\",\n    get: function get() {\n      return this.state.registeredVariables;\n    }\n  }]);\n\n  return Engine;\n}();\nEngine.nextTensorId = 0;\nEngine.nextVariableId = 0;\n\nfunction ones(shape) {\n  var values = makeOnesTypedArray(sizeFromShape(shape), 'float32');\n  return ENGINE.makeTensor(values, shape, 'float32');\n}\n\nexport function getOrMakeEngine() {\n  var ns = getGlobalNamespace();\n\n  if (ns._tfengine == null) {\n    var environment = new Environment(ns);\n    ns._tfengine = new Engine(environment);\n  }\n\n  setEnvironmentGlobal(ns._tfengine.ENV); // Tell the current tensor interface that the global engine is responsible\n  // for tracking.\n\n  setTensorTracker(function () {\n    return ns._tfengine;\n  });\n  return ns._tfengine;\n}\nexport var ENGINE = getOrMakeEngine();\n/**\r\n * A implementation of the add op for use within engine and tape.\r\n *\r\n * This allows us to avoid a circular dependency between add.ts and engine.\r\n * It is exported to be available in tape tests.\r\n */\n\nexport function add(a, b) {\n  // We duplicate Add here to avoid a circular dependency with add.ts.\n  var inputs = {\n    a: a,\n    b: b\n  };\n  return ENGINE.runKernelFunc(function (backend, save) {\n    var res = backend.add(a, b);\n    save([a, b]);\n    return res;\n  }, inputs, null\n  /* gradient */\n  , Add);\n}","map":{"version":3,"sources":["../src/engine.ts"],"names":[],"mappings":";;;;;;;AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAsC,aAAtC,QAA0D,oBAA1D;AACA,SAAQ,WAAR,EAAqB,oBAArB,QAAgD,eAAhD;AACA,SAAQ,kBAAR,QAAiC,eAAjC;AACA,SAAQ,GAAR,EAAa,IAAb,QAAwB,gBAAxB;AACA,SAAQ,WAAR,EAAqB,SAArB,EAAgC,oBAAhC,QAA+F,mBAA/F;AACA,SAAuB,QAAvB,QAAsC,YAAtC;AACA,SAAQ,sBAAR,EAAgC,oBAAhC,QAAqE,QAArE;AACA,SAAgB,gBAAhB,EAAkC,MAAlC,EAAyD,QAAzD,QAAwE,UAAxE;AAEA,SAAQ,qBAAR,QAAoC,eAApC;AAEA,OAAO,KAAK,IAAZ,MAAsB,QAAtB;AACA,SAAQ,oBAAR,EAA8B,kBAA9B,EAAkD,GAAlD,EAAuD,aAAvD,QAA2E,QAA3E;;IAsDM,W;AAAN,yBAAA;AAAA;;AACE;AACA,SAAA,mBAAA,GAAwC,EAAxC;AAEA,SAAA,cAAA,GAAiB,CAAjB;AACA,SAAA,QAAA,GAAW,CAAX;AACA,SAAA,UAAA,GAAa,CAAb;AACA,SAAA,gBAAA,GAAmB,CAAnB;AACA,SAAA,cAAA,GAAiB,CAAjB,CARF,CAWE;AACA;AACA;;AACA,SAAA,aAAA,GAAgB,CAAhB,CAdF,CAeE;AACA;;AACA,SAAA,WAAA,GAAc,CAAd;AAIA,SAAA,UAAA,GAA2B,EAA3B;AACA;;;AAGG;;AACH,SAAA,iBAAA,GAA8B,EAA9B;AACA,SAAA,WAAA,GAAc,CAAd;AAEA,SAAA,UAAA,GAAa,IAAI,OAAJ,EAAb;AAQA,SAAA,SAAA,GAAY,KAAZ;AACA,SAAA,aAAA,GACI;AAAC,MAAA,QAAQ,EAAE,CAAX;AAAc,MAAA,UAAU,EAAE,CAA1B;AAA6B,MAAA,SAAS,EAAE,CAAxC;AAA2C,MAAA,OAAO,EAAE,EAApD;AAAwD,MAAA,MAAM,EAAE;AAAhE,KADJ;AAQD;;;;8BALQ;AACL,WAAK,IAAM,YAAX,IAA2B,KAAK,mBAAhC,EAAqD;AACnD,aAAK,mBAAL,CAAyB,YAAzB,EAAuC,OAAvC;AACD;AACF;;;;;;AAGH,WAAa,MAAb;AAgBE,kBAAmB,GAAnB,EAAmC;AAAA;;AAAhB,SAAA,GAAA,GAAA,GAAA;AAbnB,SAAA,QAAA,GAA0C,EAA1C;AACA,SAAA,eAAA,GAKI,EALJ;AAUQ,SAAA,oBAAA,GAAuB,CAAvB;AAGN,SAAK,KAAL,GAAa,IAAI,WAAJ,EAAb;AACD;;AAlBH;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBAqBQ,KAAK,kBAAL,IAA2B,IArBnC;AAAA;AAAA;AAAA;;AAAA,iDAsBa,KAAK,kBAAL,CAAwB,IAAxB,CAA6B,YAAK,CAAG,CAArC,CAtBb;;AAAA;AAAA,sBAwBQ,KAAK,eAAL,IAAwB,IAxBhC;AAAA;AAAA;AAAA;;AAAA;;AAAA;AA2BU,gBAAA,cA3BV,GA2B2B,KAAK,iBAAL,EA3B3B;AA6Ba,gBAAA,CA7Bb,GA6BiB,CA7BjB;;AAAA;AAAA,sBA6BoB,CAAC,GAAG,cAAc,CAAC,MA7BvC;AAAA;AAAA;AAAA;;AA8BY,gBAAA,WA9BZ,GA8B0B,cAAc,CAAC,CAAD,CA9BxC;AAAA;AAAA,uBA+B4B,KAAK,iBAAL,CAAuB,WAAvB,EAAoC,OA/BhE;;AAAA;AA+BY,gBAAA,OA/BZ;;AAAA,qBAgCU,OAhCV;AAAA;AAAA;AAAA;;AAAA;AAAA,uBAiCc,KAAK,UAAL,CAAgB,WAAhB,CAjCd;;AAAA;AAAA;;AAAA;AA6B+C,gBAAA,CAAC,EA7BhD;AAAA;AAAA;;AAAA;AAAA,sBAsCU,IAAI,KAAJ,CACF,6EADE,CAtCV;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;AAAA;AAAA;AAAA;;AAAA;AAAA;AAAA;AAAA;AAAA,mCA+Dc;AACV,aAAO,MAAM,CAAC,IAAP,CAAY,KAAK,eAAjB,CAAP;AACD;AAjEH;AAAA;AAAA,gCAmEc,WAnEd,EAmEiC;AAC7B,UAAI,EAAE,WAAW,IAAI,KAAK,QAAtB,CAAJ,EAAqC;AACnC;AACA;AACA,YAAI,WAAW,IAAI,KAAK,eAAxB,EAAyC;AAAA,sCACnB,KAAK,iBAAL,CAAuB,WAAvB,CADmB;AAAA,cAChC,SADgC,yBAChC,SADgC;;AAEvC,cAAI,SAAJ,EAAe;AACb;AACA,mBAAO,IAAP;AACD;AACF,SAND,MAMO;AACL,iBAAO,IAAP;AACD;AACF;;AACD,aAAO,KAAK,QAAL,CAAc,WAAd,CAAP;AACD;AAlFH;AAAA;AAAA,uCAoFqB,WApFrB,EAoFwC;AAEpC,UAAI,EAAE,WAAW,IAAI,KAAK,eAAtB,CAAJ,EAA4C;AAC1C,eAAO,IAAP;AACD;;AACD,aAAO,KAAK,eAAL,CAAqB,WAArB,EAAkC,OAAzC;AACD;AA1FH;AAAA;AAAA,oCA6FM,WA7FN,EA8FM,OA9FN,EA+FkB;AAAA,UAAZ,QAAY,uEAAD,CAAC;;AACd,UAAI,WAAW,IAAI,KAAK,eAAxB,EAAyC;AACvC,QAAA,OAAO,CAAC,IAAR,CACI,UAAG,WAAH,4EADJ;AAGA,eAAO,KAAP;AACD;;AACD,WAAK,eAAL,CAAqB,WAArB,IAAoC;AAAC,QAAA,OAAO,EAAP,OAAD;AAAU,QAAA,QAAQ,EAAR;AAAV,OAApC;AACA,aAAO,IAAP;AACD;AAxGH;AAAA;AAAA;AAAA,mGA0GmB,WA1GnB;AAAA;;AAAA;AAAA;AAAA;AAAA;AAAA,sBA2GQ,KAAK,eAAL,CAAqB,WAArB,KAAqC,IA3G7C;AAAA;AAAA;AAAA;;AAAA,sBA4GY,IAAI,KAAJ,yBAA2B,WAA3B,6BA5GZ;;AAAA;AA8GI,qBAAK,WAAL,GAAmB,WAAnB;;AA9GJ,sBA+GQ,KAAK,QAAL,CAAc,WAAd,KAA8B,IA/GtC;AAAA;AAAA;AAAA;;AAgHM,qBAAK,eAAL,GAAuB,IAAvB;AAhHN,yCAiHmC,KAAK,iBAAL,CAAuB,WAAvB,CAjHnC,EAiHa,OAjHb,0BAiHa,OAjHb,EAiHsB,SAjHtB,0BAiHsB,SAjHtB;;AAAA,qBAkHqB,SAlHrB;AAAA;AAAA;AAAA;;AAAA;AAAA,uBAkHuC,OAlHvC;;AAAA;AAAA;AAAA;AAAA;;AAAA;AAAA,+BAkHiD,OAlHjD;;AAAA;AAkHY,gBAAA,MAlHZ;;AAAA,oBAmHW,MAnHX;AAAA;AAAA;AAAA;;AAAA,kDAoHe,KApHf;;AAAA;AAuHI,qBAAK,eAAL,GAAuB,KAAK,QAAL,CAAc,WAAd,CAAvB;AACA,qBAAK,sBAAL,GAxHJ,CAyHI;;AACA,qBAAK,QAAL,GAAgB,IAAI,QAAJ,CAAa,KAAK,eAAlB,CAAhB;AA1HJ,kDA4HW,IA5HX;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;AAAA;AAAA;AAAA;;AAAA;AAAA;AAAA;AAAA;AAAA,6CA+HgC;AAAA;;AAC5B,UAAM,OAAO,GAAG,oBAAoB,CAAC,KAAK,WAAN,CAApC;AACA,MAAA,OAAO,CAAC,OAAR,CAAgB,UAAA,MAAM,EAAG;AACvB,YAAI,MAAM,CAAC,SAAP,IAAoB,IAAxB,EAA8B;AAC5B,UAAA,MAAM,CAAC,SAAP,CAAiB,KAAI,CAAC,eAAtB;AACD;AACF,OAJD;AAKD;AAtIH;AAAA;AAAA,6CAwImC,WAxInC,EAwIsD;AAAA;;AAClD,UAAM,OAAO,GAAG,oBAAoB,CAAC,WAAD,CAApC;AACA,MAAA,OAAO,CAAC,OAAR,CAAgB,UAAA,MAAM,EAAG;AACvB,YAAI,MAAM,CAAC,WAAP,IAAsB,IAA1B,EAAgC;AAC9B,UAAA,MAAM,CAAC,WAAP,CAAmB,MAAI,CAAC,QAAL,CAAc,WAAd,CAAnB;AACD;AACF,OAJD;AAKD;AAED;;;;;AAKG;;AAtJL;AAAA;AAAA,sCAuJ4B,WAvJ5B,EAuJ+C;AAAA;;AAE3C,UAAM,oBAAoB,GAAG,KAAK,eAAL,CAAqB,WAArB,CAA7B;;AACA,UAAI,oBAAoB,IAAI,IAA5B,EAAkC;AAChC,cAAM,IAAI,KAAJ,qCAC2B,WAD3B,8BAAN;AAED;;AAED,UAAI;AACF,YAAM,OAAO,GAAG,oBAAoB,CAAC,OAArB,EAAhB;AACA;;;;AAIiC;;AACjC,YAAI,OAAO,IAAI,EAAE,OAAO,YAAY,aAArB,CAAX,IACG,OAAO,OAAO,CAAC,IAAf,KAAwB,UAD/B,EAC2C;AACzC,cAAM,SAAS,GAAG,EAAE,KAAK,oBAAzB;AACA,cAAM,OAAO,GACT,OAAO,CACF,IADL,CACU,UAAA,eAAe,EAAG;AACtB;AACA,gBAAI,SAAS,GAAG,MAAI,CAAC,oBAArB,EAA2C;AACzC,qBAAO,KAAP;AACD;;AACD,YAAA,MAAI,CAAC,QAAL,CAAc,WAAd,IAA6B,eAA7B;AACA,YAAA,MAAI,CAAC,kBAAL,GAA0B,IAA1B;AACA,mBAAO,IAAP;AACD,WATL,EAUK,KAVL,CAUW,UAAA,GAAG,EAAG;AACX;AACA,gBAAI,SAAS,GAAG,MAAI,CAAC,oBAArB,EAA2C;AACzC,qBAAO,KAAP;AACD;;AACD,YAAA,MAAI,CAAC,kBAAL,GAA0B,IAA1B;AACA,YAAA,OAAO,CAAC,IAAR,qCACiC,WADjC;AAEA,YAAA,OAAO,CAAC,IAAR,CAAa,GAAG,CAAC,KAAJ,IAAa,GAAG,CAAC,OAA9B;AACA,mBAAO,KAAP;AACD,WApBL,CADJ;AAsBA,eAAK,kBAAL,GAA0B,OAA1B;AACA,iBAAO;AAAC,YAAA,OAAO,EAAP,OAAD;AAAU,YAAA,SAAS,EAAE;AAArB,WAAP;AACD,SA3BD,MA2BO;AACL,eAAK,QAAL,CAAc,WAAd,IAA6B,OAA7B;AACA,iBAAO;AAAC,YAAA,OAAO,EAAE,IAAV;AAAgB,YAAA,SAAS,EAAE;AAA3B,WAAP;AACD;AACF,OAtCD,CAsCE,OAAO,GAAP,EAAY;AACZ,QAAA,OAAO,CAAC,IAAR,qCAA0C,WAA1C;AACA,QAAA,OAAO,CAAC,IAAR,CAAa,GAAG,CAAC,KAAJ,IAAa,GAAG,CAAC,OAA9B;AACA,eAAO;AAAC,UAAA,OAAO,EAAE,KAAV;AAAiB,UAAA,SAAS,EAAE;AAA5B,SAAP;AACD;AACF;AA1MH;AAAA;AAAA,kCA4MgB,WA5MhB,EA4MmC;AAC/B,UAAI,EAAE,WAAW,IAAI,KAAK,eAAtB,CAAJ,EAA4C;AAC1C,cAAM,IAAI,KAAJ,WAAa,WAAb,oCAAN;AACD;;AACD,UAAI,KAAK,WAAL,KAAqB,WAArB,IAAoC,KAAK,kBAAL,IAA2B,IAAnE,EAAyE;AACvE;AACA;AACA,aAAK,oBAAL;AACD;;AAED,UAAI,WAAW,IAAI,KAAK,QAAxB,EAAkC;AAChC,aAAK,wBAAL,CAA8B,WAA9B;AACA,aAAK,QAAL,CAAc,WAAd,EAA2B,OAA3B;AACA,eAAO,KAAK,QAAL,CAAc,WAAd,CAAP;AACD;;AAED,aAAO,KAAK,eAAL,CAAqB,WAArB,CAAP,CAhB+B,CAkB/B;;AACA,UAAI,KAAK,WAAL,KAAqB,WAAzB,EAAsC;AACpC,aAAK,kBAAL,GAA0B,IAA1B;AACA,aAAK,WAAL,GAAmB,IAAnB;AACA,aAAK,eAAL,GAAuB,IAAvB;AACD;AACF;AApOH;AAAA;AAAA,wCAsO2B;AAAA;;AACvB,UAAI,MAAM,CAAC,IAAP,CAAY,KAAK,eAAjB,EAAkC,MAAlC,KAA6C,CAAjD,EAAoD;AAClD,cAAM,IAAI,KAAJ,CAAU,+BAAV,CAAN;AACD;;AACD,aAAO,MAAM,CAAC,IAAP,CAAY,KAAK,eAAjB,EAAkC,IAAlC,CAAuC,UAAC,CAAD,EAAY,CAAZ,EAAyB;AACrE;AACA,eAAO,MAAI,CAAC,eAAL,CAAqB,CAArB,EAAwB,QAAxB,GACH,MAAI,CAAC,eAAL,CAAqB,CAArB,EAAwB,QAD5B;AAED,OAJM,CAAP;AAKD;AA/OH;AAAA;AAAA,sDAiPyC;AAErC,UAAM,cAAc,GAAG,KAAK,iBAAL,EAAvB;;AAEA,WAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,cAAc,CAAC,MAAnC,EAA2C,CAAC,EAA5C,EAAgD;AAC9C,YAAM,WAAW,GAAG,cAAc,CAAC,CAAD,CAAlC;;AAD8C,qCAEjB,KAAK,iBAAL,CAAuB,WAAvB,CAFiB;AAAA,YAEvC,OAFuC,0BAEvC,OAFuC;AAAA,YAE9B,SAF8B,0BAE9B,SAF8B;;AAG9C,YAAI,SAAS,IAAI,OAAjB,EAA0B;AACxB,iBAAO;AAAC,YAAA,IAAI,EAAE,WAAP;AAAoB,YAAA,SAAS,EAAT;AAApB,WAAP;AACD;AACF;;AACD,YAAM,IAAI,KAAJ,CACF,6EADE,CAAN;AAGD;AA/PH;AAAA;AAAA,6BAiQW,OAjQX,EAiQmC,MAjQnC,EAiQiD;AAC7C,UAAM,IAAI,GAAG,KAAK,KAAL,CAAW,UAAX,CAAsB,GAAtB,CAA0B,MAA1B,CAAb;AACA,UAAM,UAAU,GAAG,IAAI,CAAC,OAAxB;AACA,UAAM,MAAM,GAAG,KAAK,QAAL,CAAc,MAAd,CAAf,CAH6C,CAI7C;AACA;;AACA,MAAA,UAAU,CAAC,WAAX,CAAuB,MAAvB;AACA,MAAA,IAAI,CAAC,OAAL,GAAe,OAAf;AACA,MAAA,OAAO,CAAC,IAAR,CAAa,MAAb,EAAqB,MAArB,EAA6B,IAAI,CAAC,KAAlC,EAAyC,IAAI,CAAC,KAA9C;;AACA,UAAI,KAAK,sBAAL,EAAJ,EAAmC;AACjC;AACA;AACA,aAAK,KAAL,CAAW,iBAAX,CAA6B,KAAK,KAAL,CAAW,iBAAX,CAA6B,MAA7B,GAAsC,CAAnE;AACD;AACF;AA/QH;AAAA;AAAA,yBAiRkC,QAjRlC,EAiR+D,EAjR/D,EAiR8E;AAAA;;AAE1E,UAAI,IAAI,GAAW,IAAnB;;AACA,UAAI,EAAE,IAAI,IAAV,EAAgB;AACd;AACA,YAAI,OAAO,QAAP,KAAoB,UAAxB,EAAoC;AAClC,gBAAM,IAAI,KAAJ,CAAU,qCAAV,CAAN;AACD;;AACD,QAAA,EAAE,GAAG,QAAL;AACD,OAND,MAMO;AACL;AACA,YAAI,OAAO,QAAP,KAAoB,QAApB,IAAgC,EAAE,QAAQ,YAAY,MAAtB,CAApC,EAAmE;AACjE,gBAAM,IAAI,KAAJ,CACF,yDACA,4BAFE,CAAN;AAGD;;AACD,YAAI,OAAO,EAAP,KAAc,UAAlB,EAA8B;AAC5B,gBAAM,IAAI,KAAJ,CACF,uDACA,8BAFE,CAAN;AAGD;;AACD,QAAA,IAAI,GAAG,QAAP,CAZK,CAaL;AACA;AACD;;AACD,UAAI,MAAJ;AACA,aAAO,KAAK,SAAL,CACH;AAAA,eAAM,MAAI,CAAC,UAAL,CAAgB,IAAhB,CAAN;AAAA,OADG,EAC0B;AAAA,eAAM,MAAI,CAAC,QAAL,CAAc,MAAd,CAAN;AAAA,OAD1B,EACuD,YAAK;AAC7D,QAAA,MAAM,GAAG,EAAE,EAAX;;AACA,YAAI,MAAM,YAAY,OAAtB,EAA+B;AAC7B,UAAA,OAAO,CAAC,KAAR,CAAc,yCAAd;AACD;;AACD,eAAO,MAAP;AACD,OAPE,CAAP;AAQD;AAnTH;AAAA;AAAA,8BAqTuB,KArTvB,EAqT0C,GArT1C,EAqT2D,CArT3D,EAqTqE;AACjE,MAAA,KAAK;;AACL,UAAI;AACF,YAAM,GAAG,GAAG,CAAC,EAAb;AACA,QAAA,GAAG;AACH,eAAO,GAAP;AACD,OAJD,CAIE,OAAO,EAAP,EAAW;AACX,QAAA,GAAG;AACH,cAAM,EAAN;AACD;AACF;AA/TH;AAAA;AAAA,mCAkUsB;AAClB,aAAO,MAAM,CAAC,YAAP,EAAP;AACD;AApUH;AAAA;AAAA,qCAuUwB;AACpB,aAAO,MAAM,CAAC,cAAP,EAAP;AACD;AAED;;;;;;;;AAQG;;AAnVL;AAAA;AAAA,0BAoVgB,CApVhB,EAoVyB;AACrB,UAAM,CAAC,GAAG,KAAK,oBAAL,CAA0B,CAAC,CAAC,MAA5B,EAAoC,CAAC,CAAC,KAAtC,EAA6C,CAAC,CAAC,KAA/C,CAAV;AACA,UAAM,MAAM,GAAG;AAAC,QAAA,CAAC,EAAD;AAAD,OAAf;;AACA,UAAM,IAAI,GAAG,SAAP,IAAO,CAAC,EAAD;AAAA,eAAiB;AAC5B,UAAA,CAAC,EAAE,aAAK;AACN,gBAAM,KAAK,GAAG,SAAd;AACA,gBAAM,UAAU,GAAG;AAAC,cAAA,CAAC,EAAE;AAAJ,aAAnB;AACA,gBAAM,KAAK,GAAG;AAAC,cAAA,KAAK,EAAL;AAAD,aAAd;AAEA,mBAAO,MAAM,CAAC,aAAP,CACH,UAAA,OAAO;AAAA,qBAAI,OAAO,CAAC,IAAR,CAAa,EAAb,EAAiB,KAAjB,CAAJ;AAAA,aADJ,EAEH,UAFG,EAEiC;AAAK;AAFtC,cAEkD,IAFlD,EAGH,KAHG,CAAP;AAID;AAV2B,SAAjB;AAAA,OAAb;;AAYA,UAAM,KAAK,GAAa,EAAxB;AACA,WAAK,WAAL,CAAiB,KAAK,KAAL,CAAW,WAAX,CAAuB,IAAxC,EAA8C,MAA9C,EAAsD,CAAC,CAAD,CAAtD,EAA2D,IAA3D,EAAiE,KAAjE,EAAwE,EAAxE;AACA,aAAO,CAAP;AACD;AAED;;;;;;;;;;;;AAYG;;AApXL;AAAA;AAAA,8BAsXM,UAtXN,EAsX0B,MAtX1B,EAsXkD,KAtXlD,EAuXM,YAvXN,EAuX+B,aAvX/B,EAuXwD;AACpD,UAAM,WAAW,GAAS,IAA1B;AACA,UAAM,aAAa,GAAS,IAA5B,CAFoD,CAGpD;AACA;AACA;;AACA,aAAO,KAAK,aAAL,CACH,WADG,EACU,MADV,EACkB,aADlB,EACiC,UADjC,EAC6C,KAD7C,EACoD,YADpD,EAEH,aAFG,CAAP;AAGD;AAhYH;AAAA;AAAA,6CAkYgC;AAC5B,aAAO,KAAK,GAAL,CAAS,OAAT,CAAiB,SAAjB,CAAP;AACD;AApYH;AAAA;AAAA,0CAuYM,UAvYN,EAuY0B,gBAvY1B,EAwYM,QAxYN,EAwY4B;AACxB,UAAM,eAAe,GAAG,KAAK,OAAL,CAAa,UAAb,EAAxB,CADwB,CAGxB;;AACA,UAAI,gBAAgB,GAAG,CAAvB;AACA,MAAA,QAAQ,CAAC,OAAT,CAAiB,UAAA,IAAI,EAAG;AACtB;AACA;AACA,QAAA,gBAAgB,IAAK,IAAI,CAAC,KAAL,KAAe,WAAf,GAA6B,CAA7B,GAAiC,CAAtD;AACD,OAJD,EALwB,CAWxB;AACA;AACA;AACA;AACA;;AACA,UAAM,QAAQ,GACV,KAAK,KAAL,CAAW,iBAAX,CAA6B,KAAK,KAAL,CAAW,iBAAX,CAA6B,MAA7B,GAAsC,CAAnE,CADJ;AAEA,UAAM,aAAa,GACf,eAAe,GAAG,gBAAlB,GAAqC,gBAArC,GAAwD,QAD5D;;AAEA,UAAI,aAAa,GAAG,CAApB,EAAuB;AACrB,cAAM,IAAI,KAAJ,CACF,mBAAY,KAAK,WAAjB,iDACI,aADJ,uCAC8C,UAD9C,MADE,CAAN;AAGD;AACF;AAED;;;AAGG;;AAtaL;AAAA;AAAA,kCAwaM,WAxaN,EAwamC,MAxanC,EAyaM,aAzaN,EA0aM,UA1aN,EA0a2B,KA1a3B,EA0aiD,YA1ajD,EA2aM,aA3aN,EA2a+B;AAAA;;AAC3B,UAAI,OAAJ;AACA,UAAI,KAAK,GAAa,EAAtB;AACA,UAAM,QAAQ,GAAG,KAAK,QAAL,EAAjB;;AACA,UAAI,UAAU,IAAI,IAAlB,EAAwB;AACtB,QAAA,UAAU,GACN,KAAK,KAAL,CAAW,WAAX,IAA0B,IAA1B,GAAiC,KAAK,KAAL,CAAW,WAAX,CAAuB,IAAxD,GAA+D,EADnE;AAED;;AAED,UAAM,iBAAiB,GAAG,KAAK,KAAL,CAAW,QAArC;AACA,UAAM,kBAAkB,GAAG,KAAK,KAAL,CAAW,UAAtC;;AAEA,UAAI,KAAK,sBAAL,EAAJ,EAAmC;AACjC,aAAK,KAAL,CAAW,iBAAX,CAA6B,IAA7B,CAAkC,CAAlC;AACD;;AAED,UAAI,UAAJ;AACA,UAAM,MAAM,GAAG,SAAS,CAAC,UAAD,EAAa,KAAK,WAAlB,CAAxB;AACA,UAAI,GAAJ;;AACA,UAAI,MAAM,IAAI,IAAd,EAAoB;AAClB,QAAA,UAAU,GAAG,sBAAK;AAChB,cAAM,gBAAgB,GAAG,MAAI,CAAC,OAAL,CAAa,UAAb,EAAzB;;AACA,UAAA,GAAG,GAAG,MAAM,CAAC,UAAP,CAAkB;AAAC,YAAA,MAAM,EAAN,MAAD;AAAS,YAAA,KAAK,EAAL,KAAT;AAAgB,YAAA,OAAO,EAAE,MAAI,CAAC;AAA9B,WAAlB,CAAN;AACA,cAAM,QAAQ,GAAG,KAAK,CAAC,OAAN,CAAc,GAAd,IAAqB,GAArB,GAA2B,CAAC,GAAD,CAA5C;;AACA,cAAI,MAAI,CAAC,sBAAL,EAAJ,EAAmC;AACjC,YAAA,MAAI,CAAC,qBAAL,CAA2B,UAA3B,EAAuC,gBAAvC,EAAyD,QAAzD;AACD;;AACD,cAAM,UAAU,GAAG,QAAQ,CAAC,GAAT,CACf;AAAA,gBAAE,MAAF,QAAE,MAAF;AAAA,gBAAU,KAAV,QAAU,KAAV;AAAA,gBAAiB,KAAjB,QAAiB,KAAjB;AAAA,mBACI,MAAI,CAAC,oBAAL,CAA0B,MAA1B,EAAkC,KAAlC,EAAyC,KAAzC,CADJ;AAAA,WADe,CAAnB,CAPgB,CAWhB;AACA;AACA;AACA;;AACA,cAAI,QAAJ,EAAc;AACZ,gBAAI,aAAa,GACb,MAAI,CAAC,qBAAL,CAA2B,UAA3B,EAAuC,MAAvC,EAA+C,UAA/C,CADJ;;AAEA,gBAAI,aAAa,IAAI,IAArB,EAA2B;AACzB;AACA;AACA;AACA;AACA,kBAAI,aAAa,IAAI,IAArB,EAA2B;AACzB,gBAAA,aAAa,GAAG,EAAhB;AACD;;AACD,kBAAM,UAAU,GAAG,UAAU,CAAC,MAAX,CAAkB,UAAC,CAAD,EAAI,CAAJ;AAAA,uBAAU,aAAa,CAAC,CAAD,CAAvB;AAAA,eAAlB,CAAnB;AACA,cAAA,aAAa,GAAG,CAAC,YAAY,IAAI,EAAjB,EAAqB,KAArB,GAA6B,MAA7B,CAAoC,UAApC,CAAhB;AACD;;AACD,YAAA,KAAK,GAAG,MAAI,CAAC,0BAAL,CAAgC,aAAhC,CAAR;AACD;;AACD,iBAAO,UAAP;AACD,SAhCD;AAiCD,OAlCD,MAkCO;AACL,YAAM,QAAQ,GAAiB,SAAzB,QAAyB,CAAC,OAAD,EAAY;AACzC;AACA;AACA;AACA,cAAI,CAAC,QAAL,EAAe;AACb;AACD;;AACD,UAAA,KAAK,GAAG,OAAO,CAAC,GAAR,CAAY,UAAA,MAAM;AAAA,mBAAI,MAAI,CAAC,IAAL,CAAU,MAAI,CAAC,KAAL,CAAW,MAAX,CAAV,CAAJ;AAAA,WAAlB,CAAR;AACD,SARD;;AAUA,QAAA,UAAU,GAAG,sBAAK;AAChB,cAAM,gBAAgB,GAAG,MAAI,CAAC,OAAL,CAAa,UAAb,EAAzB;;AACA,UAAA,GAAG,GAAG,MAAI,CAAC,IAAL,CAAU;AAAA,mBAAM,WAAW,CAAC,MAAI,CAAC,OAAN,EAAe,QAAf,CAAjB;AAAA,WAAV,CAAN;AACA,cAAM,IAAI,GAAI,KAAK,CAAC,OAAN,CAAc,GAAd,IAAqB,GAArB,GAA2B,CAAC,GAAD,CAAzC;;AACA,cAAI,MAAI,CAAC,sBAAL,EAAJ,EAAmC;AACjC,YAAA,MAAI,CAAC,qBAAL,CAA2B,UAA3B,EAAuC,gBAAvC,EAAyD,IAAzD;AACD;;AACD,iBAAO,IAAP;AACD,SARD;AASD,OAzE0B,CA2E3B;;;AACA,UAAI,aAAJ;AACA,WAAK,SAAL,CACI;AAAA,eAAM,MAAI,CAAC,KAAL,CAAW,WAAX,EAAN;AAAA,OADJ,EACoC;AAAA,eAAM,MAAI,CAAC,KAAL,CAAW,WAAX,EAAN;AAAA,OADpC,EACoE,YAAK;AACnE,YAAI,CAAC,MAAI,CAAC,GAAL,CAAS,OAAT,CAAiB,OAAjB,CAAD,IAA8B,CAAC,MAAI,CAAC,KAAL,CAAW,SAA9C,EAAyD;AACvD,UAAA,OAAO,GAAG,UAAU,EAApB;AACD,SAFD,MAEO;AACL,UAAA,aAAa,GAAG,MAAI,CAAC,QAAL,CAAc,aAAd,CACZ,UADY,EACA,MADA,EACQ;AAAA,mBAAM,UAAU,EAAhB;AAAA,WADR,CAAhB;;AAEA,cAAI,MAAI,CAAC,GAAL,CAAS,OAAT,CAAiB,OAAjB,CAAJ,EAA+B;AAC7B,YAAA,MAAI,CAAC,QAAL,CAAc,gBAAd,CAA+B,aAA/B;AACD;;AACD,UAAA,OAAO,GAAG,aAAa,CAAC,OAAxB;AACD;AACF,OAZL;;AAcA,UAAI,QAAJ,EAAc;AACZ,aAAK,WAAL,CACI,UADJ,EACgB,MADhB,EACwB,OADxB,EACiC,aADjC,EACgD,KADhD,EACuD,KADvD;AAED;;AAED,UAAI,KAAK,KAAL,CAAW,SAAf,EAA0B;AACxB,aAAK,KAAL,CAAW,aAAX,CAAyB,OAAzB,CAAiC,IAAjC,CAAsC;AACpC,UAAA,IAAI,EAAE,UAD8B;AAEpC,UAAA,UAAU,EAAE,KAAK,KAAL,CAAW,QAAX,GAAsB,iBAFE;AAGpC,UAAA,kBAAkB,EAAE,KAAK,KAAL,CAAW,QAHK;AAIpC,UAAA,YAAY,EAAE,KAAK,KAAL,CAAW,UAAX,GAAwB,kBAJF;AAKpC,UAAA,oBAAoB,EAAE,KAAK,KAAL,CAAW,UALG;AAMpC,UAAA,WAAW,EAAE,MAAM,CAAC,IAAP,CAAY,MAAZ,EAAoB,GAApB,CACT,UAAA,GAAG;AAAA,mBAAI,MAAM,CAAC,GAAD,CAAN,IAAe,IAAf,GAAsB,MAAM,CAAC,GAAD,CAAN,CAAY,KAAlC,GAA0C,IAA9C;AAAA,WADM,CANuB;AAQpC,UAAA,YAAY,EAAE,OAAO,CAAC,GAAR,CAAY,UAAA,IAAI;AAAA,mBAAI,IAAI,CAAC,KAAT;AAAA,WAAhB,CARsB;AASpC,UAAA,YAAY,EAAE,aAAa,CAAC,MATQ;AAUpC,UAAA,SAAS,EAAE,aAAa,CAAC;AAVW,SAAtC;AAYD;;AACD,aAAQ,KAAK,CAAC,OAAN,CAAc,GAAd,IAAqB,OAArB,GAA+B,OAAO,CAAC,CAAD,CAA9C;AACD;AAED;;;;AAIG;;AAhiBL;AAAA;AAAA,+CAiiBqC,OAjiBrC,EAiiBsD;AAAA;;AAClD,UAAM,KAAK,GAAG,OAAO,CAAC,GAAR,CAAY,UAAA,MAAM;AAAA,eAAI,MAAI,CAAC,IAAL,CAAU,MAAI,CAAC,KAAL,CAAW,MAAX,CAAV,CAAJ;AAAA,OAAlB,CAAd;AACA,aAAO,KAAP;AACD;AAED;;;;;;;;;AASG;;AA/iBL;AAAA;AAAA,0CAijBM,UAjjBN,EAijB0B,MAjjB1B,EAkjBM,OAljBN,EAkjBuB;AACnB,UAAM,UAAU,GAAG,WAAW,CAAC,UAAD,CAA9B;;AACA,UAAI,UAAU,IAAI,IAAlB,EAAwB;AACtB,YAAM,YAAY,GAAa,UAAU,CAAC,YAAX,IAA2B,EAA1D;AACA,YAAM,aAAa,GAAc,UAAU,CAAC,aAAX,IAA4B,EAA7D,CAFsB,CAItB;AACA;;AACA,YAAI,kBAAJ;;AACA,YAAI,UAAU,CAAC,aAAf,EAA8B;AAC5B,UAAA,IAAI,CAAC,MAAL,CACI,KAAK,CAAC,OAAN,CAAc,MAAd,CADJ,EAEI;AAAA,mBAAM,wDAAN;AAAA,WAFJ;AAIA,UAAA,kBAAkB,GAAG,MAAM,CAAC,IAAP,CAAY,MAAZ,EAAoB,GAApB,CAAwB,UAAC,GAAD;AAAA,mBAAS,MAAM,CAAC,GAAD,CAAf;AAAA,WAAxB,CAArB;AACD,SAND,MAMO;AACL,UAAA,kBAAkB,GAAG,YAAY,CAAC,GAAb,CAAiB,UAAC,SAAD;AAAA,mBAAe,MAAM,CAAC,SAAD,CAArB;AAAA,WAAjB,CAArB;AACD;;AAED,YAAM,mBAAmB,GACrB,OAAO,CAAC,MAAR,CAAe,UAAC,CAAD,EAAI,CAAJ;AAAA,iBAAU,aAAa,CAAC,CAAD,CAAvB;AAAA,SAAf,CADJ;AAGA,eAAO,kBAAkB,CAAC,MAAnB,CAA0B,mBAA1B,CAAP;AACD,OAvBkB,CAwBnB;AACA;;;AACA,aAAO,IAAP;AACD;AAED;;;;AAIG;;AAnlBL;AAAA;AAAA,+BAqlBM,MArlBN,EAqlB0B,KArlB1B,EAqlB2C,KArlB3C,EAslBM,OAtlBN,EAslB6B;AACzB,UAAI,MAAM,IAAI,IAAd,EAAoB;AAClB,cAAM,IAAI,KAAJ,CAAU,+CAAV,CAAN;AACD;;AACD,MAAA,KAAK,GAAG,KAAK,IAAI,SAAjB;AACA,MAAA,OAAO,GAAG,OAAO,IAAI,KAAK,OAA1B;AACA,UAAI,WAAW,GAAG,MAAlB;;AACA,UAAI,KAAK,KAAK,QAAV,IAAsB,IAAI,CAAC,QAAL,CAAc,MAAM,CAAC,CAAD,CAApB,CAA1B,EAAoD;AAClD,QAAA,WAAW,GAAI,MAAmB,CAAC,GAApB,CAAwB,UAAA,CAAC;AAAA,iBAAI,IAAI,CAAC,YAAL,CAAkB,CAAlB,CAAJ;AAAA,SAAzB,CAAf;AACD;;AACD,UAAM,MAAM,GAAG,OAAO,CAAC,KAAR,CAAc,WAAd,EAA2B,KAA3B,EAAkC,KAAlC,CAAf;AACA,UAAM,CAAC,GAAG,IAAI,MAAJ,CAAW,KAAX,EAAkB,KAAlB,EAAyB,MAAzB,EAAiC,KAAK,YAAL,EAAjC,CAAV;AACA,WAAK,MAAL,CAAY,CAAZ,EAAe,OAAf,EAZyB,CAczB;;AACA,UAAI,KAAK,KAAK,QAAd,EAAwB;AACtB,YAAM,IAAI,GAAG,KAAK,KAAL,CAAW,UAAX,CAAsB,GAAtB,CAA0B,MAA1B,CAAb;AACA,YAAM,QAAQ,GAAG,oBAAoB,CAAC,WAAD,CAArC;AACA,aAAK,KAAL,CAAW,QAAX,IAAuB,QAAQ,GAAG,IAAI,CAAC,KAAvC;AACA,QAAA,IAAI,CAAC,KAAL,GAAa,QAAb;AACD;;AACD,aAAO,CAAP;AACD;AAED;;;;AAIG;;AAlnBL;AAAA;AAAA,yCAonBM,MApnBN,EAonBsB,KApnBtB,EAonBuC,KApnBvC,EAqnBM,OArnBN,EAqnB6B;AACzB,MAAA,KAAK,GAAG,KAAK,IAAI,SAAjB;AACA,UAAM,CAAC,GAAG,IAAI,MAAJ,CAAW,KAAX,EAAkB,KAAlB,EAAyB,MAAzB,EAAiC,KAAK,YAAL,EAAjC,CAAV;AACA,WAAK,MAAL,CAAY,CAAZ,EAAe,OAAf;AACA,aAAO,CAAP;AACD;AA1nBH;AAAA;AAAA,iCA6nBM,YA7nBN,EA8nBsB;AAAA,UADM,SACN,uEADkB,IAClB;AAAA,UADwB,IACxB;AAAA,UAAhB,KAAgB;AAClB,MAAA,IAAI,GAAG,IAAI,IAAI,KAAK,cAAL,GAAsB,QAAtB,EAAf;;AACA,UAAI,KAAK,IAAI,IAAT,IAAiB,KAAK,KAAK,YAAY,CAAC,KAA5C,EAAmD;AACjD,QAAA,YAAY,GAAG,YAAY,CAAC,IAAb,CAAkB,KAAlB,CAAf;AACD;;AACD,UAAM,CAAC,GAAG,IAAI,QAAJ,CAAa,YAAb,EAA2B,SAA3B,EAAsC,IAAtC,EAA4C,KAAK,YAAL,EAA5C,CAAV;;AACA,UAAI,KAAK,KAAL,CAAW,mBAAX,CAA+B,CAAC,CAAC,IAAjC,KAA0C,IAA9C,EAAoD;AAClD,cAAM,IAAI,KAAJ,8BAAgC,CAAC,CAAC,IAAlC,6BAAN;AACD;;AACD,WAAK,KAAL,CAAW,mBAAX,CAA+B,CAAC,CAAC,IAAjC,IAAyC,CAAzC;AACA,WAAK,MAAL,CAAY,CAAZ,EAAe,KAAK,OAApB;AACA,aAAO,CAAP;AACD;AA1oBH;AAAA;AAAA,2BA4oBS,CA5oBT,EA4oBoB,OA5oBpB,EA4oB0C;AACtC,UAAM,QAAQ,GAAG,KAAK,KAAL,CAAW,UAAX,CAAsB,GAAtB,CAA0B,CAAC,CAAC,MAA5B,IACb,KAAK,KAAL,CAAW,UAAX,CAAsB,GAAtB,CAA0B,CAAC,CAAC,MAA5B,EAAoC,QADvB,GAEb,CAFJ;AAGA,WAAK,KAAL,CAAW,UAAX;;AACA,UAAI,CAAC,CAAC,KAAF,KAAY,QAAhB,EAA0B;AACxB,aAAK,KAAL,CAAW,gBAAX;AACD;;AACD,UAAI,QAAQ,KAAK,CAAjB,EAAoB;AAClB,aAAK,KAAL,CAAW,cAAX,GADkB,CAGlB;AACA;;AACA,YAAI,KAAK,GAAG,CAAZ;;AACA,YAAI,CAAC,CAAC,KAAF,KAAY,WAAZ,IAA2B,CAAC,CAAC,KAAF,KAAY,QAA3C,EAAqD;AACnD,UAAA,KAAK,GAAG,CAAC,CAAC,IAAF,GAAS,IAAI,CAAC,eAAL,CAAqB,CAAC,CAAC,KAAvB,CAAjB;AACD;;AACD,aAAK,KAAL,CAAW,UAAX,CAAsB,GAAtB,CAA0B,CAAC,CAAC,MAA5B,EAAoC;AAClC,UAAA,OAAO,EAAE,OAAO,IAAI,KAAK,OADS;AAElC,UAAA,KAAK,EAAE,CAAC,CAAC,KAFyB;AAGlC,UAAA,KAAK,EAAE,CAAC,CAAC,KAHyB;AAIlC,UAAA,KAAK,EAAL,KAJkC;AAKlC,UAAA,QAAQ,EAAE;AALwB,SAApC;AAOA,aAAK,KAAL,CAAW,QAAX,IAAuB,KAAvB;AACD;;AAED,WAAK,KAAL,CAAW,UAAX,CAAsB,GAAtB,CAA0B,CAAC,CAAC,MAA5B,EAAoC,QAApC;;AAEA,UAAI,EAAE,CAAC,YAAY,QAAf,CAAJ,EAA8B;AAC5B,aAAK,KAAL,CAAW,CAAX;AACD;AACF;AA5qBH;AAAA;AAAA,kCA8qBgB,CA9qBhB,EA8qByB;AACrB,UAAI,CAAC,KAAK,KAAL,CAAW,UAAX,CAAsB,GAAtB,CAA0B,CAAC,CAAC,MAA5B,CAAL,EAA0C;AACxC;AACD;;AAED,WAAK,KAAL,CAAW,UAAX;;AACA,UAAI,CAAC,CAAC,KAAF,KAAY,QAAhB,EAA0B;AACxB,aAAK,KAAL,CAAW,gBAAX;AACD;;AACD,UAAM,IAAI,GAAG,KAAK,KAAL,CAAW,UAAX,CAAsB,GAAtB,CAA0B,CAAC,CAAC,MAA5B,CAAb;AACA,UAAM,QAAQ,GAAG,IAAI,CAAC,QAAtB;;AAEA,UAAI,QAAQ,IAAI,CAAhB,EAAmB;AACjB;AACA;AACA,YAAI,CAAC,CAAC,KAAF,KAAY,WAAhB,EAA6B;AAC3B,eAAK,KAAL,CAAW,QAAX,IAAuB,IAAI,CAAC,KAA5B;AACD;;AACD,aAAK,KAAL,CAAW,cAAX;AAEA,QAAA,IAAI,CAAC,OAAL,CAAa,WAAb,CAAyB,CAAC,CAAC,MAA3B;AACA,aAAK,KAAL,CAAW,UAAX,CAAsB,MAAtB,CAA6B,CAAC,CAAC,MAA/B;AACD,OAVD,MAUO;AACL,aAAK,KAAL,CAAW,UAAX,CAAsB,GAAtB,CAA0B,CAAC,CAAC,MAA5B,EAAoC,QAApC;AACD,OAxBoB,CAyBrB;AACA;AACA;;AACD;AA1sBH;AAAA;AAAA,uCA4sBkB;AACd,WAAK,IAAM,OAAX,IAAsB,KAAK,KAAL,CAAW,mBAAjC,EAAsD;AACpD,YAAM,CAAC,GAAG,KAAK,KAAL,CAAW,mBAAX,CAA+B,OAA/B,CAAV;AACA,aAAK,eAAL,CAAqB,CAArB;AACD;AACF;AAjtBH;AAAA;AAAA,oCAmtBkB,CAntBlB,EAmtB6B;AACzB,WAAK,aAAL,CAAmB,CAAnB;;AACA,UAAI,KAAK,KAAL,CAAW,mBAAX,CAA+B,CAAC,CAAC,IAAjC,KAA0C,IAA9C,EAAoD;AAClD,eAAO,KAAK,KAAL,CAAW,mBAAX,CAA+B,CAAC,CAAC,IAAjC,CAAP;AACD;AACF;AAxtBH;AAAA;AAAA,6BA0tBQ;AACJ,UAAM,IAAI,GAAG,KAAK,OAAL,CAAa,MAAb,EAAb;AACA,MAAA,IAAI,CAAC,UAAL,GAAkB,KAAK,KAAL,CAAW,UAA7B;AACA,MAAA,IAAI,CAAC,cAAL,GAAsB,KAAK,KAAL,CAAW,cAAjC;AACA,MAAA,IAAI,CAAC,QAAL,GAAgB,KAAK,KAAL,CAAW,QAA3B;;AACA,UAAI,KAAK,KAAL,CAAW,gBAAX,GAA8B,CAAlC,EAAqC;AACnC,QAAA,IAAI,CAAC,UAAL,GAAkB,IAAlB;;AACA,YAAI,IAAI,CAAC,OAAL,IAAgB,IAApB,EAA0B;AACxB,UAAA,IAAI,CAAC,OAAL,GAAe,EAAf;AACD;;AACD,QAAA,IAAI,CAAC,OAAL,CAAa,IAAb,CACI,mDACA,yBAFJ;AAGD;;AACD,aAAO,IAAP;AACD;AAzuBH;AAAA;AAAA;AAAA,gGA2uBgB,KA3uBhB;AAAA;;AAAA;AAAA;AAAA;AAAA;AA6uBI,qBAAK,KAAL,CAAW,SAAX,GAAuB,IAAvB;AAEM,gBAAA,UA/uBV,GA+uBuB,KAAK,KAAL,CAAW,QA/uBlC;AAgvBU,gBAAA,eAhvBV,GAgvB4B,KAAK,KAAL,CAAW,UAhvBvC;AAkvBI,qBAAK,KAAL,CAAW,aAAX,CAAyB,OAAzB,GAAmC,EAAnC;AAlvBJ;AAAA,uBAmvB4C,KAAK,EAnvBjD;;AAAA;AAmvBI,qBAAK,KAAL,CAAW,aAAX,CAAyB,MAnvB7B;AAqvBI,qBAAK,KAAL,CAAW,SAAX,GAAuB,KAAvB;AAEA,qBAAK,KAAL,CAAW,aAAX,CAAyB,SAAzB,GAAqC,IAAI,CAAC,GAAL,OAAA,IAAI,qBAClC,KAAK,KAAL,CAAW,aAAX,CAAyB,OAAzB,CAAiC,GAAjC,CAAqC,UAAA,CAAC;AAAA,yBAAI,CAAC,CAAC,kBAAN;AAAA,iBAAtC,CADkC,EAAzC;AAEA,qBAAK,KAAL,CAAW,aAAX,CAAyB,QAAzB,GAAoC,KAAK,KAAL,CAAW,QAAX,GAAsB,UAA1D;AACA,qBAAK,KAAL,CAAW,aAAX,CAAyB,UAAzB,GACI,KAAK,KAAL,CAAW,UAAX,GAAwB,eAD5B;AA1vBJ,uDA4vByB,KAAK,KAAL,CAAW,aAAX,CAAyB,OA5vBlD;AAAA;;AAAA;;AAAA;AAAA;AAAA;AAAA;AAAA;;AA4vBe,gBAAA,MA5vBf;AAAA;AAAA,uBA6vBkC,MAAM,CAAC,YA7vBzC;;AAAA;AA6vBM,gBAAA,MAAM,CAAC,YA7vBb;AAAA;AAAA,uBA8vB+B,MAAM,CAAC,SA9vBtC;;AAAA;AA8vBM,gBAAA,MAAM,CAAC,SA9vBb;;AAAA;AAAA;AAAA;;AAAA;AAAA;AAAA;;AAAA;AAAA;AAAA;;AAAA;;AAAA;AAAA;;AAAA;;AAAA;;AAAA;AAAA,kDAgwBW,KAAK,KAAL,CAAW,aAhwBtB;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;AAAA;AAAA;AAAA;;AAAA;AAAA;AAAA;AAAA;AAAA,+BAmwBU;AACN,aAAO,KAAK,KAAL,CAAW,aAAX,GAA2B,CAA3B,IAAgC,KAAK,KAAL,CAAW,WAAX,KAA2B,CAAlE;AACD;AArwBH;AAAA;AAAA,gCAwwBM,UAxwBN,EAwwB0B,MAxwB1B,EAwwBkD,OAxwBlD,EAywBM,aAzwBN,EAywB+B,KAzwB/B,EAywBgD,KAzwBhD,EAywBmE;AAAA;;AAC/D,UAAM,QAAQ,GACV;AAAC,QAAA,EAAE,EAAE,KAAK,KAAL,CAAW,cAAX,EAAL;AAAkC,QAAA,UAAU,EAAV,UAAlC;AAA8C,QAAA,MAAM,EAAN,MAA9C;AAAsD,QAAA,OAAO,EAAP,OAAtD;AAA+D,QAAA,KAAK,EAAL;AAA/D,OADJ;AAGA,UAAM,UAAU,GAAG,WAAW,CAAC,UAAD,CAA9B;;AACA,UAAI,UAAU,IAAI,IAAlB,EAAwB;AACtB,QAAA,aAAa,GAAG,UAAU,CAAC,QAA3B;AACD;;AACD,UAAI,aAAa,IAAI,IAArB,EAA2B;AACzB,QAAA,QAAQ,CAAC,QAAT,GAAoB,UAAC,GAAD,EAAkB;AACpC;AACA;AACA,UAAA,GAAG,GAAG,GAAG,CAAC,GAAJ,CAAQ,UAAC,EAAD,EAAK,CAAL,EAAU;AACtB,gBAAI,EAAE,IAAI,IAAV,EAAgB;AACd,kBAAM,MAAM,GAAG,OAAO,CAAC,CAAD,CAAtB;AACA,kBAAM,IAAI,GAAG,IAAI,CAAC,mBAAL,CAAyB,MAAM,CAAC,IAAhC,EAAsC,MAAM,CAAC,KAA7C,CAAb;AACA,qBAAO,MAAI,CAAC,UAAL,CAAgB,IAAhB,EAAsB,MAAM,CAAC,KAA7B,EAAoC,MAAM,CAAC,KAA3C,CAAP;AACD;;AACD,mBAAO,EAAP;AACD,WAPK,CAAN,CAHoC,CAWpC;AACA;;AACA,iBAAO,aAAa,CAAC,GAAG,CAAC,MAAJ,GAAa,CAAb,GAAiB,GAAjB,GAAuB,GAAG,CAAC,CAAD,CAA3B,EAAgC,KAAhC,EAAuC,KAAvC,CAApB;AACD,SAdD;AAeD;;AACD,WAAK,KAAL,CAAW,UAAX,CAAsB,IAAtB,CAA2B,QAA3B;AACD;AAnyBH;AAAA;AAAA,yBAqyByB,MAryBzB,EAqyBkC;AAC9B,MAAA,MAAM,CAAC,IAAP,GAAc,IAAd;AACA,aAAO,MAAP;AACD;AAxyBH;AAAA;AAAA,gCA0yBmB;AACf,UAAI,KAAK,KAAL,CAAW,aAAX,KAA6B,CAAjC,EAAoC;AAClC,aAAK,KAAL,CAAW,UAAX,GAAwB,EAAxB;AACD;;AACD,WAAK,KAAL,CAAW,aAAX;AACD;AA/yBH;AAAA;AAAA,8BAizBiB;AACb,WAAK,KAAL,CAAW,aAAX;AACD;AAED;;;AAGG;;AAxzBL;AAAA;AAAA,+BAyzBa,IAzzBb,EAyzB0B;AACtB,UAAM,SAAS,GAAe;AAC5B,QAAA,KAAK,EAAE,EADqB;AAE5B,QAAA,IAAI,EAAE,eAFsB;AAG5B,QAAA,EAAE,EAAE,KAAK,KAAL,CAAW,WAAX;AAHwB,OAA9B;;AAKA,UAAI,IAAJ,EAAU;AACR,QAAA,SAAS,CAAC,IAAV,GAAiB,IAAjB;AACD;;AACD,WAAK,KAAL,CAAW,UAAX,CAAsB,IAAtB,CAA2B,SAA3B;AACA,WAAK,KAAL,CAAW,WAAX,GAAyB,SAAzB;AACD;AAED;;;AAGG;;AAz0BL;AAAA;AAAA,6BA00BW,MA10BX,EA00BmC;AAAA;;AAC/B,UAAM,sBAAsB,GAAG,qBAAqB,CAAC,MAAD,CAApD;AACA,UAAM,yBAAyB,GAC3B,IAAI,GAAJ,CAAQ,sBAAsB,CAAC,GAAvB,CAA2B,UAAA,CAAC;AAAA,eAAI,CAAC,CAAC,EAAN;AAAA,OAA5B,CAAR,CADJ,CAF+B,CAK/B;;AACA,WAAK,IAAI,CAAC,GAAG,CAAb,EAAgB,CAAC,GAAG,KAAK,KAAL,CAAW,WAAX,CAAuB,KAAvB,CAA6B,MAAjD,EAAyD,CAAC,EAA1D,EAA8D;AAC5D,YAAM,MAAM,GAAG,KAAK,KAAL,CAAW,WAAX,CAAuB,KAAvB,CAA6B,CAA7B,CAAf;;AACA,YAAI,CAAC,MAAM,CAAC,IAAR,IAAgB,CAAC,yBAAyB,CAAC,GAA1B,CAA8B,MAAM,CAAC,EAArC,CAArB,EAA+D;AAC7D,UAAA,MAAM,CAAC,OAAP;AACD;AACF;;AAED,UAAM,QAAQ,GAAG,KAAK,KAAL,CAAW,UAAX,CAAsB,GAAtB,EAAjB;AACA,WAAK,KAAL,CAAW,WAAX,GAAyB,KAAK,KAAL,CAAW,UAAX,CAAsB,MAAtB,KAAiC,CAAjC,GACrB,IADqB,GAErB,KAAK,KAAL,CAAW,UAAX,CAAsB,KAAK,KAAL,CAAW,UAAX,CAAsB,MAAtB,GAA+B,CAArD,CAFJ,CAd+B,CAkB/B;;AACA,MAAA,sBAAsB,CAAC,OAAvB,CAA+B,UAAA,MAAM,EAAG;AACtC;AACA;AACA,YAAI,CAAC,MAAM,CAAC,IAAR,IAAgB,MAAM,CAAC,OAAP,KAAmB,QAAQ,CAAC,EAAhD,EAAoD;AAClD,UAAA,MAAI,CAAC,KAAL,CAAW,MAAX;AACD;AACF,OAND;AAOD;AAED;;;;;AAKG;;AA32BL;AAAA;AAAA,8BA62BM,CA72BN,EA62BkB,EA72BlB,EA62BgC,EA72BhC,EA82B8B;AAAA;;AAAA,UAAxB,gBAAwB,uEAAL,KAAK;AAC1B,MAAA,IAAI,CAAC,MAAL,CACI,EAAE,CAAC,MAAH,GAAY,CADhB,EACmB;AAAA,eAAM,2CAAN;AAAA,OADnB;;AAEA,UAAI,EAAE,IAAI,IAAN,IAAc,EAAE,CAAC,KAAH,KAAa,SAA/B,EAA0C;AACxC,cAAM,IAAI,KAAJ,kDAAoD,EAAE,CAAC,KAAvD,OAAN;AACD;;AAED,UAAM,CAAC,GAAG,KAAK,SAAL,CACN;AAAA,eAAM,OAAI,CAAC,SAAL,EAAN;AAAA,OADM,EACkB;AAAA,eAAM,OAAI,CAAC,OAAL,EAAN;AAAA,OADlB,EAEN;AAAA,eAAM,OAAI,CAAC,IAAL,CAAU,SAAV,EAAqB,CAArB,CAAN;AAAA,OAFM,CAAV;AAIA,MAAA,IAAI,CAAC,MAAL,CACI,CAAC,YAAY,MADjB,EAEI;AAAA,eAAM,gDAAN;AAAA,OAFJ,EAX0B,CAc1B;;AACA,UAAM,YAAY,GAAG,oBAAoB,CAAC,KAAK,KAAL,CAAW,UAAZ,EAAwB,EAAxB,EAA4B,CAA5B,CAAzC;;AACA,UAAI,CAAC,gBAAD,IAAqB,YAAY,CAAC,MAAb,KAAwB,CAA7C,IAAkD,EAAE,CAAC,MAAH,GAAY,CAAlE,EAAqE;AACnE,cAAM,IAAI,KAAJ,CACF,oEACA,iEADA,GAEA,OAHE,CAAN;AAID;;AAED,aAAO,KAAK,IAAL,CAAU,UAAV,EAAsB,YAAK;AAChC,YAAM,sBAAsB,GAAiC,EAA7D;AACA,QAAA,sBAAsB,CAAC,CAAC,CAAC,EAAH,CAAtB,GAAgC,EAAE,IAAI,IAAP,GAAe,IAAI,CAAC,CAAC,CAAC,KAAH,CAAnB,GAA+B,EAA9D,CAFgC,CAIhC;;AACA,QAAA,sBAAsB,CAClB,sBADkB,EACM,YADN,EAElB;AACA,kBAAA,CAAC;AAAA,iBAAI,OAAI,CAAC,IAAL,CAAU,CAAV,CAAJ;AAAA,SAHiB,EAIlB;AACA,QAAA,GALkB,CAAtB;AAMA,YAAM,KAAK,GAAG,EAAE,CAAC,GAAH,CAAO,UAAA,CAAC;AAAA,iBAAI,sBAAsB,CAAC,CAAC,CAAC,EAAH,CAA1B;AAAA,SAAR,CAAd;;AAEA,YAAI,OAAI,CAAC,KAAL,CAAW,aAAX,KAA6B,CAAjC,EAAoC;AAClC;AACA;AACA,UAAA,OAAI,CAAC,KAAL,CAAW,UAAX,CAAsB,OAAtB,CAA8B,UAAA,IAAI,EAAG;AAAA,wDACd,IAAI,CAAC,KADS;AAAA;;AAAA;AACnC,qEAAiC;AAAA,oBAAtB,MAAsB;AAC/B,gBAAA,MAAM,CAAC,OAAP;AACD;AAHkC;AAAA;AAAA;AAAA;AAAA;AAIpC,WAJD;;AAKA,UAAA,OAAI,CAAC,KAAL,CAAW,UAAX,GAAwB,IAAxB;AACD;;AACD,eAAO;AAAC,UAAA,KAAK,EAAE,CAAR;AAAW,UAAA,KAAK,EAAL;AAAX,SAAP;AACD,OAxBM,CAAP;AAyBD;AA95BH;AAAA;AAAA,+BAg6B+B,CAh6B/B,EAg6BuD;AAAA;;AAEnD,MAAA,IAAI,CAAC,MAAL,CACI,IAAI,CAAC,UAAL,CAAgB,CAAhB,CADJ,EAEI;AAAA,eAAM,mDAAN;AAAA,OAFJ;AAGA,aAAO,YAA2B;AAAA,0CAAvB,MAAuB;AAAvB,UAAA,MAAuB;AAAA;;AAChC,QAAA,IAAI,CAAC,MAAL,CACI,MAAM,CAAC,KAAP,CAAa,UAAA,CAAC;AAAA,iBAAI,CAAC,YAAY,MAAjB;AAAA,SAAd,CADJ,EAEI;AAAA,iBAAM,8DACF,SADJ;AAAA,SAFJ;AAKA,YAAI,GAAJ;AAIA,YAAM,QAAQ,GAAmB,EAAjC;AACA,QAAA,MAAM,CAAC,OAAP,CAAe,UAAC,KAAD,EAAQ,CAAR,EAAa;AAC1B,UAAA,QAAQ,CAAC,CAAD,CAAR,GAAc,KAAd;AACD,SAFD;AAGA,eAAO,OAAI,CAAC,aAAL,CACH,UAAC,CAAD,EAAI,IAAJ,EAAY;AACV,UAAA,GAAG,GAAG,CAAC,MAAD,mBAAS,MAAT,GAAiB,IAAjB,GAAN;AACA,UAAA,IAAI,CAAC,MAAL,CACI,GAAG,CAAC,KAAJ,YAAqB,MADzB,EAEI;AAAA,mBAAM,2DACF,sCADJ;AAAA,WAFJ;AAIA,UAAA,IAAI,CAAC,MAAL,CACI,IAAI,CAAC,UAAL,CAAgB,GAAG,CAAC,QAApB,CADJ,EAEI;AAAA,mBAAM,2DACF,4CADJ;AAAA,WAFJ;AAIA,iBAAO,GAAG,CAAC,KAAX;AACD,SAZE,EAaH,QAbG,EAcH,UAAC,EAAD,EAAQ,KAAR,EAA2B;AACzB,cAAM,OAAO,GAAG,GAAG,CAAC,QAAJ,CAAa,EAAb,EAAiB,KAAjB,CAAhB;AACA,cAAM,KAAK,GACP,KAAK,CAAC,OAAN,CAAc,OAAd,IAAyB,OAAzB,GAAmC,CAAC,OAAD,CADvC;AAEA,UAAA,IAAI,CAAC,MAAL,CACI,KAAK,CAAC,MAAN,KAAiB,MAAM,CAAC,MAD5B,EAEI;AAAA,mBAAM,2DACF,yDADE,GAEF,wDAFJ;AAAA,WAFJ;AAKA,UAAA,IAAI,CAAC,MAAL,CACI,KAAK,CAAC,KAAN,CAAY,UAAA,CAAC;AAAA,mBAAI,CAAC,YAAY,MAAjB;AAAA,WAAb,CADJ,EAEI;AAAA,mBAAM,2DACF,yDADE,GAEF,yBAFJ;AAAA,WAFJ;AAKA,cAAM,OAAO,GAAkC,EAA/C;AACA,UAAA,KAAK,CAAC,OAAN,CAAc,UAAC,IAAD,EAAO,CAAP,EAAY;AACxB,YAAA,OAAO,CAAC,CAAD,CAAP,GAAa;AAAA,qBAAM,IAAN;AAAA,aAAb;AACD,WAFD;AAGA,iBAAO,OAAP;AACD,SAjCE,CAAP;AAkCD,OAhDD;AAiDD;AAt9BH;AAAA;AAAA,6BAw9BW,MAx9BX,EAw9ByB;AACrB;AACA,UAAM,IAAI,GAAG,KAAK,KAAL,CAAW,UAAX,CAAsB,GAAtB,CAA0B,MAA1B,CAAb;AACA,aAAO,IAAI,CAAC,OAAL,CAAa,QAAb,CAAsB,MAAtB,CAAP;AACD;AA59BH;AAAA;AAAA,yBA69BO,MA79BP,EA69BqB;AACjB;AACA,UAAM,IAAI,GAAG,KAAK,KAAL,CAAW,UAAX,CAAsB,GAAtB,CAA0B,MAA1B,CAAb;AACA,aAAO,IAAI,CAAC,OAAL,CAAa,IAAb,CAAkB,MAAlB,CAAP;AACD;AAj+BH;AAAA;AAAA;AAAA,6FAm+Ba,KAn+Bb;AAAA;AAAA;AAAA;AAAA;AAAA;AAo+BU,gBAAA,KAp+BV,GAo+BkB,GAAG,EAp+BrB;AAAA;AAAA,uBAq+B6B,KAAK,OAAL,CAAa,IAAb,CAAkB,KAAlB,CAr+B7B;;AAAA;AAq+BU,gBAAA,UAr+BV;AAs+BI,gBAAA,UAAU,CAAC,MAAX,GAAoB,GAAG,KAAK,KAA5B;AAt+BJ,kDAu+BW,UAv+BX;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;AAAA;AAAA;AAAA;;AAAA;AAAA;AA0+BE;;;;;AAKG;;AA/+BL;AAAA;AAAA,0BAg/BkC,MAh/BlC,EAg/B2C;AACvC,UAAI,KAAK,KAAL,CAAW,WAAX,IAA0B,IAA9B,EAAoC;AAClC,QAAA,MAAM,CAAC,OAAP,GAAiB,KAAK,KAAL,CAAW,WAAX,CAAuB,EAAxC;AACA,aAAK,KAAL,CAAW,WAAX,CAAuB,KAAvB,CAA6B,IAA7B,CAAkC,MAAlC;AACD;;AAED,aAAO,MAAP;AACD;AAv/BH;AAAA;;AA6/BE;;;AAGG;AAhgCL,4BAigCO;AACH;AACA,WAAK,oBAAL;AAEA,WAAK,KAAL,CAAW,OAAX;AACA,WAAK,GAAL,CAAS,KAAT;AACA,WAAK,KAAL,GAAa,IAAI,WAAJ,EAAb;;AAEA,WAAK,IAAM,WAAX,IAA0B,KAAK,QAA/B,EAAyC;AACvC,aAAK,wBAAL,CAA8B,WAA9B;AACA,aAAK,QAAL,CAAc,WAAd,EAA2B,OAA3B;AACA,eAAO,KAAK,QAAL,CAAc,WAAd,CAAP;AACD;;AACD,WAAK,WAAL,GAAmB,IAAnB;AACA,WAAK,eAAL,GAAuB,IAAvB;AACA,WAAK,kBAAL,GAA0B,IAA1B;AACD;AAjhCH;AAAA;AAAA,wBA2Ca;AACT,UAAI,KAAK,kBAAL,IAA2B,IAA/B,EAAqC;AACnC,cAAM,IAAI,KAAJ,CACF,mBAAY,KAAK,WAAjB,kIADE,CAAN;AAID;;AACD,UAAI,KAAK,eAAL,IAAwB,IAA5B,EAAkC;AAAA,qCACN,KAAK,+BAAL,EADM;AAAA,YACzB,IADyB,0BACzB,IADyB;AAAA,YACnB,SADmB,0BACnB,SADmB;;AAEhC,YAAI,SAAJ,EAAe;AACb,gBAAM,IAAI,KAAJ,CACF,wCAAiC,IAAjC,kIADE,CAAN;AAID;;AACD,aAAK,UAAL,CAAgB,IAAhB;AACD;;AACD,aAAO,KAAK,eAAZ;AACD;AA7DH;AAAA;AAAA,wBAy/ByB;AACrB,aAAO,KAAK,KAAL,CAAW,mBAAlB;AACD;AA3/BH;;AAAA;AAAA;AAiUiB,MAAA,CAAA,YAAA,GAAe,CAAf;AAKA,MAAA,CAAA,cAAA,GAAiB,CAAjB;;AA8sBjB,SAAS,IAAT,CAAc,KAAd,EAA6B;AAC3B,MAAM,MAAM,GAAG,kBAAkB,CAAC,aAAa,CAAC,KAAD,CAAd,EAAuB,SAAvB,CAAjC;AACA,SAAO,MAAM,CAAC,UAAP,CAAkB,MAAlB,EAA0B,KAA1B,EAAiC,SAAjC,CAAP;AACD;;AAED,OAAM,SAAU,eAAV,GAAyB;AAC7B,MAAM,EAAE,GAAG,kBAAkB,EAA7B;;AACA,MAAI,EAAE,CAAC,SAAH,IAAgB,IAApB,EAA0B;AACxB,QAAM,WAAW,GAAG,IAAI,WAAJ,CAAgB,EAAhB,CAApB;AACA,IAAA,EAAE,CAAC,SAAH,GAAe,IAAI,MAAJ,CAAW,WAAX,CAAf;AACD;;AACD,EAAA,oBAAoB,CAAC,EAAE,CAAC,SAAH,CAAa,GAAd,CAApB,CAN6B,CAQ7B;AACA;;AACA,EAAA,gBAAgB,CAAC;AAAA,WAAM,EAAE,CAAC,SAAT;AAAA,GAAD,CAAhB;AACA,SAAO,EAAE,CAAC,SAAV;AACD;AAED,OAAO,IAAM,MAAM,GAAG,eAAe,EAA9B;AAEP;;;;;AAKG;;AACH,OAAM,SAAU,GAAV,CAAc,CAAd,EAAyB,CAAzB,EAAkC;AACtC;AACA,MAAM,MAAM,GAAG;AAAC,IAAA,CAAC,EAAD,CAAD;AAAI,IAAA,CAAC,EAAD;AAAJ,GAAf;AACA,SAAO,MAAM,CAAC,aAAP,CAAqB,UAAC,OAAD,EAAU,IAAV,EAAkB;AAC5C,QAAM,GAAG,GAAG,OAAO,CAAC,GAAR,CAAY,CAAZ,EAAe,CAAf,CAAZ;AACA,IAAA,IAAI,CAAC,CAAC,CAAD,EAAI,CAAJ,CAAD,CAAJ;AACA,WAAO,GAAP;AACD,GAJM,EAIJ,MAJI,EAI4B;AAAK;AAJjC,IAIiD,GAJjD,CAAP;AAKD","sourceRoot":"","sourcesContent":["/**\r\n * @license\r\n * Copyright 2018 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\r\nimport { KernelBackend } from './backends/backend';\r\nimport { Environment, setEnvironmentGlobal } from './environment';\r\nimport { getGlobalNamespace } from './global_util';\r\nimport { Add, Cast } from './kernel_names';\r\nimport { getGradient, getKernel, getKernelsForBackend } from './kernel_registry';\r\nimport { Profiler } from './profiler';\r\nimport { backpropagateGradients, getFilteredNodesXToY } from './tape';\r\nimport { setTensorTracker, Tensor, Variable } from './tensor';\r\nimport { getTensorsInContainer } from './tensor_util';\r\nimport * as util from './util';\r\nimport { bytesFromStringArray, makeOnesTypedArray, now, sizeFromShape } from './util';\r\nclass EngineState {\r\n    constructor() {\r\n        // Public since optimizers will use it.\r\n        this.registeredVariables = {};\r\n        this.nextTapeNodeId = 0;\r\n        this.numBytes = 0;\r\n        this.numTensors = 0;\r\n        this.numStringTensors = 0;\r\n        this.numDataBuffers = 0;\r\n        // Number of nested tf.grad() statements when computing higher-order\r\n        // gradients. E.g. `1` for first-order gradients and `2` for second-order\r\n        // gradients. Used to track if the tape should be removed after a backprop.\r\n        this.gradientDepth = 0;\r\n        // Number of nested kernel calls. When kernel depth is greater than 1, we turn\r\n        // off the tape.\r\n        this.kernelDepth = 0;\r\n        this.scopeStack = [];\r\n        /**\r\n         * Keeps track of the number of data moves during a kernel execution. We\r\n         * maintain a stack since kernels can call other kernels, recursively.\r\n         */\r\n        this.numDataMovesStack = [];\r\n        this.nextScopeId = 0;\r\n        this.tensorInfo = new WeakMap();\r\n        this.profiling = false;\r\n        this.activeProfile = { newBytes: 0, newTensors: 0, peakBytes: 0, kernels: [], result: null };\r\n    }\r\n    dispose() {\r\n        for (const variableName in this.registeredVariables) {\r\n            this.registeredVariables[variableName].dispose();\r\n        }\r\n    }\r\n}\r\nexport class Engine {\r\n    constructor(ENV) {\r\n        this.ENV = ENV;\r\n        this.registry = {};\r\n        this.registryFactory = {};\r\n        this.pendingBackendInitId = 0;\r\n        this.state = new EngineState();\r\n    }\r\n    async ready() {\r\n        if (this.pendingBackendInit != null) {\r\n            return this.pendingBackendInit.then(() => { });\r\n        }\r\n        if (this.backendInstance != null) {\r\n            return;\r\n        }\r\n        const sortedBackends = this.getSortedBackends();\r\n        for (let i = 0; i < sortedBackends.length; i++) {\r\n            const backendName = sortedBackends[i];\r\n            const success = await this.initializeBackend(backendName).success;\r\n            if (success) {\r\n                await this.setBackend(backendName);\r\n                return;\r\n            }\r\n        }\r\n        throw new Error(`Could not initialize any backends, all backend initializations ` +\r\n            `failed.`);\r\n    }\r\n    get backend() {\r\n        if (this.pendingBackendInit != null) {\r\n            throw new Error(`Backend '${this.backendName}' has not yet been initialized. Make ` +\r\n                `sure to await tf.ready() or await tf.setBackend() before calling ` +\r\n                `other methods`);\r\n        }\r\n        if (this.backendInstance == null) {\r\n            const { name, asyncInit } = this.initializeBackendsAndReturnBest();\r\n            if (asyncInit) {\r\n                throw new Error(`The highest priority backend '${name}' has not yet been ` +\r\n                    `initialized. Make sure to await tf.ready() or ` +\r\n                    `await tf.setBackend() before calling other methods`);\r\n            }\r\n            this.setBackend(name);\r\n        }\r\n        return this.backendInstance;\r\n    }\r\n    backendNames() {\r\n        return Object.keys(this.registryFactory);\r\n    }\r\n    findBackend(backendName) {\r\n        if (!(backendName in this.registry)) {\r\n            // If the backend hasn't been initialized but we have a registry entry for\r\n            // it, initialize it and return it.\r\n            if (backendName in this.registryFactory) {\r\n                const { asyncInit } = this.initializeBackend(backendName);\r\n                if (asyncInit) {\r\n                    // Backend is not ready yet.\r\n                    return null;\r\n                }\r\n            }\r\n            else {\r\n                return null;\r\n            }\r\n        }\r\n        return this.registry[backendName];\r\n    }\r\n    findBackendFactory(backendName) {\r\n        if (!(backendName in this.registryFactory)) {\r\n            return null;\r\n        }\r\n        return this.registryFactory[backendName].factory;\r\n    }\r\n    registerBackend(backendName, factory, priority = 1) {\r\n        if (backendName in this.registryFactory) {\r\n            console.warn(`${backendName} backend was already registered. ` +\r\n                `Reusing existing backend factory.`);\r\n            return false;\r\n        }\r\n        this.registryFactory[backendName] = { factory, priority };\r\n        return true;\r\n    }\r\n    async setBackend(backendName) {\r\n        if (this.registryFactory[backendName] == null) {\r\n            throw new Error(`Backend name '${backendName}' not found in registry`);\r\n        }\r\n        this.backendName = backendName;\r\n        if (this.registry[backendName] == null) {\r\n            this.backendInstance = null;\r\n            const { success, asyncInit } = this.initializeBackend(backendName);\r\n            const result = asyncInit ? await success : success;\r\n            if (!result) {\r\n                return false;\r\n            }\r\n        }\r\n        this.backendInstance = this.registry[backendName];\r\n        this.setupRegisteredKernels();\r\n        // Reset the profiler.\r\n        this.profiler = new Profiler(this.backendInstance);\r\n        return true;\r\n    }\r\n    setupRegisteredKernels() {\r\n        const kernels = getKernelsForBackend(this.backendName);\r\n        kernels.forEach(kernel => {\r\n            if (kernel.setupFunc != null) {\r\n                kernel.setupFunc(this.backendInstance);\r\n            }\r\n        });\r\n    }\r\n    disposeRegisteredKernels(backendName) {\r\n        const kernels = getKernelsForBackend(backendName);\r\n        kernels.forEach(kernel => {\r\n            if (kernel.disposeFunc != null) {\r\n                kernel.disposeFunc(this.registry[backendName]);\r\n            }\r\n        });\r\n    }\r\n    /**\r\n     * Initializes a backend by looking up the backend name in the factory\r\n     * registry and calling the factory method. Returns a boolean representing\r\n     * whether the initialization of the backend suceeded. Throws an error if\r\n     * there is no backend in the factory registry.\r\n     */\r\n    initializeBackend(backendName) {\r\n        const registryFactoryEntry = this.registryFactory[backendName];\r\n        if (registryFactoryEntry == null) {\r\n            throw new Error(`Cannot initialize backend ${backendName}, no registration found.`);\r\n        }\r\n        try {\r\n            const backend = registryFactoryEntry.factory();\r\n            /* Test if the factory returns a promise.\r\n            Done in a more liberal way than\r\n            previous 'Promise.resolve(backend)===backend'\r\n            as we needed to account for custom Promise\r\n            implementations (e.g. Angular) */\r\n            if (backend && !(backend instanceof KernelBackend)\r\n                && typeof backend.then === 'function') {\r\n                const promiseId = ++this.pendingBackendInitId;\r\n                const success = backend\r\n                    .then(backendInstance => {\r\n                    // Outdated promise. Another backend was set in the meantime.\r\n                    if (promiseId < this.pendingBackendInitId) {\r\n                        return false;\r\n                    }\r\n                    this.registry[backendName] = backendInstance;\r\n                    this.pendingBackendInit = null;\r\n                    return true;\r\n                })\r\n                    .catch(err => {\r\n                    // Outdated promise. Another backend was set in the meantime.\r\n                    if (promiseId < this.pendingBackendInitId) {\r\n                        return false;\r\n                    }\r\n                    this.pendingBackendInit = null;\r\n                    console.warn(`Initialization of backend ${backendName} failed`);\r\n                    console.warn(err.stack || err.message);\r\n                    return false;\r\n                });\r\n                this.pendingBackendInit = success;\r\n                return { success, asyncInit: true };\r\n            }\r\n            else {\r\n                this.registry[backendName] = backend;\r\n                return { success: true, asyncInit: false };\r\n            }\r\n        }\r\n        catch (err) {\r\n            console.warn(`Initialization of backend ${backendName} failed`);\r\n            console.warn(err.stack || err.message);\r\n            return { success: false, asyncInit: false };\r\n        }\r\n    }\r\n    removeBackend(backendName) {\r\n        if (!(backendName in this.registryFactory)) {\r\n            throw new Error(`${backendName} backend not found in registry`);\r\n        }\r\n        if (this.backendName === backendName && this.pendingBackendInit != null) {\r\n            // There is a pending promise of the backend we want to remove. Make it\r\n            // obsolete.\r\n            this.pendingBackendInitId++;\r\n        }\r\n        if (backendName in this.registry) {\r\n            this.disposeRegisteredKernels(backendName);\r\n            this.registry[backendName].dispose();\r\n            delete this.registry[backendName];\r\n        }\r\n        delete this.registryFactory[backendName];\r\n        // Unset the backend if it is active.\r\n        if (this.backendName === backendName) {\r\n            this.pendingBackendInit = null;\r\n            this.backendName = null;\r\n            this.backendInstance = null;\r\n        }\r\n    }\r\n    getSortedBackends() {\r\n        if (Object.keys(this.registryFactory).length === 0) {\r\n            throw new Error('No backend found in registry.');\r\n        }\r\n        return Object.keys(this.registryFactory).sort((a, b) => {\r\n            // Highest priority comes first.\r\n            return this.registryFactory[b].priority -\r\n                this.registryFactory[a].priority;\r\n        });\r\n    }\r\n    initializeBackendsAndReturnBest() {\r\n        const sortedBackends = this.getSortedBackends();\r\n        for (let i = 0; i < sortedBackends.length; i++) {\r\n            const backendName = sortedBackends[i];\r\n            const { success, asyncInit } = this.initializeBackend(backendName);\r\n            if (asyncInit || success) {\r\n                return { name: backendName, asyncInit };\r\n            }\r\n        }\r\n        throw new Error(`Could not initialize any backends, all backend initializations ` +\r\n            `failed.`);\r\n    }\r\n    moveData(backend, dataId) {\r\n        const info = this.state.tensorInfo.get(dataId);\r\n        const srcBackend = info.backend;\r\n        const values = this.readSync(dataId);\r\n        // Delete the tensor from the old backend and move it to the new\r\n        // backend.\r\n        srcBackend.disposeData(dataId);\r\n        info.backend = backend;\r\n        backend.move(dataId, values, info.shape, info.dtype);\r\n        if (this.shouldCheckForMemLeaks()) {\r\n            // Track the number of moves during a kernel execution to correctly\r\n            // detect memory leaks.\r\n            this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1]++;\r\n        }\r\n    }\r\n    tidy(nameOrFn, fn) {\r\n        let name = null;\r\n        if (fn == null) {\r\n            // Called with only 1 argument.\r\n            if (typeof nameOrFn !== 'function') {\r\n                throw new Error('Please provide a function to tidy()');\r\n            }\r\n            fn = nameOrFn;\r\n        }\r\n        else {\r\n            // Called with 2 arguments.\r\n            if (typeof nameOrFn !== 'string' && !(nameOrFn instanceof String)) {\r\n                throw new Error('When calling with two arguments, the first argument ' +\r\n                    'to tidy() must be a string');\r\n            }\r\n            if (typeof fn !== 'function') {\r\n                throw new Error('When calling with two arguments, the 2nd argument ' +\r\n                    'to tidy() must be a function');\r\n            }\r\n            name = nameOrFn;\r\n            // TODO(nsthorat,smilkov): Do operation logging and performance\r\n            // profiling.\r\n        }\r\n        let result;\r\n        return this.scopedRun(() => this.startScope(name), () => this.endScope(result), () => {\r\n            result = fn();\r\n            if (result instanceof Promise) {\r\n                console.error('Cannot return a Promise inside of tidy.');\r\n            }\r\n            return result;\r\n        });\r\n    }\r\n    scopedRun(start, end, f) {\r\n        start();\r\n        try {\r\n            const res = f();\r\n            end();\r\n            return res;\r\n        }\r\n        catch (ex) {\r\n            end();\r\n            throw ex;\r\n        }\r\n    }\r\n    nextTensorId() {\r\n        return Engine.nextTensorId++;\r\n    }\r\n    nextVariableId() {\r\n        return Engine.nextVariableId++;\r\n    }\r\n    /**\r\n     * This method is called instead of the public-facing tensor.clone() when\r\n     * saving a tensor for backwards pass. It makes sure to add the clone\r\n     * operation to the tape regardless of being called inside a kernel\r\n     * execution.\r\n     *\r\n     * This method will go away once all kernels are modularized since we won't\r\n     * need to turn off the tape inside runKernel().\r\n     */\r\n    clone(x) {\r\n        const y = this.makeTensorFromDataId(x.dataId, x.shape, x.dtype);\r\n        const inputs = { x };\r\n        const grad = (dy) => ({\r\n            x: () => {\r\n                const dtype = 'float32';\r\n                const gradInputs = { x: dy };\r\n                const attrs = { dtype };\r\n                return ENGINE.runKernelFunc(backend => backend.cast(dy, dtype), gradInputs, null /* grad */, Cast, attrs);\r\n            }\r\n        });\r\n        const saved = [];\r\n        this.addTapeNode(this.state.activeScope.name, inputs, [y], grad, saved, {});\r\n        return y;\r\n    }\r\n    /**\r\n     * Execute a kernel with the given name and return the output tensor.\r\n     *\r\n     * @param kernelName The name of the kernel to execute.\r\n     * @param inputs A map of input names to tensors.\r\n     * @param attrs A map of attribute names to their values. An attribute is a\r\n     *     primitive (non-tensor) input to the kernel.\r\n     * @param inputsToSave A list of tensors, inputs to save for the backprop\r\n     *     computation.\r\n     * @param outputsToSave A list of booleans, specifying which output to save\r\n     *     for the backprop computation. These are booleans since the output\r\n     * tensors are not visible to the user.\r\n     */\r\n    runKernel(kernelName, inputs, attrs, inputsToSave, outputsToSave) {\r\n        const forwardFunc = null;\r\n        const backwardsFunc = null;\r\n        // Call runKernel as a stop-gap until we modularize all kernels.\r\n        // Once we modularize all kernels, we will remove the existing\r\n        // `runKernelFunc`.\r\n        return this.runKernelFunc(forwardFunc, inputs, backwardsFunc, kernelName, attrs, inputsToSave, outputsToSave);\r\n    }\r\n    shouldCheckForMemLeaks() {\r\n        return this.ENV.getBool('IS_TEST');\r\n    }\r\n    checkKernelForMemLeak(kernelName, numDataIdsBefore, outInfos) {\r\n        const numDataIdsAfter = this.backend.numDataIds();\r\n        // Count the number of data ids associated with the result of the kernel.\r\n        let numOutputDataIds = 0;\r\n        outInfos.forEach(info => {\r\n            // Complex numbers allocate 3 data ids, one for 'real', one for\r\n            // 'imaginary', and one for the container that holds the former two.\r\n            numOutputDataIds += (info.dtype === 'complex64' ? 3 : 1);\r\n        });\r\n        // Account for the number of moves during kernel execution. A \"data move\"\r\n        // can happen in the middle of a kernel execution, placing a new (key,value)\r\n        // pair in the data storage. Since data moves have net zero effect (we\r\n        // always remove the data from the old backend), we have to cancel them out\r\n        // when detecting memory leaks.\r\n        const numMoves = this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1];\r\n        const dataIdsLeaked = numDataIdsAfter - numDataIdsBefore - numOutputDataIds - numMoves;\r\n        if (dataIdsLeaked > 0) {\r\n            throw new Error(`Backend '${this.backendName}' has an internal memory leak ` +\r\n                `(${dataIdsLeaked} data ids) after running '${kernelName}'`);\r\n        }\r\n    }\r\n    /**\r\n     * @deprecated Use `runKernel` for newly added kernels. Keep using this method\r\n     *     only for kernels that are not yet fully modularized.\r\n     */\r\n    runKernelFunc(forwardFunc, inputs, backwardsFunc, kernelName, attrs, inputsToSave, outputsToSave) {\r\n        let outputs;\r\n        let saved = [];\r\n        const isTapeOn = this.isTapeOn();\r\n        if (kernelName == null) {\r\n            kernelName =\r\n                this.state.activeScope != null ? this.state.activeScope.name : '';\r\n        }\r\n        const startingBytecount = this.state.numBytes;\r\n        const startingNumTensors = this.state.numTensors;\r\n        if (this.shouldCheckForMemLeaks()) {\r\n            this.state.numDataMovesStack.push(0);\r\n        }\r\n        let kernelFunc;\r\n        const kernel = getKernel(kernelName, this.backendName);\r\n        let out;\r\n        if (kernel != null) {\r\n            kernelFunc = () => {\r\n                const numDataIdsBefore = this.backend.numDataIds();\r\n                out = kernel.kernelFunc({ inputs, attrs, backend: this.backend });\r\n                const outInfos = Array.isArray(out) ? out : [out];\r\n                if (this.shouldCheckForMemLeaks()) {\r\n                    this.checkKernelForMemLeak(kernelName, numDataIdsBefore, outInfos);\r\n                }\r\n                const outTensors = outInfos.map(({ dataId, shape, dtype }) => this.makeTensorFromDataId(dataId, shape, dtype));\r\n                // Save the inputs and outputs.\r\n                // Do not save unless we are recording to the tape. Otherwise it would\r\n                // cause a mem leak since we would never run backprop, which disposes\r\n                // the kept tensors.\r\n                if (isTapeOn) {\r\n                    let tensorsToSave = this.getTensorsForGradient(kernelName, inputs, outTensors);\r\n                    if (tensorsToSave == null) {\r\n                        // Fallback for ops that call runKernelFunc and pass in\r\n                        // inputsToSave and outputsToSave. Currently this is the set of ops\r\n                        // with kernel support in the WASM backend. Once those ops and\r\n                        // respective gradients are modularised we can remove this path.\r\n                        if (outputsToSave == null) {\r\n                            outputsToSave = [];\r\n                        }\r\n                        const outsToSave = outTensors.filter((_, i) => outputsToSave[i]);\r\n                        tensorsToSave = (inputsToSave || []).slice().concat(outsToSave);\r\n                    }\r\n                    saved = this.saveTensorsForBackwardMode(tensorsToSave);\r\n                }\r\n                return outTensors;\r\n            };\r\n        }\r\n        else {\r\n            const saveFunc = (tensors) => {\r\n                // Do not save unless we are recording to the tape. Otherwise it would\r\n                // cause a mem leak since we would never run backprop, which disposes\r\n                // the kept tensors.\r\n                if (!isTapeOn) {\r\n                    return;\r\n                }\r\n                saved = tensors.map(tensor => this.keep(this.clone(tensor)));\r\n            };\r\n            kernelFunc = () => {\r\n                const numDataIdsBefore = this.backend.numDataIds();\r\n                out = this.tidy(() => forwardFunc(this.backend, saveFunc));\r\n                const outs = (Array.isArray(out) ? out : [out]);\r\n                if (this.shouldCheckForMemLeaks()) {\r\n                    this.checkKernelForMemLeak(kernelName, numDataIdsBefore, outs);\r\n                }\r\n                return outs;\r\n            };\r\n        }\r\n        // Stop recording to a tape when running a kernel.\r\n        let kernelProfile;\r\n        this.scopedRun(() => this.state.kernelDepth++, () => this.state.kernelDepth--, () => {\r\n            if (!this.ENV.getBool('DEBUG') && !this.state.profiling) {\r\n                outputs = kernelFunc();\r\n            }\r\n            else {\r\n                kernelProfile = this.profiler.profileKernel(kernelName, inputs, () => kernelFunc());\r\n                if (this.ENV.getBool('DEBUG')) {\r\n                    this.profiler.logKernelProfile(kernelProfile);\r\n                }\r\n                outputs = kernelProfile.outputs;\r\n            }\r\n        });\r\n        if (isTapeOn) {\r\n            this.addTapeNode(kernelName, inputs, outputs, backwardsFunc, saved, attrs);\r\n        }\r\n        if (this.state.profiling) {\r\n            this.state.activeProfile.kernels.push({\r\n                name: kernelName,\r\n                bytesAdded: this.state.numBytes - startingBytecount,\r\n                totalBytesSnapshot: this.state.numBytes,\r\n                tensorsAdded: this.state.numTensors - startingNumTensors,\r\n                totalTensorsSnapshot: this.state.numTensors,\r\n                inputShapes: Object.keys(inputs).map(key => inputs[key] != null ? inputs[key].shape : null),\r\n                outputShapes: outputs.map(item => item.shape),\r\n                kernelTimeMs: kernelProfile.timeMs,\r\n                extraInfo: kernelProfile.extraInfo\r\n            });\r\n        }\r\n        return (Array.isArray(out) ? outputs : outputs[0]);\r\n    }\r\n    /**\r\n     * Saves tensors used in forward mode for use in backward mode.\r\n     *\r\n     * @param tensors the list of tensors to save.\r\n     */\r\n    saveTensorsForBackwardMode(tensors) {\r\n        const saved = tensors.map(tensor => this.keep(this.clone(tensor)));\r\n        return saved;\r\n    }\r\n    /**\r\n     * Returns a list of tensors to save for a given gradient calculation.\r\n     *\r\n     * Returns undefined if their is no registered gradient for this kernel in the\r\n     * gradient registry.\r\n     *\r\n     * @param kernelName name of kernel to look up gradient for.\r\n     * @param inputs a map of input tensors.\r\n     * @param outputs an array of output tensors from forward mode of kernel.\r\n     */\r\n    getTensorsForGradient(kernelName, inputs, outputs) {\r\n        const gradConfig = getGradient(kernelName);\r\n        if (gradConfig != null) {\r\n            const inputsToSave = gradConfig.inputsToSave || [];\r\n            const outputsToSave = gradConfig.outputsToSave || [];\r\n            // If saveAllInputs is true, all inputs will be saved. Otherwise, inputs\r\n            // specified in inputsToSave will be saved.\r\n            let inputTensorsToSave;\r\n            if (gradConfig.saveAllInputs) {\r\n                util.assert(Array.isArray(inputs), () => 'saveAllInputs is true, expected inputs to be an array.');\r\n                inputTensorsToSave = Object.keys(inputs).map((key) => inputs[key]);\r\n            }\r\n            else {\r\n                inputTensorsToSave = inputsToSave.map((inputName) => inputs[inputName]);\r\n            }\r\n            const outputTensorsToSave = outputs.filter((_, i) => outputsToSave[i]);\r\n            return inputTensorsToSave.concat(outputTensorsToSave);\r\n        }\r\n        // TODO(yassogba) throw exception here once all runkernelFunc calls with\r\n        // inputsToSave/outputsToSave are removed\r\n        return null;\r\n    }\r\n    /**\r\n     * Internal method used by public APIs for tensor creation. Makes a new\r\n     * tensor with the provided shape, dtype and values. It always\r\n     * creates a new data id and writes the values to the underlying backend.\r\n     */\r\n    makeTensor(values, shape, dtype, backend) {\r\n        if (values == null) {\r\n            throw new Error('Values passed to engine.makeTensor() are null');\r\n        }\r\n        dtype = dtype || 'float32';\r\n        backend = backend || this.backend;\r\n        let backendVals = values;\r\n        if (dtype === 'string' && util.isString(values[0])) {\r\n            backendVals = values.map(d => util.encodeString(d));\r\n        }\r\n        const dataId = backend.write(backendVals, shape, dtype);\r\n        const t = new Tensor(shape, dtype, dataId, this.nextTensorId());\r\n        this.incRef(t, backend);\r\n        // Count bytes for string tensors.\r\n        if (dtype === 'string') {\r\n            const info = this.state.tensorInfo.get(dataId);\r\n            const newBytes = bytesFromStringArray(backendVals);\r\n            this.state.numBytes += newBytes - info.bytes;\r\n            info.bytes = newBytes;\r\n        }\r\n        return t;\r\n    }\r\n    /**\r\n     * Internal method used by backends. Makes a new tensor\r\n     * that is a wrapper around an existing data id. It doesn't create\r\n     * a new data id, only increments the ref count used in memory tracking.\r\n     */\r\n    makeTensorFromDataId(dataId, shape, dtype, backend) {\r\n        dtype = dtype || 'float32';\r\n        const t = new Tensor(shape, dtype, dataId, this.nextTensorId());\r\n        this.incRef(t, backend);\r\n        return t;\r\n    }\r\n    makeVariable(initialValue, trainable = true, name, dtype) {\r\n        name = name || this.nextVariableId().toString();\r\n        if (dtype != null && dtype !== initialValue.dtype) {\r\n            initialValue = initialValue.cast(dtype);\r\n        }\r\n        const v = new Variable(initialValue, trainable, name, this.nextTensorId());\r\n        if (this.state.registeredVariables[v.name] != null) {\r\n            throw new Error(`Variable with name ${v.name} was already registered`);\r\n        }\r\n        this.state.registeredVariables[v.name] = v;\r\n        this.incRef(v, this.backend);\r\n        return v;\r\n    }\r\n    incRef(a, backend) {\r\n        const refCount = this.state.tensorInfo.has(a.dataId) ?\r\n            this.state.tensorInfo.get(a.dataId).refCount :\r\n            0;\r\n        this.state.numTensors++;\r\n        if (a.dtype === 'string') {\r\n            this.state.numStringTensors++;\r\n        }\r\n        if (refCount === 0) {\r\n            this.state.numDataBuffers++;\r\n            // Bytes for complex numbers are counted by their components. Bytes for\r\n            // string tensors are counted when writing values.\r\n            let bytes = 0;\r\n            if (a.dtype !== 'complex64' && a.dtype !== 'string') {\r\n                bytes = a.size * util.bytesPerElement(a.dtype);\r\n            }\r\n            this.state.tensorInfo.set(a.dataId, {\r\n                backend: backend || this.backend,\r\n                dtype: a.dtype,\r\n                shape: a.shape,\r\n                bytes,\r\n                refCount: 0\r\n            });\r\n            this.state.numBytes += bytes;\r\n        }\r\n        this.state.tensorInfo.get(a.dataId).refCount++;\r\n        if (!(a instanceof Variable)) {\r\n            this.track(a);\r\n        }\r\n    }\r\n    disposeTensor(a) {\r\n        if (!this.state.tensorInfo.has(a.dataId)) {\r\n            return;\r\n        }\r\n        this.state.numTensors--;\r\n        if (a.dtype === 'string') {\r\n            this.state.numStringTensors--;\r\n        }\r\n        const info = this.state.tensorInfo.get(a.dataId);\r\n        const refCount = info.refCount;\r\n        if (refCount <= 1) {\r\n            // Don't count bytes for complex numbers as they are counted by their\r\n            // components.\r\n            if (a.dtype !== 'complex64') {\r\n                this.state.numBytes -= info.bytes;\r\n            }\r\n            this.state.numDataBuffers--;\r\n            info.backend.disposeData(a.dataId);\r\n            this.state.tensorInfo.delete(a.dataId);\r\n        }\r\n        else {\r\n            this.state.tensorInfo.get(a.dataId).refCount--;\r\n        }\r\n        // TODO(nsthorat): Construct an error and save the stack trace for\r\n        // debugging when in debug mode. Creating a stack trace is too expensive\r\n        // to do unconditionally.\r\n    }\r\n    disposeVariables() {\r\n        for (const varName in this.state.registeredVariables) {\r\n            const v = this.state.registeredVariables[varName];\r\n            this.disposeVariable(v);\r\n        }\r\n    }\r\n    disposeVariable(v) {\r\n        this.disposeTensor(v);\r\n        if (this.state.registeredVariables[v.name] != null) {\r\n            delete this.state.registeredVariables[v.name];\r\n        }\r\n    }\r\n    memory() {\r\n        const info = this.backend.memory();\r\n        info.numTensors = this.state.numTensors;\r\n        info.numDataBuffers = this.state.numDataBuffers;\r\n        info.numBytes = this.state.numBytes;\r\n        if (this.state.numStringTensors > 0) {\r\n            info.unreliable = true;\r\n            if (info.reasons == null) {\r\n                info.reasons = [];\r\n            }\r\n            info.reasons.push('Memory usage by string tensors is approximate ' +\r\n                '(2 bytes per character)');\r\n        }\r\n        return info;\r\n    }\r\n    async profile(query) {\r\n        this.state.profiling = true;\r\n        const startBytes = this.state.numBytes;\r\n        const startNumTensors = this.state.numTensors;\r\n        this.state.activeProfile.kernels = [];\r\n        this.state.activeProfile.result = await query();\r\n        this.state.profiling = false;\r\n        this.state.activeProfile.peakBytes = Math.max(...this.state.activeProfile.kernels.map(d => d.totalBytesSnapshot));\r\n        this.state.activeProfile.newBytes = this.state.numBytes - startBytes;\r\n        this.state.activeProfile.newTensors =\r\n            this.state.numTensors - startNumTensors;\r\n        for (const kernel of this.state.activeProfile.kernels) {\r\n            kernel.kernelTimeMs = await kernel.kernelTimeMs;\r\n            kernel.extraInfo = await kernel.extraInfo;\r\n        }\r\n        return this.state.activeProfile;\r\n    }\r\n    isTapeOn() {\r\n        return this.state.gradientDepth > 0 && this.state.kernelDepth === 0;\r\n    }\r\n    addTapeNode(kernelName, inputs, outputs, gradientsFunc, saved, attrs) {\r\n        const tapeNode = { id: this.state.nextTapeNodeId++, kernelName, inputs, outputs, saved };\r\n        const gradConfig = getGradient(kernelName);\r\n        if (gradConfig != null) {\r\n            gradientsFunc = gradConfig.gradFunc;\r\n        }\r\n        if (gradientsFunc != null) {\r\n            tapeNode.gradient = (dys) => {\r\n                // TODO(smilkov): To optimize back-prop, pass dys that are not used in\r\n                // the backprop graph to the user as null instead of zeros\r\n                dys = dys.map((dy, i) => {\r\n                    if (dy == null) {\r\n                        const output = outputs[i];\r\n                        const vals = util.makeZerosTypedArray(output.size, output.dtype);\r\n                        return this.makeTensor(vals, output.shape, output.dtype);\r\n                    }\r\n                    return dy;\r\n                });\r\n                // Grad functions of ops with single outputs expect a dy, while ops\r\n                // with multiple outputs expect dys (array of dy).\r\n                return gradientsFunc(dys.length > 1 ? dys : dys[0], saved, attrs);\r\n            };\r\n        }\r\n        this.state.activeTape.push(tapeNode);\r\n    }\r\n    keep(result) {\r\n        result.kept = true;\r\n        return result;\r\n    }\r\n    startTape() {\r\n        if (this.state.gradientDepth === 0) {\r\n            this.state.activeTape = [];\r\n        }\r\n        this.state.gradientDepth++;\r\n    }\r\n    endTape() {\r\n        this.state.gradientDepth--;\r\n    }\r\n    /**\r\n     * Start a scope. Use this with endScope() to achieve the same functionality\r\n     * as scope() without the need for a function closure.\r\n     */\r\n    startScope(name) {\r\n        const scopeInfo = {\r\n            track: [],\r\n            name: 'unnamed scope',\r\n            id: this.state.nextScopeId++\r\n        };\r\n        if (name) {\r\n            scopeInfo.name = name;\r\n        }\r\n        this.state.scopeStack.push(scopeInfo);\r\n        this.state.activeScope = scopeInfo;\r\n    }\r\n    /**\r\n     * End a scope. Use this with startScope() to achieve the same functionality\r\n     * as scope() without the need for a function closure.\r\n     */\r\n    endScope(result) {\r\n        const tensorsToTrackInParent = getTensorsInContainer(result);\r\n        const tensorsToTrackInParentSet = new Set(tensorsToTrackInParent.map(t => t.id));\r\n        // Dispose the arrays tracked in this scope.\r\n        for (let i = 0; i < this.state.activeScope.track.length; i++) {\r\n            const tensor = this.state.activeScope.track[i];\r\n            if (!tensor.kept && !tensorsToTrackInParentSet.has(tensor.id)) {\r\n                tensor.dispose();\r\n            }\r\n        }\r\n        const oldScope = this.state.scopeStack.pop();\r\n        this.state.activeScope = this.state.scopeStack.length === 0 ?\r\n            null :\r\n            this.state.scopeStack[this.state.scopeStack.length - 1];\r\n        // Track the current result in the parent scope.\r\n        tensorsToTrackInParent.forEach(tensor => {\r\n            // Only track the tensor if was allocated in the inner scope and is not\r\n            // globally kept.\r\n            if (!tensor.kept && tensor.scopeId === oldScope.id) {\r\n                this.track(tensor);\r\n            }\r\n        });\r\n    }\r\n    /**\r\n     * Returns gradients of `f` with respect to each of the `xs`. The gradients\r\n     * returned are of the same length as `xs`, but some might be null if `f`\r\n     * was not a function of that `x`. It also takes optional dy to multiply the\r\n     * gradient, which defaults to `1`.\r\n     */\r\n    gradients(f, xs, dy, allowNoGradients = false) {\r\n        util.assert(xs.length > 0, () => 'gradients() received an empty list of xs.');\r\n        if (dy != null && dy.dtype !== 'float32') {\r\n            throw new Error(`dy must have 'float32' dtype, but has '${dy.dtype}'`);\r\n        }\r\n        const y = this.scopedRun(() => this.startTape(), () => this.endTape(), () => this.tidy('forward', f));\r\n        util.assert(y instanceof Tensor, () => 'The result y returned by f() must be a tensor.');\r\n        // Filter out the nodes that don't connect x => y.\r\n        const filteredTape = getFilteredNodesXToY(this.state.activeTape, xs, y);\r\n        if (!allowNoGradients && filteredTape.length === 0 && xs.length > 0) {\r\n            throw new Error('Cannot compute gradient of y=f(x) with respect to x. Make sure ' +\r\n                'that the f you passed encloses all operations that lead from x ' +\r\n                'to y.');\r\n        }\r\n        return this.tidy('backward', () => {\r\n            const accumulatedGradientMap = {};\r\n            accumulatedGradientMap[y.id] = (dy == null) ? ones(y.shape) : dy;\r\n            // Backprop gradients through the filtered nodes.\r\n            backpropagateGradients(accumulatedGradientMap, filteredTape, \r\n            // Pass the tidy function to avoid circular dep with `tape.ts`.\r\n            f => this.tidy(f), \r\n            // Pass an add function to avoide a circular dep with `tape.ts`.\r\n            add);\r\n            const grads = xs.map(x => accumulatedGradientMap[x.id]);\r\n            if (this.state.gradientDepth === 0) {\r\n                // This means that we are not computing higher-order gradients\r\n                // and can clean up the tape.\r\n                this.state.activeTape.forEach(node => {\r\n                    for (const tensor of node.saved) {\r\n                        tensor.dispose();\r\n                    }\r\n                });\r\n                this.state.activeTape = null;\r\n            }\r\n            return { value: y, grads };\r\n        });\r\n    }\r\n    customGrad(f) {\r\n        util.assert(util.isFunction(f), () => 'The f passed in customGrad(f) must be a function.');\r\n        return (...inputs) => {\r\n            util.assert(inputs.every(t => t instanceof Tensor), () => 'The args passed in customGrad(f)(x1, x2,...) must all be ' +\r\n                'tensors');\r\n            let res;\r\n            const inputMap = {};\r\n            inputs.forEach((input, i) => {\r\n                inputMap[i] = input;\r\n            });\r\n            return this.runKernelFunc((_, save) => {\r\n                res = f(...[...inputs, save]);\r\n                util.assert(res.value instanceof Tensor, () => 'The function f passed in customGrad(f) must return an ' +\r\n                    'object where `obj.value` is a tensor');\r\n                util.assert(util.isFunction(res.gradFunc), () => 'The function f passed in customGrad(f) must return an ' +\r\n                    'object where `obj.gradFunc` is a function.');\r\n                return res.value;\r\n            }, inputMap, (dy, saved) => {\r\n                const gradRes = res.gradFunc(dy, saved);\r\n                const grads = Array.isArray(gradRes) ? gradRes : [gradRes];\r\n                util.assert(grads.length === inputs.length, () => 'The function f passed in customGrad(f) must return an ' +\r\n                    'object where `obj.gradFunc` is a function that returns ' +\r\n                    'the same number of tensors as inputs passed to f(...).');\r\n                util.assert(grads.every(t => t instanceof Tensor), () => 'The function f passed in customGrad(f) must return an ' +\r\n                    'object where `obj.gradFunc` is a function that returns ' +\r\n                    'a list of only tensors.');\r\n                const gradMap = {};\r\n                grads.forEach((grad, i) => {\r\n                    gradMap[i] = () => grad;\r\n                });\r\n                return gradMap;\r\n            });\r\n        };\r\n    }\r\n    readSync(dataId) {\r\n        // Route the read to the correct backend.\r\n        const info = this.state.tensorInfo.get(dataId);\r\n        return info.backend.readSync(dataId);\r\n    }\r\n    read(dataId) {\r\n        // Route the read to the correct backend.\r\n        const info = this.state.tensorInfo.get(dataId);\r\n        return info.backend.read(dataId);\r\n    }\r\n    async time(query) {\r\n        const start = now();\r\n        const timingInfo = await this.backend.time(query);\r\n        timingInfo.wallMs = now() - start;\r\n        return timingInfo;\r\n    }\r\n    /**\r\n     * Tracks a Tensor in the current scope to be automatically cleaned up\r\n     * when the current scope ends, and returns the value.\r\n     *\r\n     * @param result The Tensor to track in the current scope.\r\n     */\r\n    track(result) {\r\n        if (this.state.activeScope != null) {\r\n            result.scopeId = this.state.activeScope.id;\r\n            this.state.activeScope.track.push(result);\r\n        }\r\n        return result;\r\n    }\r\n    get registeredVariables() {\r\n        return this.state.registeredVariables;\r\n    }\r\n    /**\r\n     * Resets the engine state. Removes all backends but does not remove\r\n     * registered backend factories.\r\n     */\r\n    reset() {\r\n        // Make any pending promise obsolete.\r\n        this.pendingBackendInitId++;\r\n        this.state.dispose();\r\n        this.ENV.reset();\r\n        this.state = new EngineState();\r\n        for (const backendName in this.registry) {\r\n            this.disposeRegisteredKernels(backendName);\r\n            this.registry[backendName].dispose();\r\n            delete this.registry[backendName];\r\n        }\r\n        this.backendName = null;\r\n        this.backendInstance = null;\r\n        this.pendingBackendInit = null;\r\n    }\r\n}\r\nEngine.nextTensorId = 0;\r\nEngine.nextVariableId = 0;\r\nfunction ones(shape) {\r\n    const values = makeOnesTypedArray(sizeFromShape(shape), 'float32');\r\n    return ENGINE.makeTensor(values, shape, 'float32');\r\n}\r\nexport function getOrMakeEngine() {\r\n    const ns = getGlobalNamespace();\r\n    if (ns._tfengine == null) {\r\n        const environment = new Environment(ns);\r\n        ns._tfengine = new Engine(environment);\r\n    }\r\n    setEnvironmentGlobal(ns._tfengine.ENV);\r\n    // Tell the current tensor interface that the global engine is responsible\r\n    // for tracking.\r\n    setTensorTracker(() => ns._tfengine);\r\n    return ns._tfengine;\r\n}\r\nexport const ENGINE = getOrMakeEngine();\r\n/**\r\n * A implementation of the add op for use within engine and tape.\r\n *\r\n * This allows us to avoid a circular dependency between add.ts and engine.\r\n * It is exported to be available in tape tests.\r\n */\r\nexport function add(a, b) {\r\n    // We duplicate Add here to avoid a circular dependency with add.ts.\r\n    const inputs = { a, b };\r\n    return ENGINE.runKernelFunc((backend, save) => {\r\n        const res = backend.add(a, b);\r\n        save([a, b]);\r\n        return res;\r\n    }, inputs, null /* gradient */, Add);\r\n}\r\n//# sourceMappingURL=engine.js.map"]},"metadata":{},"sourceType":"module"}