{"ast":null,"code":"import _regeneratorRuntime from \"C:/Users/wongh/Documents/GitHub/tfjs-app/node_modules/@babel/runtime/regenerator\";\nimport _asyncToGenerator from \"C:/Users/wongh/Documents/GitHub/tfjs-app/node_modules/@babel/runtime/helpers/esm/asyncToGenerator\";\nimport _classCallCheck from \"C:/Users/wongh/Documents/GitHub/tfjs-app/node_modules/@babel/runtime/helpers/esm/classCallCheck\";\nimport _createClass from \"C:/Users/wongh/Documents/GitHub/tfjs-app/node_modules/@babel/runtime/helpers/esm/createClass\";\nimport _inherits from \"C:/Users/wongh/Documents/GitHub/tfjs-app/node_modules/@babel/runtime/helpers/esm/inherits\";\nimport _createSuper from \"C:/Users/wongh/Documents/GitHub/tfjs-app/node_modules/@babel/runtime/helpers/esm/createSuper\";\n\n/**\r\n * @license\r\n * Copyright 2018 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\nimport { dispose as _dispose } from '../globals';\nimport { variableGrads } from '../gradients';\nimport { scalar } from '../ops/ops';\nimport { Serializable } from '../serialization';\n/** @doc {heading: 'Training', subheading: 'Classes', namespace: 'train'} */\n\nexport var Optimizer = /*#__PURE__*/function (_Serializable) {\n  _inherits(Optimizer, _Serializable);\n\n  var _super = _createSuper(Optimizer);\n\n  function Optimizer() {\n    _classCallCheck(this, Optimizer);\n\n    return _super.apply(this, arguments);\n  }\n\n  _createClass(Optimizer, [{\n    key: \"minimize\",\n\n    /**\r\n     * Executes `f()` and minimizes the scalar output of `f()` by computing\r\n     * gradients of y with respect to the list of trainable variables provided by\r\n     * `varList`. If no list is provided, it defaults to all trainable variables.\r\n     *\r\n     * @param f The function to execute and whose output to minimize.\r\n     * @param returnCost Whether to return the scalar cost value produced by\r\n     * executing `f()`.\r\n     * @param varList An optional list of variables to update. If specified, only\r\n     * the trainable variables in varList will be updated by minimize. Defaults to\r\n     * all trainable variables.\r\n     *\r\n     * @doc {heading: 'Training', subheading: 'Optimizers'}\r\n     */\n    value: function minimize(f) {\n      var returnCost = arguments.length > 1 && arguments[1] !== undefined ? arguments[1] : false;\n      var varList = arguments.length > 2 ? arguments[2] : undefined;\n\n      var _this$computeGradient = this.computeGradients(f, varList),\n          value = _this$computeGradient.value,\n          grads = _this$computeGradient.grads;\n\n      if (varList != null) {\n        var gradArray = varList.map(function (v) {\n          return {\n            name: v.name,\n            tensor: grads[v.name]\n          };\n        });\n        this.applyGradients(gradArray);\n      } else {\n        this.applyGradients(grads);\n      } // Dispose gradients.\n\n\n      _dispose(grads);\n\n      if (returnCost) {\n        return value;\n      } else {\n        value.dispose();\n        return null;\n      }\n    }\n    /**\r\n     * The number of iterations that this optimizer instance has been invoked for.\r\n     */\n\n  }, {\n    key: \"incrementIterations\",\n    value: function incrementIterations() {\n      this.iterations_ = this.iterations + 1;\n    }\n    /**\r\n     * Executes f() and computes the gradient of the scalar output of f() with\r\n     * respect to the list of trainable variables provided by `varList`. If no\r\n     * list is provided, it defaults to all trainable variables.\r\n     *\r\n     * @param f The function to execute and whose output to use for computing\r\n     * gradients with respect to variables.\r\n     * @param varList An optional list of variables to compute gradients with\r\n     * respect to. If specified, only the trainable variables in varList will have\r\n     * gradients computed with respect to. Defaults to all trainable variables.\r\n     *\r\n     * @doc {heading: 'Training', subheading: 'Optimizers'}\r\n     */\n\n  }, {\n    key: \"computeGradients\",\n    value: function computeGradients(f, varList) {\n      return variableGrads(f, varList);\n    }\n    /**\r\n     * Dispose the variables (if any) owned by this optimizer instance.\r\n     */\n\n  }, {\n    key: \"dispose\",\n    value: function dispose() {\n      if (this.iterations_ != null) {\n        _dispose(this.iterations_);\n      }\n    }\n  }, {\n    key: \"saveIterations\",\n    value: function () {\n      var _saveIterations = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee() {\n        return _regeneratorRuntime.wrap(function _callee$(_context) {\n          while (1) {\n            switch (_context.prev = _context.next) {\n              case 0:\n                if (this.iterations_ == null) {\n                  this.iterations_ = 0;\n                }\n\n                return _context.abrupt(\"return\", {\n                  name: 'iter',\n                  // TODO(cais): Use 'int64' type when available.\n                  tensor: scalar(this.iterations_, 'int32')\n                });\n\n              case 2:\n              case \"end\":\n                return _context.stop();\n            }\n          }\n        }, _callee, this);\n      }));\n\n      function saveIterations() {\n        return _saveIterations.apply(this, arguments);\n      }\n\n      return saveIterations;\n    }()\n  }, {\n    key: \"getWeights\",\n    value: function () {\n      var _getWeights = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee2() {\n        return _regeneratorRuntime.wrap(function _callee2$(_context2) {\n          while (1) {\n            switch (_context2.prev = _context2.next) {\n              case 0:\n                throw new Error('getWeights() is not implemented for this optimizer yet.');\n\n              case 1:\n              case \"end\":\n                return _context2.stop();\n            }\n          }\n        }, _callee2);\n      }));\n\n      function getWeights() {\n        return _getWeights.apply(this, arguments);\n      }\n\n      return getWeights;\n    }()\n  }, {\n    key: \"setWeights\",\n    value: function () {\n      var _setWeights = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee3(weightValues) {\n        return _regeneratorRuntime.wrap(function _callee3$(_context3) {\n          while (1) {\n            switch (_context3.prev = _context3.next) {\n              case 0:\n                throw new Error(\"setWeights() is not implemented for this optimizer class \" + \"\".concat(this.getClassName()));\n\n              case 1:\n              case \"end\":\n                return _context3.stop();\n            }\n          }\n        }, _callee3, this);\n      }));\n\n      function setWeights(_x) {\n        return _setWeights.apply(this, arguments);\n      }\n\n      return setWeights;\n    }()\n    /**\r\n     * Extract the first element of the weight values and set it\r\n     * as the iterations counter variable of this instance of optimizer.\r\n     *\r\n     * @param weightValues\r\n     * @returns Weight values with the first element consumed and excluded.\r\n     */\n\n  }, {\n    key: \"extractIterations\",\n    value: function () {\n      var _extractIterations = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime.mark(function _callee4(weightValues) {\n        return _regeneratorRuntime.wrap(function _callee4$(_context4) {\n          while (1) {\n            switch (_context4.prev = _context4.next) {\n              case 0:\n                _context4.next = 2;\n                return weightValues[0].tensor.data();\n\n              case 2:\n                this.iterations_ = _context4.sent[0];\n                return _context4.abrupt(\"return\", weightValues.slice(1));\n\n              case 4:\n              case \"end\":\n                return _context4.stop();\n            }\n          }\n        }, _callee4, this);\n      }));\n\n      function extractIterations(_x2) {\n        return _extractIterations.apply(this, arguments);\n      }\n\n      return extractIterations;\n    }()\n  }, {\n    key: \"iterations\",\n    get: function get() {\n      if (this.iterations_ == null) {\n        this.iterations_ = 0;\n      }\n\n      return this.iterations_;\n    }\n  }]);\n\n  return Optimizer;\n}(Serializable);\nObject.defineProperty(Optimizer, Symbol.hasInstance, {\n  value: function value(instance) {\n    return instance.minimize != null && instance.computeGradients != null && instance.applyGradients != null;\n  }\n});","map":{"version":3,"sources":["../../src/optimizers/optimizer.ts"],"names":[],"mappings":";;;;;;;AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,OAAO,IAAP,QAAR,QAAsB,YAAtB;AACA,SAAQ,aAAR,QAA4B,cAA5B;AACA,SAAQ,MAAR,QAAqB,YAArB;AACA,SAAQ,YAAR,QAA2B,kBAA3B;AAoBA;;AACA,WAAsB,SAAtB;AAAA;;AAAA;;AAAA;AAAA;;AAAA;AAAA;;AAAA;AAAA;;AAGE;;;;;;;;;;;;;AAaG;AAhBL,6BAiBW,CAjBX,EAiBoE;AAAA,UAAxC,UAAwC,uEAA3B,KAA2B;AAAA,UAApB,OAAoB;;AAAA,kCAEzC,KAAK,gBAAL,CAAsB,CAAtB,EAAyB,OAAzB,CAFyC;AAAA,UAEzD,KAFyD,yBAEzD,KAFyD;AAAA,UAElD,KAFkD,yBAElD,KAFkD;;AAIhE,UAAI,OAAO,IAAI,IAAf,EAAqB;AACnB,YAAM,SAAS,GACX,OAAO,CAAC,GAAR,CAAY,UAAA,CAAC;AAAA,iBAAK;AAAC,YAAA,IAAI,EAAE,CAAC,CAAC,IAAT;AAAe,YAAA,MAAM,EAAE,KAAK,CAAC,CAAC,CAAC,IAAH;AAA5B,WAAL;AAAA,SAAb,CADJ;AAEA,aAAK,cAAL,CAAoB,SAApB;AACD,OAJD,MAIO;AACL,aAAK,cAAL,CAAoB,KAApB;AACD,OAV+D,CAYhE;;;AACA,MAAA,QAAO,CAAC,KAAD,CAAP;;AAEA,UAAI,UAAJ,EAAgB;AACd,eAAO,KAAP;AACD,OAFD,MAEO;AACL,QAAA,KAAK,CAAC,OAAN;AACA,eAAO,IAAP;AACD;AACF;AAED;;AAEG;;AA1CL;AAAA;AAAA,0CAkD+B;AAC3B,WAAK,WAAL,GAAmB,KAAK,UAAL,GAAkB,CAArC;AACD;AAED;;;;;;;;;;;;AAYG;;AAlEL;AAAA;AAAA,qCAmEmB,CAnEnB,EAmEoC,OAnEpC,EAmEwD;AAEpD,aAAO,aAAa,CAAC,CAAD,EAAI,OAAJ,CAApB;AACD;AAYD;;AAEG;;AApFL;AAAA;AAAA,8BAqFS;AACL,UAAI,KAAK,WAAL,IAAoB,IAAxB,EAA8B;AAC5B,QAAA,QAAO,CAAC,KAAK,WAAN,CAAP;AACD;AACF;AAzFH;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AA4FI,oBAAI,KAAK,WAAL,IAAoB,IAAxB,EAA8B;AAC5B,uBAAK,WAAL,GAAmB,CAAnB;AACD;;AA9FL,iDA+FW;AACL,kBAAA,IAAI,EAAE,MADD;AAEL;AACA,kBAAA,MAAM,EAAE,MAAM,CAAC,KAAK,WAAN,EAAmB,OAAnB;AAHT,iBA/FX;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;AAAA;AAAA;AAAA;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,sBAuGU,IAAI,KAAJ,CAAU,yDAAV,CAvGV;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;AAAA;AAAA;AAAA;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,mGA0GmB,YA1GnB;AAAA;AAAA;AAAA;AAAA;AAAA,sBA2GU,IAAI,KAAJ,CACF,wEACG,KAAK,YAAL,EADH,CADE,CA3GV;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;AAAA;AAAA;AAAA;;AAAA;AAAA;AAgHE;;;;;;AAMG;;AAtHL;AAAA;AAAA;AAAA,0GAuHoC,YAvHpC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,uBAyH8B,YAAY,CAAC,CAAD,CAAZ,CAAgB,MAAhB,CAAuB,IAAvB,EAzH9B;;AAAA;AAyHI,qBAAK,WAzHT,kBAyH6D,CAzH7D;AAAA,kDA0HW,YAAY,CAAC,KAAb,CAAmB,CAAnB,CA1HX;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;AAAA;AAAA;AAAA;;AAAA;AAAA;AAAA;AAAA;AAAA,wBA2CgB;AACZ,UAAI,KAAK,WAAL,IAAoB,IAAxB,EAA8B;AAC5B,aAAK,WAAL,GAAmB,CAAnB;AACD;;AACD,aAAO,KAAK,WAAZ;AACD;AAhDH;;AAAA;AAAA,EAAwC,YAAxC;AA8HA,MAAM,CAAC,cAAP,CAAsB,SAAtB,EAAiC,MAAM,CAAC,WAAxC,EAAqD;AACnD,EAAA,KAAK,EAAE,eAAC,QAAD,EAAwB;AAC7B,WAAO,QAAQ,CAAC,QAAT,IAAqB,IAArB,IAA6B,QAAQ,CAAC,gBAAT,IAA6B,IAA1D,IACH,QAAQ,CAAC,cAAT,IAA2B,IAD/B;AAED;AAJkD,CAArD","sourceRoot":"","sourcesContent":["/**\r\n * @license\r\n * Copyright 2018 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\r\nimport { dispose } from '../globals';\r\nimport { variableGrads } from '../gradients';\r\nimport { scalar } from '../ops/ops';\r\nimport { Serializable } from '../serialization';\r\n/** @doc {heading: 'Training', subheading: 'Classes', namespace: 'train'} */\r\nexport class Optimizer extends Serializable {\r\n    /**\r\n     * Executes `f()` and minimizes the scalar output of `f()` by computing\r\n     * gradients of y with respect to the list of trainable variables provided by\r\n     * `varList`. If no list is provided, it defaults to all trainable variables.\r\n     *\r\n     * @param f The function to execute and whose output to minimize.\r\n     * @param returnCost Whether to return the scalar cost value produced by\r\n     * executing `f()`.\r\n     * @param varList An optional list of variables to update. If specified, only\r\n     * the trainable variables in varList will be updated by minimize. Defaults to\r\n     * all trainable variables.\r\n     *\r\n     * @doc {heading: 'Training', subheading: 'Optimizers'}\r\n     */\r\n    minimize(f, returnCost = false, varList) {\r\n        const { value, grads } = this.computeGradients(f, varList);\r\n        if (varList != null) {\r\n            const gradArray = varList.map(v => ({ name: v.name, tensor: grads[v.name] }));\r\n            this.applyGradients(gradArray);\r\n        }\r\n        else {\r\n            this.applyGradients(grads);\r\n        }\r\n        // Dispose gradients.\r\n        dispose(grads);\r\n        if (returnCost) {\r\n            return value;\r\n        }\r\n        else {\r\n            value.dispose();\r\n            return null;\r\n        }\r\n    }\r\n    /**\r\n     * The number of iterations that this optimizer instance has been invoked for.\r\n     */\r\n    get iterations() {\r\n        if (this.iterations_ == null) {\r\n            this.iterations_ = 0;\r\n        }\r\n        return this.iterations_;\r\n    }\r\n    incrementIterations() {\r\n        this.iterations_ = this.iterations + 1;\r\n    }\r\n    /**\r\n     * Executes f() and computes the gradient of the scalar output of f() with\r\n     * respect to the list of trainable variables provided by `varList`. If no\r\n     * list is provided, it defaults to all trainable variables.\r\n     *\r\n     * @param f The function to execute and whose output to use for computing\r\n     * gradients with respect to variables.\r\n     * @param varList An optional list of variables to compute gradients with\r\n     * respect to. If specified, only the trainable variables in varList will have\r\n     * gradients computed with respect to. Defaults to all trainable variables.\r\n     *\r\n     * @doc {heading: 'Training', subheading: 'Optimizers'}\r\n     */\r\n    computeGradients(f, varList) {\r\n        return variableGrads(f, varList);\r\n    }\r\n    /**\r\n     * Dispose the variables (if any) owned by this optimizer instance.\r\n     */\r\n    dispose() {\r\n        if (this.iterations_ != null) {\r\n            dispose(this.iterations_);\r\n        }\r\n    }\r\n    async saveIterations() {\r\n        if (this.iterations_ == null) {\r\n            this.iterations_ = 0;\r\n        }\r\n        return {\r\n            name: 'iter',\r\n            // TODO(cais): Use 'int64' type when available.\r\n            tensor: scalar(this.iterations_, 'int32')\r\n        };\r\n    }\r\n    async getWeights() {\r\n        throw new Error('getWeights() is not implemented for this optimizer yet.');\r\n    }\r\n    async setWeights(weightValues) {\r\n        throw new Error(`setWeights() is not implemented for this optimizer class ` +\r\n            `${this.getClassName()}`);\r\n    }\r\n    /**\r\n     * Extract the first element of the weight values and set it\r\n     * as the iterations counter variable of this instance of optimizer.\r\n     *\r\n     * @param weightValues\r\n     * @returns Weight values with the first element consumed and excluded.\r\n     */\r\n    async extractIterations(weightValues) {\r\n        this.iterations_ = (await weightValues[0].tensor.data())[0];\r\n        return weightValues.slice(1);\r\n    }\r\n}\r\nObject.defineProperty(Optimizer, Symbol.hasInstance, {\r\n    value: (instance) => {\r\n        return instance.minimize != null && instance.computeGradients != null &&\r\n            instance.applyGradients != null;\r\n    }\r\n});\r\n//# sourceMappingURL=optimizer.js.map"]},"metadata":{},"sourceType":"module"}