{"ast":null,"code":"/**\r\n * @license\r\n * Copyright 2019 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { _FusedMatMul } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { matMul as unfusedMatMul } from '../mat_mul';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\r\n * Computes the dot product of two matrices with optional activation and bias.\r\n *\r\n * ```js\r\n * const a = tf.tensor2d([-1, -2], [1, 2]);\r\n * const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);\r\n * const bias = tf.tensor2d([1, 2], [1, 2]);\r\n *\r\n * tf.fused.matMul({a, b, bias, activation: 'relu'}).print();\r\n * ```\r\n *\r\n * @param obj An object with the following properties:\r\n * - `a` First matrix in dot product operation.\r\n * - `b` Second matrix in dot product operation.\r\n * - `transposeA` If true, `a` is transposed before multiplication.\r\n * - `transposeB` If true, `b` is transposed before multiplication.\r\n * - `bias` Matrix to be added to the result.\r\n * - `activation` Name of activation kernel (defaults to `linear`).\r\n * - `preluActivationWeights` Tensor of prelu weights.\r\n */\n\nfunction fusedMatMul_({\n  a,\n  b,\n  transposeA = false,\n  transposeB = false,\n  bias,\n  activation = 'linear',\n  preluActivationWeights\n}) {\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    let result = unfusedMatMul(a, b, transposeA, transposeB);\n\n    if (bias != null) {\n      result = add(result, bias);\n    }\n\n    return applyActivation(result, activation, preluActivationWeights);\n  }\n\n  let $a = convertToTensor(a, 'a', 'fused matMul');\n  let $b = convertToTensor(b, 'b', 'fused matMul');\n  [$a, $b] = makeTypesMatch($a, $b);\n  const innerShapeA = transposeA ? $a.shape[$a.rank - 2] : $a.shape[$a.rank - 1];\n  const innerShapeB = transposeB ? $b.shape[$b.rank - 1] : $b.shape[$b.rank - 2];\n  const outerShapeA = transposeA ? $a.shape[$a.rank - 1] : $a.shape[$a.rank - 2];\n  const outerShapeB = transposeB ? $b.shape[$b.rank - 2] : $b.shape[$b.rank - 1];\n  const outerDimsA = $a.shape.slice(0, -2);\n  const outerDimsB = $b.shape.slice(0, -2);\n  const batchDimA = util.sizeFromShape(outerDimsA);\n  const batchDimB = util.sizeFromShape(outerDimsB);\n  util.assert($a.rank >= 2 && $b.rank >= 2 && $a.rank === $b.rank, () => `Error in fused matMul: inputs must have the same rank of at least ` + `2, got ranks ${$a.rank} and ${$b.rank}.`);\n  util.assert(util.arraysEqual(outerDimsA, outerDimsB), () => `Error in fused matMul: outer dimensions (${outerDimsA}) and (` + `${outerDimsB}) of Tensors with shapes ${$a.shape} and ` + `${$b.shape} must match.`);\n  util.assert(innerShapeA === innerShapeB, () => `Error in fused matMul: inner shapes (${innerShapeA}) and (` + `${innerShapeB}) of Tensors with shapes ${$a.shape} and ` + `${$b.shape} and transposeA=${transposeA}` + ` and transposeB=${transposeB} must match.`);\n  const outShape = $a.shape.slice(0, -2).concat([outerShapeA, outerShapeB]);\n  const a3D = transposeA ? reshape($a, [batchDimA, innerShapeA, outerShapeA]) : reshape($a, [batchDimA, outerShapeA, innerShapeA]);\n  const b3D = transposeB ? reshape($b, [batchDimB, outerShapeB, innerShapeB]) : reshape($b, [batchDimB, innerShapeB, outerShapeB]);\n  let $bias;\n\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused matMul');\n    [$bias] = makeTypesMatch($bias, $a);\n    broadcast_util.assertAndGetBroadcastShape(outShape, $bias.shape);\n  }\n\n  let $preluActivationWeights;\n\n  if (preluActivationWeights != null) {\n    $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused matMul');\n  }\n\n  const grad = (dy, saved) => {\n    const [a3D, b3D, y, $bias] = saved; // we reshape dy because the result of the forward is not\n    // necessarily going to be a 3d tensor due to a reshape done at the end of\n    // the customOp.\n\n    const dyActivation = getFusedDyActivation(reshape(dy, y.shape), y, activation);\n    let aDer;\n    let bDer;\n\n    if (!transposeA && !transposeB) {\n      aDer = unfusedMatMul(dyActivation, b3D, false, true);\n      bDer = unfusedMatMul(a3D, dyActivation, true, false);\n    } else if (!transposeA && transposeB) {\n      aDer = unfusedMatMul(dyActivation, b3D, false, false);\n      bDer = unfusedMatMul(dyActivation, a3D, true, false);\n    } else if (transposeA && !transposeB) {\n      aDer = unfusedMatMul(b3D, dyActivation, false, true);\n      bDer = unfusedMatMul(a3D, dyActivation, false, false);\n    } else {\n      aDer = unfusedMatMul(b3D, dyActivation, true, true);\n      bDer = unfusedMatMul(dyActivation, a3D, true, true);\n    }\n\n    if (bias != null) {\n      const biasDer = getFusedBiasGradient($bias, dyActivation);\n      return [aDer, bDer, biasDer];\n    } else {\n      return [aDer, bDer];\n    }\n  };\n\n  const forward = backend => {\n    const y = backend.fusedBatchMatMul({\n      a: a3D,\n      b: b3D,\n      transposeA,\n      transposeB,\n      bias: $bias,\n      activation,\n      preluActivationWeights: $preluActivationWeights\n    });\n    return y;\n  };\n\n  const inputs = {\n    a: a3D,\n    b: b3D,\n    bias: $bias,\n    preluActivationWeights: $preluActivationWeights\n  };\n  const attrs = {\n    transposeA,\n    transposeB,\n    activation\n  }; // Depending on the the params passed in we will have different number of\n  // inputs and thus a a different number of elements in the gradient.\n\n  if (bias == null) {\n    const customOp = customGrad((a3D, b3D, save) => {\n      const res = ENGINE.runKernelFunc(forward, inputs, null\n      /* grad */\n      , _FusedMatMul, attrs);\n      save([a3D, b3D, res]);\n      return {\n        value: reshape(res, outShape),\n        gradFunc: grad\n      };\n    });\n    return customOp(a3D, b3D);\n  } else {\n    const customOpWithBias = customGrad((a3D, b3D, $bias, save) => {\n      const res = ENGINE.runKernelFunc(forward, inputs, null\n      /* grad */\n      , _FusedMatMul, attrs);\n      save([a3D, b3D, res, $bias]);\n      return {\n        value: reshape(res, outShape),\n        gradFunc: grad\n      };\n    });\n    return customOpWithBias(a3D, b3D, $bias);\n  }\n}\n\nexport const matMul = op({\n  fusedMatMul_\n});","map":{"version":3,"sources":["../../../src/ops/fused/mat_mul.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,MAAR,QAAkC,cAAlC;AACA,SAAQ,UAAR,QAAyB,iBAAzB;AACA,SAAQ,YAAR,QAAkE,oBAAlE;AAIA,SAAQ,cAAR,QAA6B,mBAA7B;AACA,SAAQ,eAAR,QAA8B,uBAA9B;AAEA,OAAO,KAAK,IAAZ,MAAsB,YAAtB;AAEA,SAAQ,GAAR,QAAkB,QAAlB;AACA,OAAO,KAAK,cAAZ,MAAgC,mBAAhC;AAEA,SAAQ,eAAR,EAAyB,oBAAzB,EAA+C,oBAA/C,EAAqE,UAArE,QAAsF,eAAtF;AACA,SAAQ,MAAM,IAAI,aAAlB,QAAsC,YAAtC;AACA,SAAQ,EAAR,QAAiB,cAAjB;AACA,SAAQ,OAAR,QAAsB,YAAtB;AAEA;;;;;;;;;;;;;;;;;;;AAmBG;;AACH,SAAS,YAAT,CAAwC;AACtC,EAAA,CADsC;AAEtC,EAAA,CAFsC;AAGtC,EAAA,UAAU,GAAG,KAHyB;AAItC,EAAA,UAAU,GAAG,KAJyB;AAKtC,EAAA,IALsC;AAMtC,EAAA,UAAU,GAAG,QANyB;AAOtC,EAAA;AAPsC,CAAxC,EAgBC;AACC,MAAI,UAAU,CAAC,MAAM,CAAC,KAAP,CAAa,aAAd,EAA6B,UAA7B,CAAV,KAAuD,KAA3D,EAAkE;AAChE,QAAI,MAAM,GAAG,aAAa,CAAC,CAAD,EAAI,CAAJ,EAAO,UAAP,EAAmB,UAAnB,CAA1B;;AACA,QAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,MAAA,MAAM,GAAG,GAAG,CAAC,MAAD,EAAS,IAAT,CAAZ;AACD;;AAED,WAAO,eAAe,CAAC,MAAD,EAAS,UAAT,EAAqB,sBAArB,CAAtB;AACD;;AAED,MAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,cAAT,CAAxB;AACA,MAAI,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,cAAT,CAAxB;AACA,GAAC,EAAD,EAAK,EAAL,IAAW,cAAc,CAAC,EAAD,EAAK,EAAL,CAAzB;AAEA,QAAM,WAAW,GACb,UAAU,GAAG,EAAE,CAAC,KAAH,CAAS,EAAE,CAAC,IAAH,GAAU,CAAnB,CAAH,GAA2B,EAAE,CAAC,KAAH,CAAS,EAAE,CAAC,IAAH,GAAU,CAAnB,CADzC;AAEA,QAAM,WAAW,GACb,UAAU,GAAG,EAAE,CAAC,KAAH,CAAS,EAAE,CAAC,IAAH,GAAU,CAAnB,CAAH,GAA2B,EAAE,CAAC,KAAH,CAAS,EAAE,CAAC,IAAH,GAAU,CAAnB,CADzC;AAGA,QAAM,WAAW,GACb,UAAU,GAAG,EAAE,CAAC,KAAH,CAAS,EAAE,CAAC,IAAH,GAAU,CAAnB,CAAH,GAA2B,EAAE,CAAC,KAAH,CAAS,EAAE,CAAC,IAAH,GAAU,CAAnB,CADzC;AAEA,QAAM,WAAW,GACb,UAAU,GAAG,EAAE,CAAC,KAAH,CAAS,EAAE,CAAC,IAAH,GAAU,CAAnB,CAAH,GAA2B,EAAE,CAAC,KAAH,CAAS,EAAE,CAAC,IAAH,GAAU,CAAnB,CADzC;AAGA,QAAM,UAAU,GAAG,EAAE,CAAC,KAAH,CAAS,KAAT,CAAe,CAAf,EAAkB,CAAC,CAAnB,CAAnB;AACA,QAAM,UAAU,GAAG,EAAE,CAAC,KAAH,CAAS,KAAT,CAAe,CAAf,EAAkB,CAAC,CAAnB,CAAnB;AACA,QAAM,SAAS,GAAG,IAAI,CAAC,aAAL,CAAmB,UAAnB,CAAlB;AACA,QAAM,SAAS,GAAG,IAAI,CAAC,aAAL,CAAmB,UAAnB,CAAlB;AAEA,EAAA,IAAI,CAAC,MAAL,CACI,EAAE,CAAC,IAAH,IAAW,CAAX,IAAgB,EAAE,CAAC,IAAH,IAAW,CAA3B,IAAgC,EAAE,CAAC,IAAH,KAAY,EAAE,CAAC,IADnD,EAEI,MACI,oEAAA,GACA,gBAAgB,EAAE,CAAC,IAAI,QAAQ,EAAE,CAAC,IAAI,GAJ9C;AAMA,EAAA,IAAI,CAAC,MAAL,CACI,IAAI,CAAC,WAAL,CAAiB,UAAjB,EAA6B,UAA7B,CADJ,EAEI,MAAM,4CAA4C,UAAU,SAAtD,GACF,GAAG,UAAU,4BAA4B,EAAE,CAAC,KAAK,OAD/C,GAEF,GAAG,EAAE,CAAC,KAAK,cAJnB;AAMA,EAAA,IAAI,CAAC,MAAL,CACI,WAAW,KAAK,WADpB,EAEI,MAAM,wCAAwC,WAAW,SAAnD,GACF,GAAG,WAAW,4BAA4B,EAAE,CAAC,KAAK,OADhD,GAEF,GAAG,EAAE,CAAC,KAAK,mBAAmB,UAAU,EAFtC,GAGF,mBAAmB,UAAU,cALrC;AAOA,QAAM,QAAQ,GAAG,EAAE,CAAC,KAAH,CAAS,KAAT,CAAe,CAAf,EAAkB,CAAC,CAAnB,EAAsB,MAAtB,CAA6B,CAAC,WAAD,EAAc,WAAd,CAA7B,CAAjB;AAEA,QAAM,GAAG,GAAa,UAAU,GAC5B,OAAO,CAAC,EAAD,EAAK,CAAC,SAAD,EAAY,WAAZ,EAAyB,WAAzB,CAAL,CADqB,GAE5B,OAAO,CAAC,EAAD,EAAK,CAAC,SAAD,EAAY,WAAZ,EAAyB,WAAzB,CAAL,CAFX;AAGA,QAAM,GAAG,GAAa,UAAU,GAC5B,OAAO,CAAC,EAAD,EAAK,CAAC,SAAD,EAAY,WAAZ,EAAyB,WAAzB,CAAL,CADqB,GAE5B,OAAO,CAAC,EAAD,EAAK,CAAC,SAAD,EAAY,WAAZ,EAAyB,WAAzB,CAAL,CAFX;AAIA,MAAI,KAAJ;;AACA,MAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,IAAA,KAAK,GAAG,eAAe,CAAC,IAAD,EAAO,MAAP,EAAe,cAAf,CAAvB;AACA,KAAC,KAAD,IAAU,cAAc,CAAC,KAAD,EAAQ,EAAR,CAAxB;AAEA,IAAA,cAAc,CAAC,0BAAf,CAA0C,QAA1C,EAAoD,KAAK,CAAC,KAA1D;AACD;;AAED,MAAI,uBAAJ;;AACA,MAAI,sBAAsB,IAAI,IAA9B,EAAoC;AAClC,IAAA,uBAAuB,GAAG,eAAe,CACrC,sBADqC,EACb,eADa,EACI,cADJ,CAAzC;AAED;;AAED,QAAM,IAAI,GAAG,CAAC,EAAD,EAAe,KAAf,KAAkC;AAC7C,UAAM,CAAC,GAAD,EAAM,GAAN,EAAW,CAAX,EAAc,KAAd,IAAuB,KAA7B,CAD6C,CAE7C;AACA;AACA;;AACA,UAAM,YAAY,GACd,oBAAoB,CAAC,OAAO,CAAC,EAAD,EAAK,CAAC,CAAC,KAAP,CAAR,EAAuB,CAAvB,EAA0B,UAA1B,CADxB;AAEA,QAAI,IAAJ;AACA,QAAI,IAAJ;;AAEA,QAAI,CAAC,UAAD,IAAe,CAAC,UAApB,EAAgC;AAC9B,MAAA,IAAI,GAAG,aAAa,CAAC,YAAD,EAAe,GAAf,EAAoB,KAApB,EAA2B,IAA3B,CAApB;AACA,MAAA,IAAI,GAAG,aAAa,CAAC,GAAD,EAAM,YAAN,EAAoB,IAApB,EAA0B,KAA1B,CAApB;AACD,KAHD,MAGO,IAAI,CAAC,UAAD,IAAe,UAAnB,EAA+B;AACpC,MAAA,IAAI,GAAG,aAAa,CAAC,YAAD,EAAe,GAAf,EAAoB,KAApB,EAA2B,KAA3B,CAApB;AACA,MAAA,IAAI,GAAG,aAAa,CAAC,YAAD,EAAe,GAAf,EAAoB,IAApB,EAA0B,KAA1B,CAApB;AACD,KAHM,MAGA,IAAI,UAAU,IAAI,CAAC,UAAnB,EAA+B;AACpC,MAAA,IAAI,GAAG,aAAa,CAAC,GAAD,EAAM,YAAN,EAAoB,KAApB,EAA2B,IAA3B,CAApB;AACA,MAAA,IAAI,GAAG,aAAa,CAAC,GAAD,EAAM,YAAN,EAAoB,KAApB,EAA2B,KAA3B,CAApB;AACD,KAHM,MAGA;AACL,MAAA,IAAI,GAAG,aAAa,CAAC,GAAD,EAAM,YAAN,EAAoB,IAApB,EAA0B,IAA1B,CAApB;AACA,MAAA,IAAI,GAAG,aAAa,CAAC,YAAD,EAAe,GAAf,EAAoB,IAApB,EAA0B,IAA1B,CAApB;AACD;;AAED,QAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,YAAM,OAAO,GAAG,oBAAoB,CAAC,KAAD,EAAQ,YAAR,CAApC;AACA,aAAO,CAAC,IAAD,EAAO,IAAP,EAAa,OAAb,CAAP;AACD,KAHD,MAGO;AACL,aAAO,CAAC,IAAD,EAAO,IAAP,CAAP;AACD;AACF,GA9BD;;AAgCA,QAAM,OAAO,GAAyB,OAAD,IAAY;AAC/C,UAAM,CAAC,GAAG,OAAO,CAAC,gBAAR,CAAyB;AACjC,MAAA,CAAC,EAAE,GAD8B;AAEjC,MAAA,CAAC,EAAE,GAF8B;AAGjC,MAAA,UAHiC;AAIjC,MAAA,UAJiC;AAKjC,MAAA,IAAI,EAAE,KAL2B;AAMjC,MAAA,UANiC;AAOjC,MAAA,sBAAsB,EAAE;AAPS,KAAzB,CAAV;AASA,WAAO,CAAP;AACD,GAXD;;AAaA,QAAM,MAAM,GAAuB;AACjC,IAAA,CAAC,EAAE,GAD8B;AAEjC,IAAA,CAAC,EAAE,GAF8B;AAGjC,IAAA,IAAI,EAAE,KAH2B;AAIjC,IAAA,sBAAsB,EAAE;AAJS,GAAnC;AAMA,QAAM,KAAK,GAAsB;AAAC,IAAA,UAAD;AAAa,IAAA,UAAb;AAAyB,IAAA;AAAzB,GAAjC,CA1HD,CA4HC;AACA;;AACA,MAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,UAAM,QAAQ,GACV,UAAU,CAAC,CAAC,GAAD,EAAgB,GAAhB,EAA+B,IAA/B,KAAqD;AAC9D,YAAM,GAAG,GAAG,MAAM,CAAC,aAAP,CACR,OADQ,EACC,MADD,EACiC;AAAK;AADtC,QAER,YAFQ,EAEM,KAFN,CAAZ;AAIA,MAAA,IAAI,CAAC,CAAC,GAAD,EAAM,GAAN,EAAW,GAAX,CAAD,CAAJ;AAEA,aAAO;AAAC,QAAA,KAAK,EAAE,OAAO,CAAC,GAAD,EAAM,QAAN,CAAf;AAAgC,QAAA,QAAQ,EAAE;AAA1C,OAAP;AACD,KARS,CADd;AAUA,WAAO,QAAQ,CAAC,GAAD,EAAM,GAAN,CAAf;AACD,GAZD,MAYO;AACL,UAAM,gBAAgB,GAAG,UAAU,CAC/B,CAAC,GAAD,EAAgB,GAAhB,EAA+B,KAA/B,EAA8C,IAA9C,KAAoE;AAClE,YAAM,GAAG,GAAG,MAAM,CAAC,aAAP,CACR,OADQ,EACC,MADD,EACiC;AAAK;AADtC,QAER,YAFQ,EAEM,KAFN,CAAZ;AAIA,MAAA,IAAI,CAAC,CAAC,GAAD,EAAM,GAAN,EAAW,GAAX,EAAgB,KAAhB,CAAD,CAAJ;AAEA,aAAO;AAAC,QAAA,KAAK,EAAE,OAAO,CAAC,GAAD,EAAM,QAAN,CAAf;AAAgC,QAAA,QAAQ,EAAE;AAA1C,OAAP;AACD,KAT8B,CAAnC;AAWA,WAAO,gBAAgB,CAAC,GAAD,EAAM,GAAN,EAAW,KAAX,CAAvB;AACD;AACF;;AAED,OAAO,MAAM,MAAM,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAjB","sourceRoot":"","sourcesContent":["/**\r\n * @license\r\n * Copyright 2019 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\r\nimport { ENGINE } from '../../engine';\r\nimport { customGrad } from '../../gradients';\r\nimport { _FusedMatMul } from '../../kernel_names';\r\nimport { makeTypesMatch } from '../../tensor_util';\r\nimport { convertToTensor } from '../../tensor_util_env';\r\nimport * as util from '../../util';\r\nimport { add } from '../add';\r\nimport * as broadcast_util from '../broadcast_util';\r\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\r\nimport { matMul as unfusedMatMul } from '../mat_mul';\r\nimport { op } from '../operation';\r\nimport { reshape } from '../reshape';\r\n/**\r\n * Computes the dot product of two matrices with optional activation and bias.\r\n *\r\n * ```js\r\n * const a = tf.tensor2d([-1, -2], [1, 2]);\r\n * const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);\r\n * const bias = tf.tensor2d([1, 2], [1, 2]);\r\n *\r\n * tf.fused.matMul({a, b, bias, activation: 'relu'}).print();\r\n * ```\r\n *\r\n * @param obj An object with the following properties:\r\n * - `a` First matrix in dot product operation.\r\n * - `b` Second matrix in dot product operation.\r\n * - `transposeA` If true, `a` is transposed before multiplication.\r\n * - `transposeB` If true, `b` is transposed before multiplication.\r\n * - `bias` Matrix to be added to the result.\r\n * - `activation` Name of activation kernel (defaults to `linear`).\r\n * - `preluActivationWeights` Tensor of prelu weights.\r\n */\r\nfunction fusedMatMul_({ a, b, transposeA = false, transposeB = false, bias, activation = 'linear', preluActivationWeights }) {\r\n    if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\r\n        let result = unfusedMatMul(a, b, transposeA, transposeB);\r\n        if (bias != null) {\r\n            result = add(result, bias);\r\n        }\r\n        return applyActivation(result, activation, preluActivationWeights);\r\n    }\r\n    let $a = convertToTensor(a, 'a', 'fused matMul');\r\n    let $b = convertToTensor(b, 'b', 'fused matMul');\r\n    [$a, $b] = makeTypesMatch($a, $b);\r\n    const innerShapeA = transposeA ? $a.shape[$a.rank - 2] : $a.shape[$a.rank - 1];\r\n    const innerShapeB = transposeB ? $b.shape[$b.rank - 1] : $b.shape[$b.rank - 2];\r\n    const outerShapeA = transposeA ? $a.shape[$a.rank - 1] : $a.shape[$a.rank - 2];\r\n    const outerShapeB = transposeB ? $b.shape[$b.rank - 2] : $b.shape[$b.rank - 1];\r\n    const outerDimsA = $a.shape.slice(0, -2);\r\n    const outerDimsB = $b.shape.slice(0, -2);\r\n    const batchDimA = util.sizeFromShape(outerDimsA);\r\n    const batchDimB = util.sizeFromShape(outerDimsB);\r\n    util.assert($a.rank >= 2 && $b.rank >= 2 && $a.rank === $b.rank, () => `Error in fused matMul: inputs must have the same rank of at least ` +\r\n        `2, got ranks ${$a.rank} and ${$b.rank}.`);\r\n    util.assert(util.arraysEqual(outerDimsA, outerDimsB), () => `Error in fused matMul: outer dimensions (${outerDimsA}) and (` +\r\n        `${outerDimsB}) of Tensors with shapes ${$a.shape} and ` +\r\n        `${$b.shape} must match.`);\r\n    util.assert(innerShapeA === innerShapeB, () => `Error in fused matMul: inner shapes (${innerShapeA}) and (` +\r\n        `${innerShapeB}) of Tensors with shapes ${$a.shape} and ` +\r\n        `${$b.shape} and transposeA=${transposeA}` +\r\n        ` and transposeB=${transposeB} must match.`);\r\n    const outShape = $a.shape.slice(0, -2).concat([outerShapeA, outerShapeB]);\r\n    const a3D = transposeA ?\r\n        reshape($a, [batchDimA, innerShapeA, outerShapeA]) :\r\n        reshape($a, [batchDimA, outerShapeA, innerShapeA]);\r\n    const b3D = transposeB ?\r\n        reshape($b, [batchDimB, outerShapeB, innerShapeB]) :\r\n        reshape($b, [batchDimB, innerShapeB, outerShapeB]);\r\n    let $bias;\r\n    if (bias != null) {\r\n        $bias = convertToTensor(bias, 'bias', 'fused matMul');\r\n        [$bias] = makeTypesMatch($bias, $a);\r\n        broadcast_util.assertAndGetBroadcastShape(outShape, $bias.shape);\r\n    }\r\n    let $preluActivationWeights;\r\n    if (preluActivationWeights != null) {\r\n        $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused matMul');\r\n    }\r\n    const grad = (dy, saved) => {\r\n        const [a3D, b3D, y, $bias] = saved;\r\n        // we reshape dy because the result of the forward is not\r\n        // necessarily going to be a 3d tensor due to a reshape done at the end of\r\n        // the customOp.\r\n        const dyActivation = getFusedDyActivation(reshape(dy, y.shape), y, activation);\r\n        let aDer;\r\n        let bDer;\r\n        if (!transposeA && !transposeB) {\r\n            aDer = unfusedMatMul(dyActivation, b3D, false, true);\r\n            bDer = unfusedMatMul(a3D, dyActivation, true, false);\r\n        }\r\n        else if (!transposeA && transposeB) {\r\n            aDer = unfusedMatMul(dyActivation, b3D, false, false);\r\n            bDer = unfusedMatMul(dyActivation, a3D, true, false);\r\n        }\r\n        else if (transposeA && !transposeB) {\r\n            aDer = unfusedMatMul(b3D, dyActivation, false, true);\r\n            bDer = unfusedMatMul(a3D, dyActivation, false, false);\r\n        }\r\n        else {\r\n            aDer = unfusedMatMul(b3D, dyActivation, true, true);\r\n            bDer = unfusedMatMul(dyActivation, a3D, true, true);\r\n        }\r\n        if (bias != null) {\r\n            const biasDer = getFusedBiasGradient($bias, dyActivation);\r\n            return [aDer, bDer, biasDer];\r\n        }\r\n        else {\r\n            return [aDer, bDer];\r\n        }\r\n    };\r\n    const forward = (backend) => {\r\n        const y = backend.fusedBatchMatMul({\r\n            a: a3D,\r\n            b: b3D,\r\n            transposeA,\r\n            transposeB,\r\n            bias: $bias,\r\n            activation,\r\n            preluActivationWeights: $preluActivationWeights\r\n        });\r\n        return y;\r\n    };\r\n    const inputs = {\r\n        a: a3D,\r\n        b: b3D,\r\n        bias: $bias,\r\n        preluActivationWeights: $preluActivationWeights\r\n    };\r\n    const attrs = { transposeA, transposeB, activation };\r\n    // Depending on the the params passed in we will have different number of\r\n    // inputs and thus a a different number of elements in the gradient.\r\n    if (bias == null) {\r\n        const customOp = customGrad((a3D, b3D, save) => {\r\n            const res = ENGINE.runKernelFunc(forward, inputs, null /* grad */, _FusedMatMul, attrs);\r\n            save([a3D, b3D, res]);\r\n            return { value: reshape(res, outShape), gradFunc: grad };\r\n        });\r\n        return customOp(a3D, b3D);\r\n    }\r\n    else {\r\n        const customOpWithBias = customGrad((a3D, b3D, $bias, save) => {\r\n            const res = ENGINE.runKernelFunc(forward, inputs, null /* grad */, _FusedMatMul, attrs);\r\n            save([a3D, b3D, res, $bias]);\r\n            return { value: reshape(res, outShape), gradFunc: grad };\r\n        });\r\n        return customOpWithBias(a3D, b3D, $bias);\r\n    }\r\n}\r\nexport const matMul = op({ fusedMatMul_ });\r\n//# sourceMappingURL=mat_mul.js.map"]},"metadata":{},"sourceType":"module"}