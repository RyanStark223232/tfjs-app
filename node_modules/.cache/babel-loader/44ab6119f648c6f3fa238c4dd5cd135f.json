{"ast":null,"code":"/**\r\n * @license\r\n * Copyright 2019 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\nimport { ENGINE } from '../../engine';\nimport { customGrad } from '../../gradients';\nimport { FusedConv2D } from '../../kernel_names';\nimport { makeTypesMatch } from '../../tensor_util';\nimport { convertToTensor } from '../../tensor_util_env';\nimport * as util from '../../util';\nimport { add } from '../add';\nimport * as broadcast_util from '../broadcast_util';\nimport { conv2d as unfusedConv2d } from '../conv2d';\nimport { conv2DBackpropFilter } from '../conv2d_backprop_filter';\nimport { conv2DBackpropInput } from '../conv2d_backprop_input';\nimport * as conv_util from '../conv_util';\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\nimport { op } from '../operation';\nimport { reshape } from '../reshape';\n/**\r\n * Computes a 2D convolution over the input x, optionally fused with adding a\r\n * bias and applying an activation.\r\n *\r\n * ```js\r\n * const inputDepth = 2;\r\n * const inShape = [2, 2, 2, inputDepth];\r\n * const outputDepth = 2;\r\n * const fSize = 1;\r\n * const pad = 0;\r\n * const strides = 1;\r\n *\r\n * const x = tf.tensor4d( [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\r\n * 16], inShape);\r\n * const w = tf.tensor4d([-1, 1, -2, 0.5], [fSize, fSize, inputDepth,\r\n * outputDepth]);\r\n *\r\n * tf.fused.conv2d({ x, filter: w, strides, pad, dataFormat: 'NHWC',\r\n * dilations: [1, 1], bias: tf.scalar(5), activation: 'relu' }).print();\r\n * ```\r\n *\r\n * @param obj An object with the following properties:\r\n * @param x The input tensor, of rank 4 or rank 3, of shape\r\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\r\n * assumed.\r\n * @param filter The filter, rank 4, of shape\r\n *     `[filterHeight, filterWidth, inDepth, outDepth]`.\r\n * @param strides The strides of the convolution: `[strideHeight,\r\n * strideWidth]`.\r\n * @param pad The type of padding algorithm.\r\n *   - `same` and stride 1: output will be of same size as input,\r\n *       regardless of filter size.\r\n *   - `valid` output will be smaller than input if filter is larger\r\n *       than 1x1.\r\n *   - For more info, see this guide:\r\n *     [https://www.tensorflow.org/api_guides/python/nn#Convolution](\r\n *          https://www.tensorflow.org/api_guides/python/nn#Convolution)\r\n * @param dataFormat An optional string from: \"NHWC\", \"NCHW\". Defaults to\r\n *     \"NHWC\". Specify the data format of the input and output data. With the\r\n *     default format \"NHWC\", the data is stored in the order of: [batch,\r\n *     height, width, channels]. Only \"NHWC\" is currently supported.\r\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\r\n *     in which we sample input values across the height and width dimensions\r\n *     in atrous convolution. Defaults to `[1, 1]`. If `dilations` is a single\r\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\r\n *     1, then all values of `strides` must be 1.\r\n * @param dimRoundingMode The rounding mode used when computing output\r\n *     dimensions if pad is a number. If none is provided, it will not round\r\n *     and error if the output is of fractional size.\r\n * @param bias Tensor to be added to the result.\r\n * @param activation Name of activation kernel (defaults to `linear`) to be\r\n *     applied\r\n *      after biasAdd.\r\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\r\n *     of a `prelu` activation, typically the same shape as `x`.\r\n */\n\nfunction fusedConv2d_({\n  x,\n  filter,\n  strides,\n  pad,\n  dataFormat = 'NHWC',\n  dilations = [1, 1],\n  dimRoundingMode,\n  bias,\n  activation = 'linear',\n  preluActivationWeights\n}) {\n  activation = activation || 'linear';\n\n  if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\n    let result = unfusedConv2d(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\n\n    if (bias != null) {\n      result = add(result, bias);\n    }\n\n    return applyActivation(result, activation, preluActivationWeights);\n  }\n\n  const $x = convertToTensor(x, 'x', 'conv2d');\n  const $filter = convertToTensor(filter, 'filter', 'conv2d');\n  let x4D = $x;\n  let reshapedTo4D = false;\n\n  if ($x.rank === 3) {\n    reshapedTo4D = true;\n    x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\n  }\n\n  util.assert(x4D.rank === 4, () => `Error in fused conv2d: input must be rank 4, but got rank ` + `${x4D.rank}.`);\n  util.assert($filter.rank === 4, () => `Error in fused conv2d: filter must be rank 4, but got rank ` + `${$filter.rank}.`);\n\n  if (dimRoundingMode != null) {\n    util.assert(util.isInt(pad), () => `Error in fused conv2d: pad must be an integer when using, ` + `dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\n  }\n\n  util.assert(x4D.shape[3] === $filter.shape[2], () => `Error in conv2d: depth of input (${x4D.shape[3]}) must match ` + `input depth for filter ${$filter.shape[2]}.`);\n  util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in conv2D: Either strides or dilations must be 1. ' + `Got strides ${strides} and dilations '${dilations}'`);\n  util.assert(dataFormat === 'NHWC', () => `Error in conv2d: got dataFormat of ${dataFormat} but only NHWC is currently supported.`);\n  const convInfo = conv_util.computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode);\n  let $bias;\n\n  if (bias != null) {\n    $bias = convertToTensor(bias, 'bias', 'fused conv2d');\n    [$bias] = makeTypesMatch($bias, $x);\n    broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\n  }\n\n  let $preluActivationWeights;\n\n  if (preluActivationWeights != null) {\n    $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused conv2d');\n  }\n\n  const grad = (dy, saved) => {\n    const [$filter, x4D, y, $bias] = saved;\n    const dyActivation = getFusedDyActivation(dy, y, activation);\n    util.assert(conv_util.tupleValuesAreOne(dilations), () => 'Error in gradient of fused conv2D: ' + `dilation rates greater than 1 ` + `are not yet supported in gradients. Got dilations '${dilations}'`);\n    const xDer = conv2DBackpropInput(x4D.shape, dyActivation, $filter, strides, pad);\n    const filterDer = conv2DBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad);\n    const der = [xDer, filterDer];\n\n    if ($bias != null) {\n      const biasDer = getFusedBiasGradient($bias, dyActivation);\n      der.push(biasDer);\n    }\n\n    return der;\n  };\n\n  const forward = backend => {\n    const res = backend.fusedConv2d({\n      input: x4D,\n      filter: $filter,\n      convInfo,\n      bias: $bias,\n      activation,\n      preluActivationWeights: $preluActivationWeights\n    });\n    return res;\n  };\n\n  const inputs = {\n    x: x4D,\n    filter: $filter,\n    bias: $bias,\n    preluActivationWeights: $preluActivationWeights\n  };\n  const attrs = {\n    strides,\n    pad,\n    dataFormat,\n    dilations,\n    dimRoundingMode,\n    activation\n  }; // Depending on the the params passed in we will have different number of\n  // inputs and thus a a different number of elements in the gradient.\n\n  if (bias == null) {\n    const customOp = customGrad((x4D, filter, save) => {\n      let res = ENGINE.runKernelFunc(forward, inputs, null\n      /* grad */\n      , FusedConv2D, attrs);\n      save([filter, x4D, res]);\n\n      if (reshapedTo4D) {\n        res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n      }\n\n      return {\n        value: res,\n        gradFunc: grad\n      };\n    });\n    return customOp(x4D, $filter);\n  } else {\n    const customOpWithBias = customGrad((x4D, filter, bias, save) => {\n      let res = ENGINE.runKernelFunc(forward, inputs, null\n      /* grad */\n      , FusedConv2D, attrs);\n      save([filter, x4D, res, bias]);\n\n      if (reshapedTo4D) {\n        res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\n      }\n\n      return {\n        value: res,\n        gradFunc: grad\n      };\n    });\n    return customOpWithBias(x4D, $filter, $bias);\n  }\n}\n\nexport const conv2d = op({\n  fusedConv2d_\n});","map":{"version":3,"sources":["../../../src/ops/fused/conv2d.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,MAAR,QAAkC,cAAlC;AACA,SAAQ,UAAR,QAAyB,iBAAzB;AACA,SAAQ,WAAR,QAA+D,oBAA/D;AAIA,SAAQ,cAAR,QAA6B,mBAA7B;AACA,SAAQ,eAAR,QAA8B,uBAA9B;AAEA,OAAO,KAAK,IAAZ,MAAsB,YAAtB;AACA,SAAQ,GAAR,QAAkB,QAAlB;AACA,OAAO,KAAK,cAAZ,MAAgC,mBAAhC;AACA,SAAQ,MAAM,IAAI,aAAlB,QAAsC,WAAtC;AACA,SAAQ,oBAAR,QAAmC,2BAAnC;AACA,SAAQ,mBAAR,QAAkC,0BAAlC;AACA,OAAO,KAAK,SAAZ,MAA2B,cAA3B;AAEA,SAAQ,eAAR,EAAyB,oBAAzB,EAA+C,oBAA/C,EAAqE,UAArE,QAAsF,eAAtF;AACA,SAAQ,EAAR,QAAiB,cAAjB;AACA,SAAQ,OAAR,QAAsB,YAAtB;AAEA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAuDG;;AACH,SAAS,YAAT,CAAmD;AACjD,EAAA,CADiD;AAEjD,EAAA,MAFiD;AAGjD,EAAA,OAHiD;AAIjD,EAAA,GAJiD;AAKjD,EAAA,UAAU,GAAG,MALoC;AAMjD,EAAA,SAAS,GAAG,CAAC,CAAD,EAAI,CAAJ,CANqC;AAOjD,EAAA,eAPiD;AAQjD,EAAA,IARiD;AASjD,EAAA,UAAU,GAAG,QAToC;AAUjD,EAAA;AAViD,CAAnD,EAsBC;AACC,EAAA,UAAU,GAAG,UAAU,IAAI,QAA3B;;AAEA,MAAI,UAAU,CAAC,MAAM,CAAC,KAAP,CAAa,aAAd,EAA6B,UAA7B,CAAV,KAAuD,KAA3D,EAAkE;AAChE,QAAI,MAAM,GAAG,aAAa,CACtB,CADsB,EACnB,MADmB,EACX,OADW,EACF,GADE,EACG,UADH,EACe,SADf,EAC0B,eAD1B,CAA1B;;AAEA,QAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,MAAA,MAAM,GAAG,GAAG,CAAC,MAAD,EAAS,IAAT,CAAZ;AACD;;AAED,WAAO,eAAe,CAAC,MAAD,EAAS,UAAT,EAAqB,sBAArB,CAAtB;AACD;;AAED,QAAM,EAAE,GAAG,eAAe,CAAC,CAAD,EAAI,GAAJ,EAAS,QAAT,CAA1B;AACA,QAAM,OAAO,GAAG,eAAe,CAAC,MAAD,EAAS,QAAT,EAAmB,QAAnB,CAA/B;AAEA,MAAI,GAAG,GAAG,EAAV;AACA,MAAI,YAAY,GAAG,KAAnB;;AAEA,MAAI,EAAE,CAAC,IAAH,KAAY,CAAhB,EAAmB;AACjB,IAAA,YAAY,GAAG,IAAf;AACA,IAAA,GAAG,GAAG,OAAO,CAAC,EAAD,EAAK,CAAC,CAAD,EAAI,EAAE,CAAC,KAAH,CAAS,CAAT,CAAJ,EAAiB,EAAE,CAAC,KAAH,CAAS,CAAT,CAAjB,EAA8B,EAAE,CAAC,KAAH,CAAS,CAAT,CAA9B,CAAL,CAAb;AACD;;AACD,EAAA,IAAI,CAAC,MAAL,CACI,GAAG,CAAC,IAAJ,KAAa,CADjB,EAEI,MAAM,4DAAA,GACF,GAAG,GAAG,CAAC,IAAI,GAHnB;AAIA,EAAA,IAAI,CAAC,MAAL,CACI,OAAO,CAAC,IAAR,KAAiB,CADrB,EAEI,MAAM,6DAAA,GACF,GAAG,OAAO,CAAC,IAAI,GAHvB;;AAIA,MAAI,eAAe,IAAI,IAAvB,EAA6B;AAC3B,IAAA,IAAI,CAAC,MAAL,CACI,IAAI,CAAC,KAAL,CAAW,GAAX,CADJ,EAEI,MAAM,4DAAA,GACF,mBAAmB,eAAe,gBAAgB,GAAG,GAH7D;AAID;;AAED,EAAA,IAAI,CAAC,MAAL,CACI,GAAG,CAAC,KAAJ,CAAU,CAAV,MAAiB,OAAO,CAAC,KAAR,CAAc,CAAd,CADrB,EAEI,MAAM,oCAAoC,GAAG,CAAC,KAAJ,CAAU,CAAV,CAAY,eAAhD,GACF,0BAA0B,OAAO,CAAC,KAAR,CAAc,CAAd,CAAgB,GAHlD;AAIA,EAAA,IAAI,CAAC,MAAL,CACI,SAAS,CAAC,8BAAV,CAAyC,OAAzC,EAAkD,SAAlD,CADJ,EAEI,MAAM,6DACF,eAAe,OAAO,mBAAmB,SAAS,GAH1D;AAIA,EAAA,IAAI,CAAC,MAAL,CACI,UAAU,KAAK,MADnB,EAEI,MAAM,sCACF,UAAU,wCAHlB;AAKA,QAAM,QAAQ,GAAG,SAAS,CAAC,iBAAV,CACb,GAAG,CAAC,KADS,EACF,OAAO,CAAC,KADN,EACa,OADb,EACsB,SADtB,EACiC,GADjC,EACsC,eADtC,CAAjB;AAGA,MAAI,KAAJ;;AACA,MAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,IAAA,KAAK,GAAG,eAAe,CAAC,IAAD,EAAO,MAAP,EAAe,cAAf,CAAvB;AACA,KAAC,KAAD,IAAU,cAAc,CAAC,KAAD,EAAQ,EAAR,CAAxB;AAEA,IAAA,cAAc,CAAC,0BAAf,CAA0C,QAAQ,CAAC,QAAnD,EAA6D,KAAK,CAAC,KAAnE;AACD;;AAED,MAAI,uBAAJ;;AACA,MAAI,sBAAsB,IAAI,IAA9B,EAAoC;AAClC,IAAA,uBAAuB,GAAG,eAAe,CACrC,sBADqC,EACb,eADa,EACI,cADJ,CAAzC;AAED;;AAED,QAAM,IAAI,GAAG,CAAC,EAAD,EAAe,KAAf,KAAkC;AAC7C,UAAM,CAAC,OAAD,EAAU,GAAV,EAAe,CAAf,EAAkB,KAAlB,IACF,KADJ;AAGA,UAAM,YAAY,GAAG,oBAAoB,CAAC,EAAD,EAAK,CAAL,EAAQ,UAAR,CAAzC;AAEA,IAAA,IAAI,CAAC,MAAL,CACI,SAAS,CAAC,iBAAV,CAA4B,SAA5B,CADJ,EAEI,MAAM,wCACF,gCADE,GAEF,sDAAsD,SAAS,GAJvE;AAMA,UAAM,IAAI,GACN,mBAAmB,CAAC,GAAG,CAAC,KAAL,EAAY,YAAZ,EAA0B,OAA1B,EAAmC,OAAnC,EAA4C,GAA5C,CADvB;AAEA,UAAM,SAAS,GACX,oBAAoB,CAAC,GAAD,EAAM,YAAN,EAAoB,OAAO,CAAC,KAA5B,EAAmC,OAAnC,EAA4C,GAA5C,CADxB;AAEA,UAAM,GAAG,GAAa,CAAC,IAAD,EAAO,SAAP,CAAtB;;AAEA,QAAI,KAAK,IAAI,IAAb,EAAmB;AACjB,YAAM,OAAO,GAAG,oBAAoB,CAAC,KAAD,EAAQ,YAAR,CAApC;AACA,MAAA,GAAG,CAAC,IAAJ,CAAS,OAAT;AACD;;AACD,WAAO,GAAP;AACD,GAvBD;;AAyBA,QAAM,OAAO,GAAyB,OAAD,IAAY;AAC/C,UAAM,GAAG,GAAG,OAAO,CAAC,WAAR,CAAoB;AAC9B,MAAA,KAAK,EAAE,GADuB;AAE9B,MAAA,MAAM,EAAE,OAFsB;AAG9B,MAAA,QAH8B;AAI9B,MAAA,IAAI,EAAE,KAJwB;AAK9B,MAAA,UAL8B;AAM9B,MAAA,sBAAsB,EAAE;AANM,KAApB,CAAZ;AAQA,WAAO,GAAP;AACD,GAVD;;AAYA,QAAM,MAAM,GAAsB;AAChC,IAAA,CAAC,EAAE,GAD6B;AAEhC,IAAA,MAAM,EAAE,OAFwB;AAGhC,IAAA,IAAI,EAAE,KAH0B;AAIhC,IAAA,sBAAsB,EAAE;AAJQ,GAAlC;AAOA,QAAM,KAAK,GACP;AAAC,IAAA,OAAD;AAAU,IAAA,GAAV;AAAe,IAAA,UAAf;AAA2B,IAAA,SAA3B;AAAsC,IAAA,eAAtC;AAAuD,IAAA;AAAvD,GADJ,CAhHD,CAmHC;AACA;;AACA,MAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,UAAM,QAAQ,GACV,UAAU,CAAC,CAAC,GAAD,EAAgB,MAAhB,EAAkC,IAAlC,KAAwD;AACjE,UAAI,GAAG,GAAG,MAAM,CAAC,aAAP,CACN,OADM,EACG,MADH,EACmC;AAAK;AADxC,QAEN,WAFM,EAEO,KAFP,CAAV;AAIA,MAAA,IAAI,CAAC,CAAC,MAAD,EAAS,GAAT,EAAc,GAAd,CAAD,CAAJ;;AAEA,UAAI,YAAJ,EAAkB;AAChB,QAAA,GAAG,GAAG,OAAO,CAAC,GAAD,EAAM,CAAC,GAAG,CAAC,KAAJ,CAAU,CAAV,CAAD,EAAe,GAAG,CAAC,KAAJ,CAAU,CAAV,CAAf,EAA6B,GAAG,CAAC,KAAJ,CAAU,CAAV,CAA7B,CAAN,CAAb;AACD;;AAED,aAAO;AAAC,QAAA,KAAK,EAAE,GAAR;AAAa,QAAA,QAAQ,EAAE;AAAvB,OAAP;AACD,KAZS,CADd;AAcA,WAAO,QAAQ,CAAC,GAAD,EAAM,OAAN,CAAf;AACD,GAhBD,MAgBO;AACL,UAAM,gBAAgB,GAAG,UAAU,CAC/B,CAAC,GAAD,EAAgB,MAAhB,EAAkC,IAAlC,EAAgD,IAAhD,KAAsE;AACpE,UAAI,GAAG,GAAG,MAAM,CAAC,aAAP,CACN,OADM,EACG,MADH,EACmC;AAAK;AADxC,QAEN,WAFM,EAEO,KAFP,CAAV;AAIA,MAAA,IAAI,CAAC,CAAC,MAAD,EAAS,GAAT,EAAc,GAAd,EAAmB,IAAnB,CAAD,CAAJ;;AAEA,UAAI,YAAJ,EAAkB;AAChB,QAAA,GAAG,GAAG,OAAO,CAAC,GAAD,EAAM,CAAC,GAAG,CAAC,KAAJ,CAAU,CAAV,CAAD,EAAe,GAAG,CAAC,KAAJ,CAAU,CAAV,CAAf,EAA6B,GAAG,CAAC,KAAJ,CAAU,CAAV,CAA7B,CAAN,CAAb;AACD;;AAED,aAAO;AAAC,QAAA,KAAK,EAAE,GAAR;AAAa,QAAA,QAAQ,EAAE;AAAvB,OAAP;AACD,KAb8B,CAAnC;AAeA,WAAO,gBAAgB,CAAC,GAAD,EAAM,OAAN,EAAe,KAAf,CAAvB;AACD;AACF;;AACD,OAAO,MAAM,MAAM,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAAjB","sourceRoot":"","sourcesContent":["/**\r\n * @license\r\n * Copyright 2019 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\r\nimport { ENGINE } from '../../engine';\r\nimport { customGrad } from '../../gradients';\r\nimport { FusedConv2D } from '../../kernel_names';\r\nimport { makeTypesMatch } from '../../tensor_util';\r\nimport { convertToTensor } from '../../tensor_util_env';\r\nimport * as util from '../../util';\r\nimport { add } from '../add';\r\nimport * as broadcast_util from '../broadcast_util';\r\nimport { conv2d as unfusedConv2d } from '../conv2d';\r\nimport { conv2DBackpropFilter } from '../conv2d_backprop_filter';\r\nimport { conv2DBackpropInput } from '../conv2d_backprop_input';\r\nimport * as conv_util from '../conv_util';\r\nimport { applyActivation, getFusedBiasGradient, getFusedDyActivation, shouldFuse } from '../fused_util';\r\nimport { op } from '../operation';\r\nimport { reshape } from '../reshape';\r\n/**\r\n * Computes a 2D convolution over the input x, optionally fused with adding a\r\n * bias and applying an activation.\r\n *\r\n * ```js\r\n * const inputDepth = 2;\r\n * const inShape = [2, 2, 2, inputDepth];\r\n * const outputDepth = 2;\r\n * const fSize = 1;\r\n * const pad = 0;\r\n * const strides = 1;\r\n *\r\n * const x = tf.tensor4d( [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,\r\n * 16], inShape);\r\n * const w = tf.tensor4d([-1, 1, -2, 0.5], [fSize, fSize, inputDepth,\r\n * outputDepth]);\r\n *\r\n * tf.fused.conv2d({ x, filter: w, strides, pad, dataFormat: 'NHWC',\r\n * dilations: [1, 1], bias: tf.scalar(5), activation: 'relu' }).print();\r\n * ```\r\n *\r\n * @param obj An object with the following properties:\r\n * @param x The input tensor, of rank 4 or rank 3, of shape\r\n *     `[batch, height, width, inChannels]`. If rank 3, batch of 1 is\r\n * assumed.\r\n * @param filter The filter, rank 4, of shape\r\n *     `[filterHeight, filterWidth, inDepth, outDepth]`.\r\n * @param strides The strides of the convolution: `[strideHeight,\r\n * strideWidth]`.\r\n * @param pad The type of padding algorithm.\r\n *   - `same` and stride 1: output will be of same size as input,\r\n *       regardless of filter size.\r\n *   - `valid` output will be smaller than input if filter is larger\r\n *       than 1x1.\r\n *   - For more info, see this guide:\r\n *     [https://www.tensorflow.org/api_guides/python/nn#Convolution](\r\n *          https://www.tensorflow.org/api_guides/python/nn#Convolution)\r\n * @param dataFormat An optional string from: \"NHWC\", \"NCHW\". Defaults to\r\n *     \"NHWC\". Specify the data format of the input and output data. With the\r\n *     default format \"NHWC\", the data is stored in the order of: [batch,\r\n *     height, width, channels]. Only \"NHWC\" is currently supported.\r\n * @param dilations The dilation rates: `[dilationHeight, dilationWidth]`\r\n *     in which we sample input values across the height and width dimensions\r\n *     in atrous convolution. Defaults to `[1, 1]`. If `dilations` is a single\r\n *     number, then `dilationHeight == dilationWidth`. If it is greater than\r\n *     1, then all values of `strides` must be 1.\r\n * @param dimRoundingMode The rounding mode used when computing output\r\n *     dimensions if pad is a number. If none is provided, it will not round\r\n *     and error if the output is of fractional size.\r\n * @param bias Tensor to be added to the result.\r\n * @param activation Name of activation kernel (defaults to `linear`) to be\r\n *     applied\r\n *      after biasAdd.\r\n * @param preluActivationWeights Tensor of prelu weights to be applied as part\r\n *     of a `prelu` activation, typically the same shape as `x`.\r\n */\r\nfunction fusedConv2d_({ x, filter, strides, pad, dataFormat = 'NHWC', dilations = [1, 1], dimRoundingMode, bias, activation = 'linear', preluActivationWeights }) {\r\n    activation = activation || 'linear';\r\n    if (shouldFuse(ENGINE.state.gradientDepth, activation) === false) {\r\n        let result = unfusedConv2d(x, filter, strides, pad, dataFormat, dilations, dimRoundingMode);\r\n        if (bias != null) {\r\n            result = add(result, bias);\r\n        }\r\n        return applyActivation(result, activation, preluActivationWeights);\r\n    }\r\n    const $x = convertToTensor(x, 'x', 'conv2d');\r\n    const $filter = convertToTensor(filter, 'filter', 'conv2d');\r\n    let x4D = $x;\r\n    let reshapedTo4D = false;\r\n    if ($x.rank === 3) {\r\n        reshapedTo4D = true;\r\n        x4D = reshape($x, [1, $x.shape[0], $x.shape[1], $x.shape[2]]);\r\n    }\r\n    util.assert(x4D.rank === 4, () => `Error in fused conv2d: input must be rank 4, but got rank ` +\r\n        `${x4D.rank}.`);\r\n    util.assert($filter.rank === 4, () => `Error in fused conv2d: filter must be rank 4, but got rank ` +\r\n        `${$filter.rank}.`);\r\n    if (dimRoundingMode != null) {\r\n        util.assert(util.isInt(pad), () => `Error in fused conv2d: pad must be an integer when using, ` +\r\n            `dimRoundingMode ${dimRoundingMode} but got pad ${pad}.`);\r\n    }\r\n    util.assert(x4D.shape[3] === $filter.shape[2], () => `Error in conv2d: depth of input (${x4D.shape[3]}) must match ` +\r\n        `input depth for filter ${$filter.shape[2]}.`);\r\n    util.assert(conv_util.eitherStridesOrDilationsAreOne(strides, dilations), () => 'Error in conv2D: Either strides or dilations must be 1. ' +\r\n        `Got strides ${strides} and dilations '${dilations}'`);\r\n    util.assert(dataFormat === 'NHWC', () => `Error in conv2d: got dataFormat of ${dataFormat} but only NHWC is currently supported.`);\r\n    const convInfo = conv_util.computeConv2DInfo(x4D.shape, $filter.shape, strides, dilations, pad, dimRoundingMode);\r\n    let $bias;\r\n    if (bias != null) {\r\n        $bias = convertToTensor(bias, 'bias', 'fused conv2d');\r\n        [$bias] = makeTypesMatch($bias, $x);\r\n        broadcast_util.assertAndGetBroadcastShape(convInfo.outShape, $bias.shape);\r\n    }\r\n    let $preluActivationWeights;\r\n    if (preluActivationWeights != null) {\r\n        $preluActivationWeights = convertToTensor(preluActivationWeights, 'prelu weights', 'fused conv2d');\r\n    }\r\n    const grad = (dy, saved) => {\r\n        const [$filter, x4D, y, $bias] = saved;\r\n        const dyActivation = getFusedDyActivation(dy, y, activation);\r\n        util.assert(conv_util.tupleValuesAreOne(dilations), () => 'Error in gradient of fused conv2D: ' +\r\n            `dilation rates greater than 1 ` +\r\n            `are not yet supported in gradients. Got dilations '${dilations}'`);\r\n        const xDer = conv2DBackpropInput(x4D.shape, dyActivation, $filter, strides, pad);\r\n        const filterDer = conv2DBackpropFilter(x4D, dyActivation, $filter.shape, strides, pad);\r\n        const der = [xDer, filterDer];\r\n        if ($bias != null) {\r\n            const biasDer = getFusedBiasGradient($bias, dyActivation);\r\n            der.push(biasDer);\r\n        }\r\n        return der;\r\n    };\r\n    const forward = (backend) => {\r\n        const res = backend.fusedConv2d({\r\n            input: x4D,\r\n            filter: $filter,\r\n            convInfo,\r\n            bias: $bias,\r\n            activation,\r\n            preluActivationWeights: $preluActivationWeights\r\n        });\r\n        return res;\r\n    };\r\n    const inputs = {\r\n        x: x4D,\r\n        filter: $filter,\r\n        bias: $bias,\r\n        preluActivationWeights: $preluActivationWeights\r\n    };\r\n    const attrs = { strides, pad, dataFormat, dilations, dimRoundingMode, activation };\r\n    // Depending on the the params passed in we will have different number of\r\n    // inputs and thus a a different number of elements in the gradient.\r\n    if (bias == null) {\r\n        const customOp = customGrad((x4D, filter, save) => {\r\n            let res = ENGINE.runKernelFunc(forward, inputs, null /* grad */, FusedConv2D, attrs);\r\n            save([filter, x4D, res]);\r\n            if (reshapedTo4D) {\r\n                res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\r\n            }\r\n            return { value: res, gradFunc: grad };\r\n        });\r\n        return customOp(x4D, $filter);\r\n    }\r\n    else {\r\n        const customOpWithBias = customGrad((x4D, filter, bias, save) => {\r\n            let res = ENGINE.runKernelFunc(forward, inputs, null /* grad */, FusedConv2D, attrs);\r\n            save([filter, x4D, res, bias]);\r\n            if (reshapedTo4D) {\r\n                res = reshape(res, [res.shape[1], res.shape[2], res.shape[3]]);\r\n            }\r\n            return { value: res, gradFunc: grad };\r\n        });\r\n        return customOpWithBias(x4D, $filter, $bias);\r\n    }\r\n}\r\nexport const conv2d = op({ fusedConv2d_ });\r\n//# sourceMappingURL=conv2d.js.map"]},"metadata":{},"sourceType":"module"}