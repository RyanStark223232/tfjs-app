{"ast":null,"code":"import _slicedToArray from \"C:/Users/wongh/Documents/GitHub/tfjs-app/node_modules/@babel/runtime/helpers/esm/slicedToArray\";\n\n/**\r\n * @license\r\n * Copyright 2020 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\nimport { LogSoftmax } from '../kernel_names';\nimport { exp } from '../ops/exp';\nimport { mul } from '../ops/mul';\nimport { sub } from '../ops/sub';\nimport { sum } from '../ops/sum';\nexport var logSoftmaxGradConfig = {\n  kernelName: LogSoftmax,\n  inputsToSave: [],\n  outputsToSave: [true],\n  gradFunc: function gradFunc(dy, saved, attrs) {\n    var _saved = _slicedToArray(saved, 1),\n        value = _saved[0];\n\n    var axis = attrs.axis;\n    return {\n      logits: function logits() {\n        var keepDims = true;\n        var softmax = exp(value);\n        return sub(dy, mul(sum(dy, axis, keepDims), softmax));\n      }\n    };\n  }\n};","map":{"version":3,"sources":["../../src/gradients/LogSoftmax_grad.ts"],"names":[],"mappings":";;AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,UAAR,QAA0C,iBAA1C;AAEA,SAAQ,GAAR,QAAkB,YAAlB;AACA,SAAQ,GAAR,QAAkB,YAAlB;AACA,SAAQ,GAAR,QAAkB,YAAlB;AACA,SAAQ,GAAR,QAAkB,YAAlB;AAGA,OAAO,IAAM,oBAAoB,GAAe;AAC9C,EAAA,UAAU,EAAE,UADkC;AAE9C,EAAA,YAAY,EAAE,EAFgC;AAG9C,EAAA,aAAa,EAAE,CAAC,IAAD,CAH+B;AAI9C,EAAA,QAAQ,EAAE,kBAAC,EAAD,EAAa,KAAb,EAA8B,KAA9B,EAAqD;AAAA,gCAC7C,KAD6C;AAAA,QACtD,KADsD;;AAAA,QAEtD,IAFsD,GAE9C,KAF8C,CAEtD,IAFsD;AAG7D,WAAO;AACL,MAAA,MAAM,EAAE,kBAAK;AACX,YAAM,QAAQ,GAAG,IAAjB;AACA,YAAM,OAAO,GAAG,GAAG,CAAC,KAAD,CAAnB;AACA,eAAO,GAAG,CAAC,EAAD,EAAK,GAAG,CAAC,GAAG,CAAC,EAAD,EAAK,IAAL,EAAW,QAAX,CAAJ,EAA0B,OAA1B,CAAR,CAAV;AACD;AALI,KAAP;AAOD;AAd6C,CAAzC","sourceRoot":"","sourcesContent":["/**\r\n * @license\r\n * Copyright 2020 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\r\nimport { LogSoftmax } from '../kernel_names';\r\nimport { exp } from '../ops/exp';\r\nimport { mul } from '../ops/mul';\r\nimport { sub } from '../ops/sub';\r\nimport { sum } from '../ops/sum';\r\nexport const logSoftmaxGradConfig = {\r\n    kernelName: LogSoftmax,\r\n    inputsToSave: [],\r\n    outputsToSave: [true],\r\n    gradFunc: (dy, saved, attrs) => {\r\n        const [value] = saved;\r\n        const { axis } = attrs;\r\n        return {\r\n            logits: () => {\r\n                const keepDims = true;\r\n                const softmax = exp(value);\r\n                return sub(dy, mul(sum(dy, axis, keepDims), softmax));\r\n            }\r\n        };\r\n    }\r\n};\r\n//# sourceMappingURL=LogSoftmax_grad.js.map"]},"metadata":{},"sourceType":"module"}