{"ast":null,"code":"/**\r\n * @license\r\n * Copyright 2020 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\nimport { convertToTensor } from '../../tensor_util_env';\nimport { assertShapesMatch } from '../../util';\nimport { abs } from '../abs';\nimport { add } from '../add';\nimport { exp } from '../exp';\nimport { log1p } from '../log1p';\nimport { Reduction } from '../loss_ops_utils';\nimport { mul } from '../mul';\nimport { neg } from '../neg';\nimport { op } from '../operation';\nimport { relu } from '../relu';\nimport { scalar } from '../scalar';\nimport { sub } from '../sub';\nimport { computeWeightedLoss } from './compute_weighted_loss';\n\nfunction sigmoidCrossEntropyWithLogits_(labels, logits) {\n  const $labels = convertToTensor(labels, 'labels', 'sigmoidCrossEntropyWithLogits');\n  const $logits = convertToTensor(logits, 'logits', 'sigmoidCrossEntropyWithLogits');\n  assertShapesMatch($labels.shape, $logits.shape, 'Error in sigmoidCrossEntropyWithLogits: ');\n  /**\r\n   * Implementation Details:\r\n   *\r\n   * For brevity, let `x = logits`, `z = labels`.  The logistic loss is\r\n   *     z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\r\n   *   = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\r\n   *   = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\r\n   *   = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\r\n   *   = (1 - z) * x + log(1 + exp(-x))\r\n   *   = x - x * z + log(1 + exp(-x))\r\n   *\r\n   *   For x < 0, to avoid overflow in exp(-x), we reformulate the above\r\n   *     x - x * z + log(1 + exp(-x))\r\n   *   = log(exp(x)) - x * z + log(1 + exp(-x))\r\n   *   = - x * z + log(1 + exp(x))\r\n   *\r\n   * Hence, to ensure stability and avoid overflow, the implementation uses\r\n   * this equivalent formulation:\r\n   *     max(x, 0) - x * z + log(1 + exp(-abs(x)))\r\n   */\n\n  const maxOutput = relu($logits);\n  const outputXTarget = mul($logits, $labels);\n  const sigmoidOutput = log1p(exp(neg(abs($logits))));\n  return add(sub(maxOutput, outputXTarget), sigmoidOutput);\n}\n/**\r\n * Computes the sigmoid cross entropy loss between two tensors.\r\n *\r\n * If labelSmoothing is nonzero, smooth the labels towards 1/2:\r\n *\r\n *   newMulticlassLabels = multiclassLabels * (1 - labelSmoothing)\r\n *                         + 0.5 * labelSmoothing\r\n *\r\n * @param multiClassLabels The ground truth output tensor of shape\r\n * [batch_size, num_classes], same dimensions as 'predictions'.\r\n * @param logits The predicted outputs.\r\n * @param weights Tensor whose rank is either 0, or the same rank as\r\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\r\n *    must be either `1`, or the same as the corresponding `losses`\r\n *    dimension).\r\n * @param labelSmoothing If greater than 0, then smooth the labels.\r\n * @param reduction Type of reduction to apply to loss. Should be of type\r\n *    `Reduction`\r\n *\r\n * @doc { heading: 'Training', subheading: 'Losses', namespace: 'losses' }\r\n */\n\n\nfunction sigmoidCrossEntropy_(multiClassLabels, logits, weights, labelSmoothing = 0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\n  let $multiClassLabels = convertToTensor(multiClassLabels, 'multiClassLabels', 'sigmoidCrossEntropy');\n  const $logits = convertToTensor(logits, 'logits', 'sigmoidCrossEntropy');\n  let $weights = null;\n\n  if (weights != null) {\n    $weights = convertToTensor(weights, 'weights', 'sigmoidCrossEntropy');\n  }\n\n  assertShapesMatch($multiClassLabels.shape, $logits.shape, 'Error in sigmoidCrossEntropy: ');\n\n  if (labelSmoothing > 0) {\n    const labelSmoothingScalar = scalar(labelSmoothing);\n    const one = scalar(1);\n    const half = scalar(0.5);\n    $multiClassLabels = add(mul($multiClassLabels, sub(one, labelSmoothingScalar)), mul(half, labelSmoothingScalar));\n  }\n\n  const losses = sigmoidCrossEntropyWithLogits_($multiClassLabels, $logits);\n  return computeWeightedLoss(losses, $weights, reduction);\n}\n\nexport const sigmoidCrossEntropy = op({\n  sigmoidCrossEntropy_\n});","map":{"version":3,"sources":["../../../src/ops/losses/sigmoid_cross_entropy.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;AAeG;AAGH,SAAQ,eAAR,QAA8B,uBAA9B;AAEA,SAAQ,iBAAR,QAAgC,YAAhC;AACA,SAAQ,GAAR,QAAkB,QAAlB;AACA,SAAQ,GAAR,QAAkB,QAAlB;AACA,SAAQ,GAAR,QAAkB,QAAlB;AACA,SAAQ,KAAR,QAAoB,UAApB;AACA,SAAQ,SAAR,QAAwB,mBAAxB;AACA,SAAQ,GAAR,QAAkB,QAAlB;AACA,SAAQ,GAAR,QAAkB,QAAlB;AACA,SAAQ,EAAR,QAAiB,cAAjB;AACA,SAAQ,IAAR,QAAmB,SAAnB;AACA,SAAQ,MAAR,QAAqB,WAArB;AACA,SAAQ,GAAR,QAAkB,QAAlB;AAEA,SAAQ,mBAAR,QAAkC,yBAAlC;;AAEA,SAAS,8BAAT,CACI,MADJ,EAC0B,MAD1B,EAC8C;AAC5C,QAAM,OAAO,GACT,eAAe,CAAC,MAAD,EAAS,QAAT,EAAmB,+BAAnB,CADnB;AAEA,QAAM,OAAO,GACT,eAAe,CAAC,MAAD,EAAS,QAAT,EAAmB,+BAAnB,CADnB;AAEA,EAAA,iBAAiB,CACb,OAAO,CAAC,KADK,EACE,OAAO,CAAC,KADV,EACiB,0CADjB,CAAjB;AAGA;;;;;;;;;;;;;;;;;;;AAmBG;;AACH,QAAM,SAAS,GAAG,IAAI,CAAC,OAAD,CAAtB;AACA,QAAM,aAAa,GAAG,GAAG,CAAC,OAAD,EAAU,OAAV,CAAzB;AACA,QAAM,aAAa,GAAG,KAAK,CAAC,GAAG,CAAC,GAAG,CAAC,GAAG,CAAC,OAAD,CAAJ,CAAJ,CAAJ,CAA3B;AAEA,SAAO,GAAG,CAAC,GAAG,CAAC,SAAD,EAAY,aAAZ,CAAJ,EAAgC,aAAhC,CAAV;AACD;AAED;;;;;;;;;;;;;;;;;;;;AAoBG;;;AACH,SAAS,oBAAT,CACI,gBADJ,EACoC,MADpC,EAEI,OAFJ,EAEiC,cAAc,GAAG,CAFlD,EAGI,SAAS,GAAG,SAAS,CAAC,sBAH1B,EAGgD;AAC9C,MAAI,iBAAiB,GAAG,eAAe,CACnC,gBADmC,EACjB,kBADiB,EACG,qBADH,CAAvC;AAEA,QAAM,OAAO,GAAG,eAAe,CAAC,MAAD,EAAS,QAAT,EAAmB,qBAAnB,CAA/B;AACA,MAAI,QAAQ,GAAW,IAAvB;;AACA,MAAI,OAAO,IAAI,IAAf,EAAqB;AACnB,IAAA,QAAQ,GAAG,eAAe,CAAC,OAAD,EAAU,SAAV,EAAqB,qBAArB,CAA1B;AACD;;AACD,EAAA,iBAAiB,CACb,iBAAiB,CAAC,KADL,EACY,OAAO,CAAC,KADpB,EAC2B,gCAD3B,CAAjB;;AAGA,MAAI,cAAc,GAAG,CAArB,EAAwB;AACtB,UAAM,oBAAoB,GAAG,MAAM,CAAC,cAAD,CAAnC;AACA,UAAM,GAAG,GAAG,MAAM,CAAC,CAAD,CAAlB;AACA,UAAM,IAAI,GAAG,MAAM,CAAC,GAAD,CAAnB;AAEA,IAAA,iBAAiB,GACb,GAAG,CAAC,GAAG,CAAC,iBAAD,EAAoB,GAAG,CAAC,GAAD,EAAM,oBAAN,CAAvB,CAAJ,EACC,GAAG,CAAC,IAAD,EAAO,oBAAP,CADJ,CADP;AAGD;;AACD,QAAM,MAAM,GAAG,8BAA8B,CAAC,iBAAD,EAAoB,OAApB,CAA7C;AAEA,SAAO,mBAAmB,CAAC,MAAD,EAAS,QAAT,EAAmB,SAAnB,CAA1B;AACD;;AAED,OAAO,MAAM,mBAAmB,GAAG,EAAE,CAAC;AAAC,EAAA;AAAD,CAAD,CAA9B","sourceRoot":"","sourcesContent":["/**\r\n * @license\r\n * Copyright 2020 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\r\nimport { convertToTensor } from '../../tensor_util_env';\r\nimport { assertShapesMatch } from '../../util';\r\nimport { abs } from '../abs';\r\nimport { add } from '../add';\r\nimport { exp } from '../exp';\r\nimport { log1p } from '../log1p';\r\nimport { Reduction } from '../loss_ops_utils';\r\nimport { mul } from '../mul';\r\nimport { neg } from '../neg';\r\nimport { op } from '../operation';\r\nimport { relu } from '../relu';\r\nimport { scalar } from '../scalar';\r\nimport { sub } from '../sub';\r\nimport { computeWeightedLoss } from './compute_weighted_loss';\r\nfunction sigmoidCrossEntropyWithLogits_(labels, logits) {\r\n    const $labels = convertToTensor(labels, 'labels', 'sigmoidCrossEntropyWithLogits');\r\n    const $logits = convertToTensor(logits, 'logits', 'sigmoidCrossEntropyWithLogits');\r\n    assertShapesMatch($labels.shape, $logits.shape, 'Error in sigmoidCrossEntropyWithLogits: ');\r\n    /**\r\n     * Implementation Details:\r\n     *\r\n     * For brevity, let `x = logits`, `z = labels`.  The logistic loss is\r\n     *     z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\r\n     *   = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\r\n     *   = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\r\n     *   = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\r\n     *   = (1 - z) * x + log(1 + exp(-x))\r\n     *   = x - x * z + log(1 + exp(-x))\r\n     *\r\n     *   For x < 0, to avoid overflow in exp(-x), we reformulate the above\r\n     *     x - x * z + log(1 + exp(-x))\r\n     *   = log(exp(x)) - x * z + log(1 + exp(-x))\r\n     *   = - x * z + log(1 + exp(x))\r\n     *\r\n     * Hence, to ensure stability and avoid overflow, the implementation uses\r\n     * this equivalent formulation:\r\n     *     max(x, 0) - x * z + log(1 + exp(-abs(x)))\r\n     */\r\n    const maxOutput = relu($logits);\r\n    const outputXTarget = mul($logits, $labels);\r\n    const sigmoidOutput = log1p(exp(neg(abs($logits))));\r\n    return add(sub(maxOutput, outputXTarget), sigmoidOutput);\r\n}\r\n/**\r\n * Computes the sigmoid cross entropy loss between two tensors.\r\n *\r\n * If labelSmoothing is nonzero, smooth the labels towards 1/2:\r\n *\r\n *   newMulticlassLabels = multiclassLabels * (1 - labelSmoothing)\r\n *                         + 0.5 * labelSmoothing\r\n *\r\n * @param multiClassLabels The ground truth output tensor of shape\r\n * [batch_size, num_classes], same dimensions as 'predictions'.\r\n * @param logits The predicted outputs.\r\n * @param weights Tensor whose rank is either 0, or the same rank as\r\n *    `labels`, and must be broadcastable to `labels` (i.e., all dimensions\r\n *    must be either `1`, or the same as the corresponding `losses`\r\n *    dimension).\r\n * @param labelSmoothing If greater than 0, then smooth the labels.\r\n * @param reduction Type of reduction to apply to loss. Should be of type\r\n *    `Reduction`\r\n *\r\n * @doc { heading: 'Training', subheading: 'Losses', namespace: 'losses' }\r\n */\r\nfunction sigmoidCrossEntropy_(multiClassLabels, logits, weights, labelSmoothing = 0, reduction = Reduction.SUM_BY_NONZERO_WEIGHTS) {\r\n    let $multiClassLabels = convertToTensor(multiClassLabels, 'multiClassLabels', 'sigmoidCrossEntropy');\r\n    const $logits = convertToTensor(logits, 'logits', 'sigmoidCrossEntropy');\r\n    let $weights = null;\r\n    if (weights != null) {\r\n        $weights = convertToTensor(weights, 'weights', 'sigmoidCrossEntropy');\r\n    }\r\n    assertShapesMatch($multiClassLabels.shape, $logits.shape, 'Error in sigmoidCrossEntropy: ');\r\n    if (labelSmoothing > 0) {\r\n        const labelSmoothingScalar = scalar(labelSmoothing);\r\n        const one = scalar(1);\r\n        const half = scalar(0.5);\r\n        $multiClassLabels =\r\n            add(mul($multiClassLabels, sub(one, labelSmoothingScalar)), mul(half, labelSmoothingScalar));\r\n    }\r\n    const losses = sigmoidCrossEntropyWithLogits_($multiClassLabels, $logits);\r\n    return computeWeightedLoss(losses, $weights, reduction);\r\n}\r\nexport const sigmoidCrossEntropy = op({ sigmoidCrossEntropy_ });\r\n//# sourceMappingURL=sigmoid_cross_entropy.js.map"]},"metadata":{},"sourceType":"module"}